@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@InProceedings{cut,
	author="Park, Taesung
	and Efros, Alexei A.
	and Zhang, Richard
	and Zhu, Jun-Yan",
	editor="Vedaldi, Andrea
	and Bischof, Horst
	and Brox, Thomas
	and Frahm, Jan-Michael",
	title="Contrastive Learning for Unpaired Image-to-Image Translation",
	booktitle="Computer Vision -- ECCV 2020",
	year="2020",
	publisher="Springer International Publishing",
	address="Cham",
	pages="319--345",
	abstract="In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so -- maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each ``domain'' is only a single image.",
	isbn="978-3-030-58545-7"
}

@InProceedings{negcut,
	author    = {Wang, Weilun and Zhou, Wengang and Bao, Jianmin and Chen, Dong and Li, Houqiang},
	title     = {Instance-Wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation},
	booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
	month     = {October},
	year      = {2021},
	pages     = {14020-14029}
}

@InProceedings{sesim,
	author    = {Zheng, Chuanxia and Cham, Tat-Jen and Cai, Jianfei},
	title     = {The Spatially-Correlative Loss for Various Image Translation Tasks},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2021},
	pages     = {16407-16417}
}

@InProceedings{qsAttn,
	author    = {Hu, Xueqi and Zhou, Xinyue and Huang, Qiusheng and Shi, Zhengyi and Sun, Li and Li, Qingli},
	title     = {QS-Attn: Query-Selected Attention for Contrastive Learning in I2I Translation},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2022},
	pages     = {18291-18300}
}

@InProceedings{mcl,
	author    = {Zhan, Fangneng and Yu, Yingchen and Wu, Rongliang and Zhang, Jiahui and Lu, Shijian and Zhang, Changgong},
	title     = {Marginal Contrastive Correspondence for Guided Image Generation},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2022},
	pages     = {10663-10672}
}

@InProceedings{moNCE,
	author    = {Zhan, Fangneng and Zhang, Jiahui and Yu, Yingchen and Wu, Rongliang and Lu, Shijian},
	title     = {Modulated Contrast for Versatile Image Synthesis},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2022},
	pages     = {18280-18290}
}

@InProceedings{HnegSRC,
	author    = {Jung, Chanyong and Kwon, Gihyun and Ye, Jong Chul},
	title     = {Exploring Patch-Wise Semantic Relation for Contrastive Learning in Image-to-Image Translation Tasks},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2022},
	pages     = {18260-18269}
}

@InProceedings{splicing,
	author    = {Tumanyan, Narek and Bar-Tal, Omer and Bagon, Shai and Dekel, Tali},
	title     = {Splicing ViT Features for Semantic Appearance Transfer},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2022},
	pages     = {10748-10757}
}

@InProceedings{text2live,
	author="Bar-Tal, Omer
	and Ofri-Amar, Dolev
	and Fridman, Rafail
	and Kasten, Yoni
	and Dekel, Tali",
	editor="Avidan, Shai
	and Brostow, Gabriel
	and Ciss{\'e}, Moustapha
	and Farinella, Giovanni Maria
	and Hassner, Tal",
	title="Text2LIVE: Text-Driven Layered Image andÂ Video Editing",
	booktitle="Computer Vision -- ECCV 2022",
	year="2022",
	publisher="Springer Nature Switzerland",
	address="Cham",
	pages="707--723",
	abstract="We present a method for zero-shot, text-driven editing of natural images and videos. Given an image or a video and a text prompt, our goal is to edit the appearance of existing objects (e.g., texture) or augment the scene with visual effects (e.g., smoke, fire) in a semantic manner. We train a generator on an internal dataset, extracted from a single input, while leveraging an external pretrained CLIP model to impose our losses. Rather than directly generating the edited output, our key idea is to generate an edit layer (color+opacity) that is composited over the input. This allows us to control the generation and maintain high fidelity to the input via novel text-driven losses applied directly to the edit layer. Our method neither relies on a pretrained generator nor requires user-provided masks. We demonstrate localized, semantic edits on high-resolution images and videos across a variety of objects and scenes. Webpage: http://www.text2live.github.io.",
	isbn="978-3-031-19784-0"
}

@inproceedings{
	gcn,
	title={Semi-Supervised Classification with Graph Convolutional Networks},
	author={Thomas N. Kipf and Max Welling},
	booktitle={International Conference on Learning Representations},
	year={2017},
	url={https://openreview.net/forum?id=SJU4ayYgl}
}

@misc{tagcn,
	doi = {10.48550/ARXIV.1710.10370},
	
	url = {https://arxiv.org/abs/1710.10370},
	
	author = {Du, Jian and Zhang, Shanghang and Wu, Guanhang and Moura, Jose M. F. and Kar, Soummya},
	
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Topology Adaptive Graph Convolutional Networks},
	
	publisher = {arXiv},
	
	year = {2017},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{lagraph,
	title = 	 {Self-Supervised Representation Learning via Latent Graph Prediction},
	author =       {Xie, Yaochen and Xu, Zhao and Ji, Shuiwang},
	booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
	pages = 	 {24460--24477},
	year = 	 {2022},
	editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume = 	 {162},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {17--23 Jul},
	publisher =    {PMLR},
	pdf = 	 {https://proceedings.mlr.press/v162/xie22e/xie22e.pdf},
	url = 	 {https://proceedings.mlr.press/v162/xie22e.html},
	abstract = 	 {Self-supervised learning (SSL) of graph neural networks is emerging as a promising way of leveraging unlabeled data. Currently, most methods are based on contrastive learning adapted from the image domain, which requires view generation and a sufficient number of negative samples. In contrast, existing predictive models do not require negative sampling, but lack theoretical guidance on the design of pretext training tasks. In this work, we propose the LaGraph, a theoretically grounded predictive SSL framework based on latent graph prediction. Learning objectives of LaGraph are derived as self-supervised upper bounds to objectives for predicting unobserved latent graphs. In addition to its improved performance, LaGraph provides explanations for recent successes of predictive models that include invariance-based objectives. We provide theoretical analysis comparing LaGraph to related methods in different domains. Our experimental results demonstrate the superiority of LaGraph in performance and the robustness to decreasing of training sample size on both graph-level and node-level tasks.}
}



@InProceedings{sep,
	title = 	 {Structural Entropy Guided Graph Hierarchical Pooling},
	author =       {Wu, Junran and Chen, Xueyuan and Xu, Ke and Li, Shangzhe},
	booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
	pages = 	 {24017--24030},
	year = 	 {2022},
	editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume = 	 {162},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {17--23 Jul},
	publisher =    {PMLR},
	pdf = 	 {https://proceedings.mlr.press/v162/wu22b/wu22b.pdf},
	url = 	 {https://proceedings.mlr.press/v162/wu22b.html},
	abstract = 	 {Following the success of convolution on non-Euclidean space, the corresponding pooling approaches have also been validated on various tasks regarding graphs. However, because of the fixed compression ratio and stepwise pooling design, these hierarchical pooling methods still suffer from local structure damage and suboptimal problem. In this work, inspired by structural entropy, we propose a hierarchical pooling approach, SEP, to tackle the two issues. Specifically, without assigning the layer-specific compression ratio, a global optimization algorithm is designed to generate the cluster assignment matrices for pooling at once. Then, we present an illustration of the local structure damage from previous methods in reconstruction of ring and grid synthetic graphs. In addition to SEP, we further design two classification models, SEP-G and SEP-N for graph classification and node classification, respectively. The results show that SEP outperforms state-of-the-art graph pooling methods on graph classification benchmarks and obtains superior performance on node classifications.}
}

@inproceedings{
	structPool,
	title={StructPool: Structured Graph Pooling via Conditional Random Fields},
	author={Hao Yuan and Shuiwang Ji},
	booktitle={International Conference on Learning Representations},
	year={2020},
	url={https://openreview.net/forum?id=BJxg_hVtwH}
}

@InProceedings{hkd,
	author    = {Zhou, Sheng and Wang, Yucheng and Chen, Defang and Chen, Jiawei and Wang, Xin and Wang, Can and Bu, Jiajun},
	title     = {Distilling Holistic Knowledge With Graph Neural Networks},
	booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
	month     = {October},
	year      = {2021},
	pages     = {10387-10396}
}

@InProceedings{tokenCut,
	author    = {Wang, Yangtao and Shen, Xi and Hu, Shell Xu and Yuan, Yuan and Crowley, James L. and Vaufreydaz, Dominique},
	title     = {Self-Supervised Transformers for Unsupervised Object Discovery Using Normalized Cut},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2022},
	pages     = {14543-14553}
}

@InProceedings{deepSpectral,
	author    = {Melas-Kyriazi, Luke and Rupprecht, Christian and Laina, Iro and Vedaldi, Andrea},
	title     = {Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised Semantic Segmentation and Localization},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2022},
	pages     = {8364-8375}
}

@inproceedings{
	visionGNN,
	title={Vision {GNN}: An Image is Worth Graph of Nodes},
	author={Kai Han and Yunhe Wang and Jianyuan Guo and Yehui Tang and Enhua Wu},
	booktitle={Advances in Neural Information Processing Systems},
	editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
	year={2022},
	url={https://openreview.net/forum?id=htM1WJZVB2I}
}

@InProceedings{superGlue,
	author = {Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
	title = {SuperGlue: Learning Feature Matching With Graph Neural Networks},
	booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2020}
}

@misc{semantic2graph,
	doi = {10.48550/ARXIV.2209.05653},
	
	url = {https://arxiv.org/abs/2209.05653},
	
	author = {Zhang, Junbin and Tsai, Pei-Hsuan and Tsai, Meng-Hsun},
	
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Multimedia (cs.MM), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.10; I.4.8; I.5, 68T01, 68T30, 68T45},
	
	title = {Semantic2Graph: Graph-based Multi-modal Feature for Action Segmentation in Videos},
	
	publisher = {arXiv},
	
	year = {2022},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{cpc,
	doi = {10.48550/ARXIV.1807.03748},
	
	url = {https://arxiv.org/abs/1807.03748},
	
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Representation Learning with Contrastive Predictive Coding},
	
	publisher = {arXiv},
	
	year = {2018},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{cyclegan,
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	title = {Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks},
	booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
	month = {Oct},
	year = {2017}
}
@inproceedings{pix2pix,
	title={Image-to-image translation with conditional adversarial networks},
	author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={1125--1134},
	year={2017}
}

@InProceedings{gcgan,
	author = {Fu, Huan and Gong, Mingming and Wang, Chaohui and Batmanghelich, Kayhan and Zhang, Kun and Tao, Dacheng},
	title = {Geometry-Consistent Generative Adversarial Networks for One-Sided Unsupervised Domain Mapping},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2019}
}

@inproceedings{distancegan,
	author = {Benaim, Sagie and Wolf, Lior},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {One-Sided Unsupervised Domain Mapping},
	url = {https://proceedings.neurips.cc/paper/2017/file/59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf},
	volume = {30},
	year = {2017}
}

@InProceedings{munit,
	author = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},
	title = {Multimodal Unsupervised Image-to-image Translation},
	booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
	month = {September},
	year = {2018}
}
@inproceedings{nicegan,
	title={Reusing discriminators for encoding: Towards unsupervised image-to-image translation},
	author={Chen, Runfa and Huang, Wenbing and Huang, Binghui and Sun, Fuchun and Fang, Bin},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={8168--8177},
	year={2020}
}

@article{fid,
	title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
	author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}

@article{kid,
	title={Demystifying mmd gans},
	author={Bi{\'n}kowski, Miko{\l}aj and Sutherland, Danica J and Arbel, Michael and Gretton, Arthur},
	journal={arXiv preprint arXiv:1801.01401},
	year={2018}
}

@article{vgg,
	title={Very deep convolutional networks for large-scale image recognition},
	author={Simonyan, Karen and Zisserman, Andrew},
	journal={arXiv preprint arXiv:1409.1556},
	year={2014}
}

@InProceedings{strotss,
	author = {Kolkin, Nicholas and Salavon, Jason and Shakhnarovich, Gregory},
	title = {Style Transfer by Relaxed Optimal Transport and Self-Similarity},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2019}
}


@InProceedings{wct2,
	author = {Yoo, Jaejun and Uh, Youngjung and Chun, Sanghyuk and Kang, Byeongkyu and Ha, Jung-Woo},
	title = {Photorealistic Style Transfer via Wavelet Transforms},
	booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
	month = {October},
	year = {2019}
}


@InProceedings{graphUnet,
	title = 	 {Graph U-Nets},
	author =       {Gao, Hongyang and Ji, Shuiwang},
	booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
	pages = 	 {2083--2092},
	year = 	 {2019},
	editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume = 	 {97},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {09--15 Jun},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v97/gao19a/gao19a.pdf},
	url = 	 {https://proceedings.mlr.press/v97/gao19a.html},
	abstract = 	 {We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and unpooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.}
}

@InProceedings{cbam,
	author = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
	title = {CBAM: Convolutional Block Attention Module},
	booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
	month = {September},
	year = {2018}
}

@inproceedings{bam,
	author    = {Jongchan Park and
	Sanghyun Woo and
	Joon{-}Young Lee and
	In So Kweon},
	title     = {{BAM:} Bottleneck Attention Module},
	booktitle = {British Machine Vision Conference 2018, {BMVC} 2018, Newcastle, UK,
	September 3-6, 2018},
	pages     = {147},
	publisher = {{BMVA} Press},
	year      = {2018},
	url       = {http://bmvc2018.org/contents/papers/0092.pdf},
	timestamp = {Tue, 21 Apr 2020 23:17:57 +0200},
	biburl    = {https://dblp.org/rec/conf/bmvc/ParkWLK18.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gkd,
	title={Deep geometric knowledge distillation with graphs},
	author={Lassance, Carlos and Bontonou, Myriam and Hacene, Ghouthi Boukli and Gripon, Vincent and Tang, Jian and Ortega, Antonio},
	booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages={8484--8488},
	year={2020},
	organization={IEEE}
}


@inproceedings{lsgan,
	title={Least squares generative adversarial networks},
	author={Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond YK and Wang, Zhen and Paul Smolley, Stephen},
	booktitle={Proceedings of the IEEE international conference on computer vision},
	pages={2794--2802},
	year={2017}
}