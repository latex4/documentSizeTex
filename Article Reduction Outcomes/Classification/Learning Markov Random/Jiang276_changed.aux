\relax 
\bibstyle{aaai23}
\citation{DBLP:journals/neco/Hinton02,Tsochantaridis2005StructualSVM,goodfellow2014generative,kingma2013auto,Germain2015MADEMA,Larochelle2011Autoregressive,Oord2016PixelRN,arjovsky17WGAN,song2019generative,song21scoredifussion,murphy1999loopy,yedidia2001generalized,wainwright2006relaxation}
\citation{Braunstein2005SurveyPA,AnHaHoTi2007,chavira2006compiling,VanHentenryck1989,GOGATE2012AndOr,Sang2005}
\citation{kusner17GrammarVAE,Jin2018JunctionTV,Dai2018SyntaxDirectedVA,Hu17ControlledTextGen,lowd2008learning,DBLP:conf/icml/DingMXX21}
\citation{erdHos1973problems}
\citation{DBLP:journals/jacm/MoserT10}
\citation{DBLP:journals/jacm/GuoJL19}
\citation{DBLP:conf/iclr/SelsamLBLMD19,DBLP:conf/icml/DuanVPRM22,DBLP:conf/nips/LiCK18}
\citation{DBLP:conf/nips/YolcuP19,DBLP:conf/nips/ChenT19}
\citation{DBLP:conf/nips/KaraliasL20,DBLP:journals/eor/BengioLP21}
\citation{DBLP:conf/aaai/MandiDSG20}
\citation{neal1993probabilistic,DBLP:journals/pami/DagumC93,pmlr-v84-ge18b}
\citation{murphy1999loopy,DBLP:journals/jmlr/IhlerFW05,DBLP:journals/corr/abs-2011-02303,pmlr-v138-fan20a}
\citation{DBLP:journals/ai/GogateD11}
\citation{Gomes2006Sampling,Ermon13Wish,DBLP:conf/sat/AchlioptasT17,Chakraborty2013scalable}
\citation{nanyiye2022}
\citation{DBLP:journals/neco/Hinton02}
\citation{Sang2005}
\citation{Mansour1994LearningBF}
\newlabel{sec:prelim}{{}{2}{}{}{}}
\newlabel{eq:mrf}{{1}{2}{}{}{}}
\newlabel{eq:constr_mrf}{{2}{2}{}{}{}}
\newlabel{eq:log-likelihood}{{3}{2}{}{}{}}
\newlabel{eq:gradient}{{4}{2}{}{}{}}
\citation{DBLP:journals/jacm/GuoJL19}
\citation{erdHos1973problems}
\citation{DBLP:journals/jacm/GuoJL19}
\citation{DBLP:journals/jacm/GuoJL19}
\newlabel{cond:extreme}{{1}{3}{}{}{}}
\newlabel{eq:constr_mrf_single}{{6}{3}{}{}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:lll-sampler}{{1}{3}{Sampling Through Lov\'asz Local Lemma.}{}{}}
\newlabel{th:product-dist}{{1}{3}{}{}{}}
\newlabel{sec:methodology}{{}{3}{}{}{}}
\newlabel{sec:nls}{{}{3}{}{}{}}
\newlabel{eq:cnf}{{7}{4}{}{}{}}
\newlabel{eq:weight}{{8}{4}{}{}{}}
\newlabel{eq:bias}{{9}{4}{}{}{}}
\newlabel{eq:mapping-matrix}{{10}{4}{}{}{}}
\newlabel{eq:Zt}{{12}{4}{}{}{}}
\newlabel{notation:C}{{13}{4}{}{}{}}
\newlabel{eq:iterative}{{15}{4}{}{}{}}
\newlabel{example:matrix}{{1}{4}{}{}{}}
\citation{carter1994gibbs}
\citation{DBLP:conf/tacas/GuptaSRM19}
\citation{DBLP:conf/aaai/ChakrabortyFMSV14}
\citation{DBLP:conf/nips/ErmonGSS13,DBLP:conf/uai/DingX21}
\citation{DBLP:conf/fmcad/GoliaSCM21}
\citation{DBLP:conf/icse/DutraLBS18}
\citation{DBLP:conf/cav/SoosGM20}
\citation{DBLP:conf/lpar/SharmaGRM18}
\citation{DBLP:conf/aiia/DodaroP19}
\citation{DBLP:conf/sat/LauriaENV17}
\citation{DBLP:conf/sat/IgnatievMM18}
\citation{DBLP:conf/aaai/ChakrabortyM19}
\newlabel{alg:main}{{2}{5}{Learn Constrained MRFs via \textsc  {Nelson}\xspace  -CD.}{}{}}
\newlabel{fig:uniform_sample}{{1}{5}{Running time and the percentage of valid structures sampled uniformly at random from solutions of K-SAT problems. {Among all the problem sizes}, \textsc  {Nelson}\xspace  always generate valid solutions and is the most efficient sampler.}{}{}}
\citation{DBLP:journals/combinatorics/CohnPP02}
\citation{takahashi2009communication}
\newlabel{tab:sampler}{{1}{6}{Sampling efficiency and accuracy for learning $K$-SAT solutions with preferences. The proposed \textsc  {Nelson}\xspace  is the most efficient (see ``Training Time Per Epoch'') and always generates valid assignments (see ``Validness'') with a small approximation error (see ``Approximation Error of Gradient'') against all baselines. T.O. means time out.}{}{}}
\newlabel{tab:learn-sat}{{2}{6}{The quality of learning outcomes for learning random $K$-SAT solutions with preferences. \textsc  {Nelson}\xspace  achieves the best likelihood and MAP@10 scores. T.O. is time out.}{}{}}
\newlabel{fig:weighted_sample}{{2}{6}{Running time, the percentage of valid solutions generated, and rounds of resampling for weighted sample generation of K-SAT solutions. {Among all the problem sizes}, \textsc  {Nelson}\xspace  scales the best among all approaches and always generates valid solutions. }{}{}}
\bibdata{reference,fan}
\newlabel{tab:sink-free}{{3}{7}{Sample efficiency and learning performance of the sink-free orientation task. \textsc  {Nelson}\xspace  is the most efficient (see Training Time Per Epoch) and always generates valid assignments (see Validness), has the smallest error approximating gradients, and has the best learning performance (see MAP@10) among all baselines.}{}{}}
\newlabel{fig:route}{{3}{7}{Frequency histograms for the number of resample and the total time of \textsc  {Nelson}\xspace  method for uniformly sampling visiting paths for vehicle routing problem.}{}{}}
\gdef \@abspage@last{8}
