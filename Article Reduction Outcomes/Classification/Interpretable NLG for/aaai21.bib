@inproceedings{wen-etal-2015-semantically,
	title = "Semantically Conditioned {LSTM}-based Natural Language Generation for Spoken Dialogue Systems",
	author = "Wen, Tsung-Hsien  and
	Ga{\v{s}}i{\'c}, Milica  and
	Mrk{\v{s}}i{\'c}, Nikola  and
	Su, Pei-Hao  and
	Vandyke, David  and
	Young, Steve",
	booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
	month = sep,
	year = "2015",
	address = "Lisbon, Portugal",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D15-1199",
	doi = "10.18653/v1/D15-1199",
	pages = "1711--1721",
}

@inproceedings{wen-etal-2015-stochastic,
	title = "Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking",
	author = "Wen, Tsung-Hsien  and
	Ga{\v{s}}i{\'c}, Milica  and
	Kim, Dongho  and
	Mrk{\v{s}}i{\'c}, Nikola  and
	Su, Pei-Hao  and
	Vandyke, David  and
	Young, Steve",
	booktitle = "Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
	month = sep,
	year = "2015",
	address = "Prague, Czech Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/W15-4639",
	doi = "10.18653/v1/W15-4639",
	pages = "275--284",
}

@inproceedings{zhu2019multi,
	title={Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue},
	author={Zhu, Chenguang and Zeng, Michael and Huang, Xuedong},
	booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	pages={1261--1266},
	year={2019}
}

@inproceedings{tran-nguyen-2017-natural,
	title = "Natural Language Generation for Spoken Dialogue System using {RNN} Encoder-Decoder Networks",
	author = "Tran, Van-Khanh  and
	Nguyen, Le-Minh",
	booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)",
	month = aug,
	year = "2017",
	address = "Vancouver, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/K17-1044",
	doi = "10.18653/v1/K17-1044",
	pages = "442--451",
	abstract = "Natural language generation (NLG) is a critical component in a spoken dialogue system. This paper presents a Recurrent Neural Network based Encoder-Decoder architecture, in which an LSTM-based decoder is introduced to select, aggregate semantic elements produced by an attention mechanism over the input elements, and to produce the required utterances. The proposed generator can be jointly trained both sentence planning and surface realization to produce natural language sentences. The proposed model was extensively evaluated on four different NLG datasets. The experimental results showed that the proposed generators not only consistently outperform the previous methods across all the NLG domains but also show an ability to generalize from a new, unseen domain and learn from multi-domain datasets.",
}

@inproceedings{dusek-jurcicek-2016-sequence,
	title = "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings",
	author = "Du{\v{s}}ek, Ond{\v{r}}ej  and
	Jur{\v{c}}{\'\i}{\v{c}}ek, Filip",
	booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
	month = aug,
	year = "2016",
	address = "Berlin, Germany",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P16-2008",
	doi = "10.18653/v1/P16-2008",
	pages = "45--51",
}

@article{sutskever2014sequence,
	title={Sequence to sequence learning with neural networks},
	author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	journal={arXiv preprint arXiv:1409.3215},
	year={2014}
}

@inproceedings{novikova-etal-2017-e2e,
	title = "The {E}2{E} Dataset: New Challenges For End-to-End Generation",
	author = "Novikova, Jekaterina  and
	Du{\v{s}}ek, Ond{\v{r}}ej  and
	Rieser, Verena",
	booktitle = "Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue",
	month = aug,
	year = "2017",
	address = {Saarbr{\"u}cken, Germany},
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/W17-5525",
	doi = "10.18653/v1/W17-5525",
	pages = "201--206",
	abstract = "This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data.",
}

@misc{mirkovic2011dialogue,
	title={Dialogue management using scripts},
	author={Mirkovic, Danilo and Cavedon, Lawrence},
	year={2011},
	month=oct # "~18",
	publisher={Google Patents},
	note={US Patent 8,041,570}
}

@misc{cheyer2014method,
	title={Method and apparatus for building an intelligent automated assistant},
	author={Cheyer, Adam and Guzzoni, Didier},
	year={2014},
	month=mar # "~18",
	publisher={Google Patents},
	note={US Patent 8,677,377}
}

@article{bahdanau2014neural,
	title={Neural machine translation by jointly learning to align and translate},
	author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	journal={arXiv preprint arXiv:1409.0473},
	year={2014}
}

@inproceedings{juraska-etal-2018-deep,
	title = "A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation",
	author = "Juraska, Juraj  and
	Karagiannis, Panagiotis  and
	Bowden, Kevin  and
	Walker, Marilyn",
	booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
	month = jun,
	year = "2018",
	address = "New Orleans, Louisiana",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N18-1014",
	doi = "10.18653/v1/N18-1014",
	pages = "152--162",
	abstract = "Natural language generation lies at the core of generative dialogue systems and conversational agents. We describe an ensemble neural language generator, and present several novel methods for data representation and augmentation that yield improved results in our model. We test the model on three datasets in the restaurant, TV and laptop domains, and report both objective and subjective evaluations of our best model. Using a range of automatic metrics, as well as human evaluators, we show that our approach achieves better results than state-of-the-art models on the same datasets.",
}

@inproceedings{nayak2017plan,
	title={To Plan or not to Plan? Discourse Planning in Slot-Value Informed Sequence to Sequence Models for Language Generation.},
	author={Nayak, Neha and Hakkani-T{\"u}r, Dilek and Walker, Marilyn A and Heck, Larry P},
	booktitle={INTERSPEECH},
	pages={3339--3343},
	year={2017}
}

@inproceedings{vinyals2015pointer,
	title={Pointer networks},
	author={Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
	booktitle={Advances in neural information processing systems},
	pages={2692--2700},
	year={2015}
}

@inproceedings{mikolov2010recurrent,
	title={Recurrent neural network based language model},
	author={Mikolov, Tom{\'a}{\v{s}} and Karafi{\'a}t, Martin and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
	booktitle={Eleventh annual conference of the international speech communication association},
	year={2010}
}

@article{jang2016categorical,
	title={Categorical reparameterization with gumbel-softmax},
	author={Jang, Eric and Gu, Shixiang and Poole, Ben},
	journal={arXiv preprint arXiv:1611.01144},
	year={2016}
}

@inproceedings{van2017neural,
	title={Neural discrete representation learning},
	author={van den Oord, Aaron and Vinyals, Oriol and others},
	booktitle={Advances in Neural Information Processing Systems},
	pages={6306--6315},
	year={2017}
}

@article{hochreiter1997long,
	title={Long short-term memory},
	author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	journal={Neural computation},
	volume={9},
	number={8},
	pages={1735--1780},
	year={1997},
	publisher={MIT Press}
}

@inproceedings{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	booktitle={Advances in neural information processing systems},
	pages={5998--6008},
	year={2017}
}

@inproceedings{oh2000stochastic,
	title={Stochastic language generation for spoken dialogue systems},
	author={Oh, Alice and Rudnicky, Alexander},
	booktitle={ANLP-NAACL 2000 Workshop: Conversational Systems},
	year={2000}
}

@inproceedings{ratnaparkhi2000trainable,
	title={Trainable methods for surface natural language generation},
	author={Ratnaparkhi, Adwait},
	booktitle={Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference},
	pages={194--201},
	year={2000},
	organization={Association for Computational Linguistics}
}

@article{mairesse2014stochastic,
	title={Stochastic language generation in dialogue using factored language models},
	author={Mairesse, Fran{\c{c}}ois and Young, Steve},
	journal={Computational Linguistics},
	volume={40},
	number={4},
	pages={763--799},
	year={2014},
	publisher={MIT Press}
}

@book{gumbel1948statistical,
	title={Statistical theory of extreme values and some practical applications: a series of lectures},
	author={Gumbel, Emil Julius},
	volume={33},
	year={1948},
	publisher={US Government Printing Office}
}

@article{bengio2013estimating,
	title={Estimating or propagating gradients through stochastic neurons for conditional computation},
	author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
	journal={arXiv preprint arXiv:1308.3432},
	year={2013}
}

@article{kingma2013auto,
	title={Auto-encoding variational bayes},
	author={Kingma, Diederik P and Welling, Max},
	journal={arXiv preprint arXiv:1312.6114},
	year={2013}
}

@article{kingma2014adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@inproceedings{li-etal-2020-slot,
	title = "Slot-consistent {NLG} for Task-oriented Dialogue Systems with Iterative Rectification Network",
	author = "Li, Yangming  and
	Yao, Kaisheng  and
	Qin, Libo  and
	Che, Wanxiang  and
	Li, Xiaolong  and
	Liu, Ting",
	booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/2020.acl-main.10",
	doi = "10.18653/v1/2020.acl-main.10",
	pages = "97--106",
	abstract = "Data-driven approaches using neural networks have achieved promising performances in natural language generation (NLG). However, neural generators are prone to make mistakes, e.g., neglecting an input slot value and generating a redundant slot value. Prior works refer this to hallucination phenomenon. In this paper, we study slot consistency for building reliable NLG systems with all slot values of input dialogue act (DA) properly generated in output sentences. We propose Iterative Rectification Network (IRN) for improving general NLG systems to produce both correct and fluent responses. It applies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness.",
}

@inproceedings{li-etal-2020-handling,
	title = "Handling Rare Entities for Neural Sequence Labeling",
	author = "Li, Yangming  and
	Li, Han  and
	Yao, Kaisheng  and
	Li, Xiaolong",
	booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/2020.acl-main.574",
	doi = "10.18653/v1/2020.acl-main.574",
	pages = "6441--6451",
	abstract = "One great challenge in neural sequence labeling is the data sparsity problem for rare entity words and phrases. Most of test set entities appear only few times and are even unseen in training corpus, yielding large number of out-of-vocabulary (OOV) and low-frequency (LF) entities during evaluation. In this work, we propose approaches to address this problem. For OOV entities, we introduce local context reconstruction to implicitly incorporate contextual information into their representations. For LF entities, we present delexicalized entity identification to explicitly extract their frequency-agnostic and entity-type-specific representations. Extensive experiments on multiple benchmark datasets show that our model has significantly outperformed all previous methods and achieved new start-of-the-art results. Notably, our methods surpass the model fine-tuned on pre-trained language models without external resource.",
}

@article{Li_Yao_Qin_Peng_Liu_Li_2020, 
	title={Span-Based Neural Buffer: Towards Efficient and Effective Utilization of Long-Distance Context for Neural Sequence Models}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6343}, DOI={10.1609/aaai.v34i05.6343}, abstractNote={&lt;p&gt;Neural sequence model, though widely used for modeling sequential data such as the language model, has &lt;em&gt;sequential recency bias&lt;/em&gt; (Kuncoro et al. 2018) to the local context, limiting its full potential to capture long-distance context. To address this problem, this paper proposes augmenting sequence models with a span-based neural buffer that efficiently represents long-distance context, allowing a gate policy network to make interpolated predictions from both the neural buffer and the underlying sequence model. Training this policy network to utilize long-distance context is however challenging due to the &lt;em&gt;simple sentence dominance&lt;/em&gt; problem (Marvin and Linzen 2018). To alleviate this problem, we propose a novel training algorithm that combines an annealed maximum likelihood estimation with an intrinsic reward-driven reinforcement learning. Sequence models with the proposed span-based neural buffer significantly improve the state-of-the-art perplexities on the benchmark Penn Treebank and WikiText-2 datasets to 43.9 and 35.2 respectively. We conduct extensive analysis and confirm that the proposed architecture and the training algorithm both contribute to the improvements.&lt;/p&gt;}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Li, Yangming and Yao, Kaisheng and Qin, Libo and Peng, Shuang and Liu, Yijia and Li, Xiaolong}, year={2020}, month={Apr.}, pages={8277-8284} 
}

@article{li2020rewriter,
	title={Rewriter-Evaluator Framework for Neural Machine Translation},
	author={Li, Yangming and Yao, Kaisheng},
	journal={arXiv preprint arXiv:2012.05414},
	year={2020}
}

@inproceedings{jain-wallace-2019-attention,
	title = "{A}ttention is not {E}xplanation",
	author = "Jain, Sarthak  and
	Wallace, Byron C.",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N19-1357",
	doi = "10.18653/v1/N19-1357",
	pages = "3543--3556",
	abstract = "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful {``}explanations{''} for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.",
}
