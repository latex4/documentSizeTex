\relax 
\citation{DBLP:conf/cvpr/MisraSGH16}
\citation{DBLP:conf/cvpr/ZamirSSGMS18}
\citation{DBLP:conf/icml/CollobertW08}
\citation{DBLP:journals/corr/LuongLSVK15}
\citation{DBLP:conf/acl/LiuQH17}
\citation{DBLP:journals/ml/Caruana97}
\citation{DBLP:conf/icml/CollobertW08}
\citation{DBLP:conf/iclr/SubramanianTBP18}
\citation{DBLP:conf/acl/LiuHCG19}
\citation{DBLP:conf/cvpr/MisraSGH16}
\citation{DBLP:conf/ijcai/LiuQH16}
\citation{DBLP:conf/aaai/RuderBAS19}
\citation{DBLP:conf/acl/SogaardG16}
\citation{DBLP:conf/emnlp/HashimotoXTS17}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{sfig:hard-sharing}{{1a}{1}{Hard sharing}{}{}}
\newlabel{sub@sfig:hard-sharing}{{a}{1}{Hard sharing}{}{}}
\newlabel{sfig:soft-sharing}{{1b}{1}{Soft sharing}{}{}}
\newlabel{sub@sfig:soft-sharing}{{b}{1}{Soft sharing}{}{}}
\newlabel{sfig:hier-sharing}{{1c}{1}{Hierarchical sharing}{}{}}
\newlabel{sub@sfig:hier-sharing}{{c}{1}{Hierarchical sharing}{}{}}
\newlabel{sfig:sparse-sharing}{{1d}{1}{Sparse sharing}{}{}}
\newlabel{sub@sfig:sparse-sharing}{{d}{1}{Sparse sharing}{}{}}
\newlabel{fig:param-sharing}{{1}{1}{Parameter sharing mechanisms. \includegraphics [scale=0.25]{3353_shared_neuron.pdf} and \textcolor [rgb]{0.68,0.35,0.13}{\textbf  {---}} represent shared neurons and weights respectively. \includegraphics [scale=0.25]{3353_taskA_neuron.pdf}, \includegraphics [scale=0.25]{3353_taskB_neuron.pdf}, \includegraphics [scale=0.25]{3353_taskC_neuron.pdf} represent task-specific neurons. \textcolor [rgb]{0.36,0.61,0.84}{\textbf  {---}}, \textcolor [rgb]{0.44,0.68,0.28}{\textbf  {---}}, \textcolor [rgb]{0.67,0.21,0.86}{\textbf  {---}} denote task-specific weights for three different tasks.}{}{}}
\citation{DBLP:conf/iclr/FrankleC19}
\citation{DBLP:conf/iclr/FrankleC19}
\citation{DBLP:conf/cvpr/KendallGC18}
\newlabel{eq:hard}{{1}{2}{}{}{}}
\newlabel{eq:hier}{{3}{2}{}{}{}}
\citation{DBLP:conf/iclr/ZophL17}
\citation{DBLP:conf/aaai/RealAHL19}
\citation{DBLP:conf/iclr/FrankleC19}
\citation{DBLP:conf/iclr/FrankleC19}
\citation{DBLP:conf/iclr/FrankleC19}
\citation{DBLP:conf/icml/CollobertW08}
\citation{DBLP:conf/icml/CollobertW08}
\citation{DBLP:conf/aaai/SanhWR19}
\newlabel{fig:frame}{{2}{3}{Illustration of our approach to learn sparse sharing architectures. Gray squares are the parameters of a base network. Orange squares represent the shared parameters, while blue and green ones represent private parameters of task 1 and task 2 respectively. }{}{}}
\newlabel{al:imp}{{1}{3}{Sparse Sharing Architecture Learning}{}{}}
\citation{frankle2019lottery}
\citation{frankle2019lottery}
\citation{DBLP:journals/coling/MarcusSM94}
\citation{DBLP:conf/conll/SangB00}
\citation{DBLP:conf/conll/SangM03}
\citation{DBLP:conf/conll/PradhanMXUZ12}
\citation{DBLP:conf/aaai/ChenQLH18}
\citation{DBLP:conf/aaai/ChenQLH18}
\citation{DBLP:conf/acl/MaH16}
\citation{DBLP:journals/neco/HochreiterS97}
\citation{DBLP:conf/emnlp/PenningtonSM14}
\citation{DBLP:journals/jmlr/SrivastavaHKSS14}
\citation{DBLP:conf/icml/LaffertyMP01}
\citation{DBLP:conf/acl/SogaardG16}
\citation{DBLP:conf/acl/SogaardG16}
\citation{DBLP:conf/cvpr/MisraSGH16}
\newlabel{tb:ds}{{1}{4}{Number of tokens for each dataset.}{}{}}
\newlabel{tab:hyper}{{2}{4}{Hyper-parameters used in our experiments}{}{}}
\citation{DBLP:conf/aaai/RuderBAS19}
\citation{DBLP:conf/aaai/RuderBAS19}
\citation{DBLP:conf/aaai/ChenQLH18}
\citation{DBLP:conf/aaai/ChenQLH18}
\citation{DBLP:journals/jmlr/CollobertWBKKK11}
\citation{DBLP:journals/jmlr/CollobertWBKKK11}
\citation{huang2015bidirectional}
\citation{huang2015bidirectional}
\citation{DBLP:conf/aaai/ChenQLH18}
\citation{DBLP:conf/aaai/ChenQLH18}
\citation{DBLP:conf/aaai/ChenQLH18}
\citation{DBLP:conf/aaai/ChenQLH18}
\citation{DBLP:conf/aaai/ChenQLH18}
\citation{DBLP:conf/aaai/ChenQLH18}
\newlabel{tb:main_results}{{3}{5}{Experimental results of Exp1 and Exp2. $\Delta $ denotes the improvement compared with single task baselines. We report accuracy(\%) for POS and F1 score(\%) for NER and Chunking. The best scores are in bold. The performance deterioration due to negative transfer is in gray. The embeddings and output layer are excluded when counting parameters. For the sake of simplicity and focusing the comparison of different shared mechanisms, we just use a simple fully-connected output layer as decoder in Exp1 and Exp2. }{}{}}
\newlabel{tb:sparsity}{{4}{5}{The sparsity (percent of remaining parameters) of our selected subnets.}{}{}}
\newlabel{fig:pruning}{{3}{5}{Performance on CoNLL-2003 development set with iteratively pruning. Each point represents a subnet.}{}{}}
\newlabel{tab:exp3}{{5}{6}{Experimental results on Exp3. POS, NER and Chunking tasks come from PTB, CoNLL-2003, CoNLL-2000 respectively. \dag  \ denotes the model is implemented by \def 1##2{##1}\unhbox \voidb@x \def -1000{1000}\def (\nobreak  \hskip 0in{##})##2{{##1}}\let \reserved@d =[\def \par }{}{}}
\newlabel{tb:task_rel}{{6}{6}{Results of the synthetic experiment. The performance deterioration due to negative transfer is in gray.}{}{}}
\newlabel{eq:or}{{4}{6}{}{}{}}
\newlabel{tb:task_or}{{7}{6}{Mask Overlap Ratio ($\mathrm  {OR}$) and the improvement for sparse sharing ($S^2$) compared to hard sharing ($HS$) of tasks on CoNLL-2003. The improvement is calculated using the average performance on the test set.}{}{}}
\citation{DBLP:journals/ml/Caruana97}
\citation{DBLP:journals/jair/Baxter00}
\citation{DBLP:conf/acl/SogaardG16}
\citation{DBLP:conf/emnlp/HashimotoXTS17}
\citation{DBLP:conf/eacl/PlankA17}
\citation{DBLP:conf/cvpr/MisraSGH16}
\citation{DBLP:conf/aaai/LiuFDQC19}
\citation{DBLP:conf/aaai/RuderBAS19}
\citation{DBLP:conf/iclr/FrankleC19}
\citation{DBLP:conf/iclr/FrankleC19}
\citation{DBLP:conf/iclr/FrankleC19}
\citation{frankle2019lottery}
\citation{frankle2019lottery}
\citation{yu2019playing}
\citation{yu2019playing}
\citation{DBLP:conf/icml/MolchanovAV17}
\citation{louizos2017learning}
\bibstyle{aaai}
\bibdata{3353_ref}
\newlabel{fig:avg_or}{{4}{7}{Average test performance and mask OR with different sparsity combinations.}{}{}}
\newlabel{tb:warmup}{{8}{7}{Test accuracy (for POS) and F1 (for NER and Chunking) on CoNLL-2003 and OntoNotes 5.0. MTW: Multi-Task Warmup.}{}{}}
\gdef \@abspage@last{8}
