\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi:\discretionary{}{}{}#1}\else
  \providecommand{\doi}{doi:\discretionary{}{}{}\begingroup
  \urlstyle{rm}\Url}\fi

\bibitem[{Back et~al.(2020)Back, Chinthakindi, Kedia, Lee, and
  Choo}]{back2020neurquri}
Back, S.; Chinthakindi, S.~C.; Kedia, A.; Lee, H.; and Choo, J. 2020.
\newblock NeurQuRI: Neural Question Requirement Inspector for Answerability
  Prediction in Machine Reading Comprehension.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Chen, Bolton, and Manning(2016)}]{Chen2016A}
Chen, D.; Bolton, J.; and Manning, C.~D. 2016.
\newblock A Thorough Examination of the {CNN}/Daily Mail Reading Comprehension
  Task.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 2358--2367.

\bibitem[{Choi et~al.(2018)Choi, He, Iyyer, Yatskar, Yih, Choi, Liang, and
  Zettlemoyer}]{choi2018quac}
Choi, E.; He, H.; Iyyer, M.; Yatskar, M.; Yih, W.-t.; Choi, Y.; Liang, P.; and
  Zettlemoyer, L. 2018.
\newblock {Q}u{AC}: Question Answering in Context.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, 2174--2184.

\bibitem[{Clark et~al.(2020)Clark, Luong, Le, and Manning}]{clark2019electra}
Clark, K.; Luong, M.-T.; Le, Q.~V.; and Manning, C.~D. 2020.
\newblock {ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than
  Generators.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Cui et~al.(2017)Cui, Chen, Wei, Wang, Liu, and Hu}]{Cui2017Attention}
Cui, Y.; Chen, Z.; Wei, S.; Wang, S.; Liu, T.; and Hu, G. 2017.
\newblock Attention-over-Attention Neural Networks for Reading Comprehension.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 593--602.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{devlin2018bert}
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
\newblock {BERT}: Pre-training of Deep Bidirectional Transformers for Language
  Understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, 4171--4186.

\bibitem[{Dhingra et~al.(2017)Dhingra, Liu, Yang, Cohen, and
  Salakhutdinov}]{Dhingra2017Gated}
Dhingra, B.; Liu, H.; Yang, Z.; Cohen, W.~W.; and Salakhutdinov, R. 2017.
\newblock Gated-Attention Readers for Text Comprehension.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 1832--1846.

\bibitem[{Guthrie and Mosenthal(1987)}]{doi:10.1080/00461520.1987.9653053}
Guthrie, J.~T.; and Mosenthal, P. 1987.
\newblock Literacy as Multidimensional: Locating Information and Reading
  Comprehension.
\newblock \emph{Educational Psychologist} 22(3-4): 279--297.

\bibitem[{Hermann et~al.(2015)Hermann, Kocisky, Grefenstette, Espeholt, Kay,
  Suleyman, and Blunsom}]{hermann2015teaching}
Hermann, K.~M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.; Kay, W.; Suleyman,
  M.; and Blunsom, P. 2015.
\newblock Teaching machines to read and comprehend.
\newblock \emph{Advances in neural information processing systems} 28:
  1693--1701.

\bibitem[{Hill et~al.(2015)Hill, Bordes, Chopra, and
  Weston}]{hill2015goldilocks}
Hill, F.; Bordes, A.; Chopra, S.; and Weston, J. 2015.
\newblock The Goldilocks Principle: Reading Children's Books with Explicit
  Memory Representations.
\newblock \emph{arXiv preprint arXiv:1511.02301} .

\bibitem[{Hu et~al.(2019)Hu, Wei, Peng, Huang, Yang, and Li}]{hu2019read}
Hu, M.; Wei, F.; Peng, Y.; Huang, Z.; Yang, N.; and Li, D. 2019.
\newblock Read+ verify: Machine reading comprehension with unanswerable
  questions.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, 6529--6537.

\bibitem[{Joshi et~al.(2017)Joshi, Choi, Weld, and
  Zettlemoyer}]{Joshi2017TriviaQA}
Joshi, M.; Choi, E.; Weld, D.; and Zettlemoyer, L. 2017.
\newblock {T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset
  for Reading Comprehension.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 1601--1611.

\bibitem[{Kadlec et~al.(2016)Kadlec, Schmid, Bajgar, and
  Kleindienst}]{kadlec2016text}
Kadlec, R.; Schmid, M.; Bajgar, O.; and Kleindienst, J. 2016.
\newblock Text Understanding with the Attention Sum Reader Network.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 908--918.

\bibitem[{Kundu and Ng(2018)}]{DBLP:conf/aaai/KunduN18}
Kundu, S.; and Ng, H.~T. 2018.
\newblock A Question-Focused Multi-Factor Attention Network for Question
  Answering.
\newblock In McIlraith, S.~A.; and Weinberger, K.~Q., eds., \emph{AAAI},
  5828--5835.

\bibitem[{Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy}]{lai2017race}
Lai, G.; Xie, Q.; Liu, H.; Yang, Y.; and Hovy, E. 2017.
\newblock {RACE}: Large-scale {R}e{A}ding Comprehension Dataset From
  Examinations.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, 785--794.

\bibitem[{Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut}]{Lan2020ALBERT}
Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; and Soricut, R. 2020.
\newblock {ALBERT}: A Lite BERT for Self-supervised Learning of Language
  Representations.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Li et~al.(2018)Li, He, Cai, Zhang, Zhao, Liu, Li, and
  Si}]{li2018unified}
Li, Z.; He, S.; Cai, J.; Zhang, Z.; Zhao, H.; Liu, G.; Li, L.; and Si, L. 2018.
\newblock A unified syntax-aware framework for semantic role labeling.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, 2401--2411.

\bibitem[{Li et~al.(2019)Li, He, Zhao, Zhang, Zhang, Zhou, and
  Zhou}]{li2019dependency}
Li, Z.; He, S.; Zhao, H.; Zhang, Y.; Zhang, Z.; Zhou, X.; and Zhou, X. 2019.
\newblock Dependency or span, end-to-end uniform semantic role labeling.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, 6730--6737.

\bibitem[{Li et~al.(2020)Li, Wang, Chen, Utiyama, Sumita, Zhang, and
  Zhao}]{li2020explicit}
Li, Z.; Wang, R.; Chen, K.; Utiyama, M.; Sumita, E.; Zhang, Z.; and Zhao, H.
  2020.
\newblock Explicit Sentence Compression for Neural Machine Translation.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}
  34(05): 8311--8318.

\bibitem[{Liu et~al.(2021)Liu, Zhang, , Zhao, Zhou, and Zhou}]{liu2021filling}
Liu, L.; Zhang, Z.; ; Zhao, H.; Zhou, X.; and Zhou, X. 2021.
\newblock Filling the Gap of Utterance-aware and Speaker-aware Representation
  for Multi-turn Dialogue.
\newblock In \emph{The Thirty-Fifth AAAI Conference on Artificial Intelligence
  (AAAI-21)}.

\bibitem[{Liu et~al.(2018)Liu, Li, Fang, Kim, Duh, and Gao}]{liu2018stochastic}
Liu, X.; Li, W.; Fang, Y.; Kim, A.; Duh, K.; and Gao, J. 2018.
\newblock Stochastic Answer Networks for {SQuAD} 2.0.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 1694--1704.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{liu2019roberta}
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.;
  Zettlemoyer, L.; and Stoyanov, V. 2019.
\newblock {RoBERTa}: A robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692} .

\bibitem[{Lu et~al.(2019)Lu, Batra, Parikh, and Lee}]{lu2019vilbert}
Lu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019.
\newblock ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations
  for Vision-and-Language Tasks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 13--23.

\bibitem[{McNemar(1947)}]{mcnemar1947note}
McNemar, Q. 1947.
\newblock Note on the sampling error of the difference between correlated
  proportions or percentages.
\newblock \emph{Psychometrika} 12(2): 153--157.

\bibitem[{Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer}]{peters2018deep}
Peters, M.~E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and
  Zettlemoyer, L. 2018.
\newblock Deep contextualized word representations.
\newblock In \emph{NAACL-HLT}, 2227--2237.

\bibitem[{Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever}]{radford2018improving}
Radford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I. 2018.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{Technical report} .

\bibitem[{Rajpurkar, Jia, and Liang(2018)}]{Rajpurkar2018Know}
Rajpurkar, P.; Jia, R.; and Liang, P. 2018.
\newblock Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, 784--789.

\bibitem[{Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang}]{Rajpurkar2016SQuAD}
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
\newblock {SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, 2383--2392.

\bibitem[{Reddy et~al.(2020)Reddy, Sultan, Kayi, Zhang, Castelli, and
  Sil}]{reddy2020answer}
Reddy, R.~G.; Sultan, M.~A.; Kayi, E.~S.; Zhang, R.; Castelli, V.; and Sil, A.
  2020.
\newblock Answer Span Correction in Machine Reading Comprehension.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}.

\bibitem[{Reddy, Chen, and Manning(2019)}]{reddy2019coqa}
Reddy, S.; Chen, D.; and Manning, C.~D. 2019.
\newblock {C}o{QA}: A Conversational Question Answering Challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics}
  7: 249--266.

\bibitem[{Seo et~al.(2016)Seo, Kembhavi, Farhadi, and
  Hajishirzi}]{Seo2016Bidirectional}
Seo, M.; Kembhavi, A.; Farhadi, A.; and Hajishirzi, H. 2016.
\newblock Bidirectional Attention Flow for Machine Comprehension.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Tay et~al.(2018)Tay, Luu, Hui, and Su}]{tay2018densely}
Tay, Y.; Luu, A.~T.; Hui, S.~C.; and Su, J. 2018.
\newblock Densely connected attention propagation for reading comprehension.
\newblock In \emph{Advances in neural information processing systems},
  4906--4917.

\bibitem[{Trischler et~al.(2017)Trischler, Wang, Yuan, Harris, Sordoni,
  Bachman, and Suleman}]{trischler2017newsqa}
Trischler, A.; Wang, T.; Yuan, X.; Harris, J.; Sordoni, A.; Bachman, P.; and
  Suleman, K. 2017.
\newblock {N}ews{QA}: A Machine Comprehension Dataset.
\newblock In \emph{Proceedings of the 2nd Workshop on Representation Learning
  for {NLP}}, 191--200.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{DBLP:conf/nips/VaswaniSPUJGKP17}
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.~N.;
  Kaiser, L.; and Polosukhin, I. 2017.
\newblock Attention is All you Need.
\newblock \emph{Advances in neural information processing systems} 30:
  5998--6008.

\bibitem[{Wang and Jiang(2017)}]{DBLP:conf/iclr/Wang017a}
Wang, S.; and Jiang, J. 2017.
\newblock Machine Comprehension Using Match-LSTM and Answer Pointer.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Wang et~al.(2017)Wang, Yang, Wei, Chang, and Zhou}]{Wang2017Gated}
Wang, W.; Yang, N.; Wei, F.; Chang, B.; and Zhou, M. 2017.
\newblock Gated Self-Matching Networks for Reading Comprehension and Question
  Answering.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 189--198.

\bibitem[{Weissenborn(2017)}]{DBLP:journals/corr/Weissenborn17}
Weissenborn, D. 2017.
\newblock Reading Twice for Natural Language Understanding.
\newblock \emph{CoRR} abs/1706.02596.

\bibitem[{Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun,
  Cao, Gao, Macherey et~al.}]{wu2016google}
Wu, Y.; Schuster, M.; Chen, Z.; Le, Q.~V.; Norouzi, M.; Macherey, W.; Krikun,
  M.; Cao, Y.; Gao, Q.; Macherey, K.; et~al. 2016.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock \emph{arXiv preprint arXiv:1609.08144} .

\bibitem[{Xu, Zhao, and Zhang(2021)}]{xu2020topic}
Xu, Y.; Zhao, H.; and Zhang, Z. 2021.
\newblock Topic-aware multi-turn dialogue modeling.
\newblock In \emph{The Thirty-Fifth AAAI Conference on Artificial Intelligence
  (AAAI-21)}.

\bibitem[{Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le}]{yang2019xlnet}
Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R.~R.; and Le, Q.~V.
  2019.
\newblock {XLNET}: Generalized autoregressive pretraining for language
  understanding.
\newblock In \emph{Advances in neural information processing systems},
  5753--5763.

\bibitem[{Zhang et~al.(2020{\natexlab{a}})Zhang, Zhao, Wu, Zhang, Zhou, and
  Zhou}]{dcmn20}
Zhang, S.; Zhao, H.; Wu, Y.; Zhang, Z.; Zhou, X.; and Zhou, X.
  2020{\natexlab{a}}.
\newblock {DCMN}+: Dual Co-Matching Network for Multi-choice Reading
  Comprehension.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}
  34(05): 9563--9570.

\bibitem[{Zhang, Huang, and Zhao(2018)}]{zhang2018mrc}
Zhang, Z.; Huang, Y.; and Zhao, H. 2018.
\newblock Subword-augmented Embedding for Cloze Reading Comprehension.
\newblock In \emph{Proceedings of the 27th International Conference on
  Computational Linguistics (COLING 2018)}, 1802--–1814.

\bibitem[{Zhang et~al.(2018)Zhang, Li, Zhu, Zhao, and Liu}]{zhang2018modeling}
Zhang, Z.; Li, J.; Zhu, P.; Zhao, H.; and Liu, G. 2018.
\newblock Modeling Multi-turn Conversation with Deep Utterance Aggregation.
\newblock In \emph{Proceedings of the 27th International Conference on
  Computational Linguistics}, 3740--3752.

\bibitem[{Zhang et~al.(2019)Zhang, Wu, Li, and Zhao}]{zhang2019explicit}
Zhang, Z.; Wu, Y.; Li, Z.; and Zhao, H. 2019.
\newblock Explicit Contextual Semantics for Text Comprehension.
\newblock In \emph{Proceedings of the 33rd Pacific Asia Conference on Language,
  Information and Computation (PACLIC 33)}.

\bibitem[{Zhang et~al.(2020{\natexlab{b}})Zhang, Wu, Zhao, Li, Zhang, Zhou, and
  Zhou}]{zhang2019semantics}
Zhang, Z.; Wu, Y.; Zhao, H.; Li, Z.; Zhang, S.; Zhou, X.; and Zhou, X.
  2020{\natexlab{b}}.
\newblock Semantics-aware {BERT} for language understanding.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}
  34(05): 9628--9635.

\bibitem[{Zhang et~al.(2020{\natexlab{c}})Zhang, Wu, Zhou, Duan, Zhao, and
  Wang}]{zhang2019sg}
Zhang, Z.; Wu, Y.; Zhou, J.; Duan, S.; Zhao, H.; and Wang, R.
  2020{\natexlab{c}}.
\newblock {SG-Net}: Syntax-Guided Machine Reading Comprehension.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}
  34(05): 9628--9635.

\bibitem[{Zhang, Zhao, and Wang(2020)}]{zhang2020mrc}
Zhang, Z.; Zhao, H.; and Wang, R. 2020.
\newblock Machine Reading Comprehension: The Role of Contextualized Language
  Models and Beyond.
\newblock \emph{arXiv preprint arXiv:2005.06249} .

\bibitem[{Zheng et~al.(2019)Zheng, Mao, Liu, Ye, Zhang, and
  Ma}]{DBLP:conf/sigir/ZhengMLYZM19}
Zheng, Y.; Mao, J.; Liu, Y.; Ye, Z.; Zhang, M.; and Ma, S. 2019.
\newblock Human Behavior Inspired Machine Reading Comprehension.
\newblock In \emph{Proceedings of the 42nd International ACM SIGIR Conference
  on Research and Development in Information Retrieval}, 425--434.

\bibitem[{Zhou, Zhang, and Zhao(2019)}]{zhou2019limit}
Zhou, J.; Zhang, Z.; and Zhao, H. 2019.
\newblock {LIMIT-BERT}: Linguistic Informed Multi-Task BERT.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, 4450--4461.

\bibitem[{Zhu et~al.(2018)Zhu, Zhang, Li, Huang, and Zhao}]{zhu2018lingke}
Zhu, P.; Zhang, Z.; Li, J.; Huang, Y.; and Zhao, H. 2018.
\newblock Lingke: A Fine-grained Multi-turn Chatbot for Customer Service.
\newblock In \emph{Proceedings of the 27th International Conference on
  Computational Linguistics (COLING 2018), System Demonstrations}, 108--–112.

\bibitem[{Zhu, Zhao, and Li(2020)}]{zhu2020dual}
Zhu, P.; Zhao, H.; and Li, X. 2020.
\newblock Dual multi-head co-attention for multi-choice reading comprehension.
\newblock \emph{arXiv preprint arXiv:2001.09415} .

\bibitem[{Ziser and Reichart(2017)}]{ziser2016neural}
Ziser, Y.; and Reichart, R. 2017.
\newblock Neural Structural Correspondence Learning for Domain Adaptation.
\newblock In \emph{Proceedings of the 21st Conference on Computational Natural
  Language Learning (CoNLL 2017)}, 400--410.

\end{thebibliography}
