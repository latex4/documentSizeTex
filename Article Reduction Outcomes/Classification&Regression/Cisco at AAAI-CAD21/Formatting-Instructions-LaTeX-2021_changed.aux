\relax 
\bibstyle{aaai21}
\citation{shirani2019learning}
\citation{shirani2020let}
\citation{shirani-etal-2020-semeval}
\citation{shirani2021CAD21}
\citation{liu2019RoBERTa}
\citation{yang2019XLNet}
\citation{singhal2020iitk,anand2020midas}
\citation{shirani2019learning}
\citation{singhal2020iitk}
\citation{kullback1951information}
\citation{geng2016label}
\citation{shirani2019learning}
\citation{augenstein2017semeval,zhang2016keyphrase}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig1}{{1}{1}{The left slide is plain text. The right side shows the important emphasized words in the slide.}{}{}}
\citation{mishra2012word,chen2017automatic}
\citation{shirani2019learning}
\citation{shirani2020let}
\citation{shirani2019learning}
\citation{pennington2014glove}
\citation{huang2020ernie}
\citation{singhal2020iitk}
\citation{anand2020midas}
\citation{laws2011active}
\citation{10.1145/3178876.3186033,rodrigues2017deep,Rodrigues2013SequenceLW}
\citation{shirani2019learning}
\citation{geng2016label}
\citation{shirani2021CAD21}
\newlabel{table: 1}{{1}{2}{Train, Development and Test Dataset Description}{}{}}
\newlabel{table:2}{{2}{2}{Token length description}{}{}}
\citation{DBLP:journals/corr/abs-1802-05365}
\citation{sahrawat2019keyphrase}
\citation{shirani2019learning}
\citation{peters2018deep}
\citation{singhal2020iitk}
\citation{singhal2020iitk}
\newlabel{tab:my_label}{{3}{3}{Annotation scheme and emphasis probability calculation on a sample sentence from the Train dataset.}{}{}}
\newlabel{fig1}{{2}{3}{Word Cloud of tokens having emphasis probability of $\geq 0.5$}{}{}}
\newlabel{table:3}{{4}{3}{Average Emphasis Scores and Count}{}{}}
\newlabel{table:4}{{5}{3}{Keyphrase Extraction Model Results}{}{}}
\newlabel{fig2}{{3}{4}{The BiLSTM-ELMo Model}{}{}}
\citation{shirani2019learning}
\citation{shirani2019learning}
\citation{shirani2020let}
\newlabel{The Transformer-Based Model}{{4}{5}{The Transformer-Based Model}{}{}}
\newlabel{table:5}{{6}{5}{Performance of BiLSTM-ELMo and Transformers approach on development and Test set. The results are expressed in terms of average $Match_m$ for m $\in \{1,5,10\}$. LDL indicates that label distribution learning was employed to train the model with KL-Divergence as the loss function, Binary Cross Entropy otherwise. For BiLSTM-ELMo model the extra features concatenated at the attention layer have been mentioned with each experiment. Baseline. indicates the scores by the baseline model defined by \citet  {shirani2019learning}}{}{}}
\citation{luo2019hierarchical}
\bibdata{ex}
\newlabel{table:6}{{7}{6}{Performance of different ensemble models}{}{}}
\newlabel{table:7}{{8}{6}{Sentence-wise results on the Development set}{}{}}
\newlabel{result_ex}{{5}{6}{Emphasis Heatmaps i) Ground Truth ii) BiLSTM-ELMo iii) XLNet iv) Best Ensemble Model}{}{}}
\newlabel{table:8}{{9}{6}{Average $Match_m$ for best performing XLNet model on different size of instances in the development set}{}{}}
\newlabel{table:9}{{10}{6}{POS tags vs. average emphasis on development dataset}{}{}}
\gdef \@abspage@last{7}
