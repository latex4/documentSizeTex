\relax 
\bibstyle{aaai24}
\citation{zhang2021textoir,zhang2021discovering}
\citation{10.1145/3503161.3547906,saha-etal-2020-towards}
\citation{tsai2019multimodal,10.1145/3394171.3413678,rahman2020integrating}
\citation{dong2022improving}
\citation{yu-etal-2023-speech}
\citation{devlin2018bert}
\citation{sohn2016improved}
\citation{10.1145/3503161.3547906}
\citation{saha-etal-2020-towards}
\citation{zadeh2017tensor,liu2018efficient,hou2019deep}
\citation{zadeh2018memory}
\citation{tsai2019multimodal}
\citation{10.1145/3394171.3413678}
\citation{rahman2020integrating}
\citation{devlin2018bert}
\citation{han2021improving}
\citation{han2021bi}
\citation{paraskevopoulos2022mmlatch}
\citation{zhou2022learning}
\citation{zhou2022conditional}
\citation{rao2022denseclip}
\citation{Wang_2022_CVPR,Li_2022_CVPR,gan2023decorate}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{TCL_MAP}{{1}{2}{ \label {TCL_MAP} The overview architecture of TCL-MAP. In the Prompt-Based Augmentation module, we first create the modality-aware prompt using multimodal features, and then concatenate text tokens, prompt tokens and [MASK]/Label token to construct augmented pair. In the Representation Learning module, we extract the refined tokens for classification and conduct contrastive learning between the [MASK] token and the Label token. }{}{}}
\citation{wu2018unsupervised,ye2019unsupervised,tian2020contrastive}
\citation{he2020momentum}
\citation{chen2020simple}
\citation{grill2020bootstrap}
\citation{chen2021exploring}
\citation{caron2021emerging}
\citation{10349963,10097558}
\citation{devlin2018bert}
\citation{Liu_2021_ICCV}
\citation{5206848}
\citation{NEURIPS2020_92d1e1eb}
\citation{zhou2022learning,zhou2022conditional,rao2022denseclip}
\citation{zhou2022conditional,rao2022denseclip,10.1145/1143844.1143891}
\citation{10.1145/1143844.1143891}
\citation{tsai2019multimodal}
\citation{zhang2021deep}
\citation{rahman2020integrating}
\citation{devlin2018bert}
\citation{sohn2016improved}
\newlabel{MAP}{{2}{4}{\label {MAP} The details of Modality-Aware Prompting (MAP) module. We align multimodal features based on the content by computing the similarity matrix as weights and enhance correlations between modalities through a cross-modality transformer to create the modality-aware prompt. }{}{}}
\citation{10.1145/3503161.3547906}
\citation{saha-etal-2020-towards}
\citation{10.1145/3503161.3547906}
\citation{rahman2020integrating}
\citation{tsai2019multimodal}
\citation{10.1145/3394171.3413678}
\citation{wolf2019huggingface}
\citation{5206848}
\citation{torchvision2016}
\citation{loshchilov2017decoupled}
\newlabel{results}{{1}{5}{\label {results} Multimodal intent recognition results on the MIntRec dataset and the MELD-DA dataset. $\Delta $ represents the maximum enhancement attained by our method compared to the baseline across the evaluation metrics.}{}{}}
\newlabel{results_each_classes}{{2}{5}{\label {results_each_classes} F1-Score (\%) comparison between baselines and our method for each class of MIntRec. For the results of our method, bold indicates the best performance while underlining indicates the second best performance within each class. }{}{}}
\citation{10.1145/1143844.1143891}
\newlabel{ablation}{{3}{6}{\label {ablation} Ablation experiments of modules in TCL-MAP on the MIntRec dataset and the MELD-DA dataset. SBMA stands for Similarity-Based Modality Alignment, MAP stands for Modality-Aware Prompt and TCL stands for Token-Level Contrastive Learning. With SBMA incorporated into MAP, there exist three distinct settings.}{}{}}
\citation{zhou2022conditional}
\newlabel{analyze}{{3}{7}{\label {analyze} The comparison between Handcraft Prompt and Modality-Aware Prompt }{}{}}
\bibdata{aaai24}
\gdef \@abspage@last{8}
