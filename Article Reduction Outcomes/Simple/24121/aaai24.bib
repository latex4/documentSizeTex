% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{grosz-sidner-1986-attention,
    title = "Attention, Intentions, and the Structure of Discourse",
    author = "Grosz, Barbara J.  and
      Sidner, Candace L.",
    journal = "Computational Linguistics",
    volume = "12",
    number = "3",
    year = "1986",
    url = "https://aclanthology.org/J86-3001",
    pages = "175--204",
}
@inproceedings{joty-etal-2018-coherence,
    title = "Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach",
    author = "Joty, Shafiq  and
      Mohiuddin, Muhammad Tasnim  and
      Tien Nguyen, Dat",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1052",
    doi = "10.18653/v1/P18-1052",
    pages = "558--568",
    abstract = "We propose a novel coherence model for written asynchronous conversations (e.g., forums, emails), and show its applications in coherence assessment and thread reconstruction tasks. We conduct our research in two steps. First, we propose improvements to the recently proposed neural entity grid model by lexicalizing its entity transitions. Then, we extend the model to asynchronous conversations by incorporating the underlying conversational structure in the entity grid representation and feature computation. Our model achieves state of the art results on standard coherence assessment tasks in monologue and conversations outperforming existing models. We also demonstrate its effectiveness in reconstructing thread structures.",
}




@article{grosz-etal-1995-centering,
    title = "{C}entering: A Framework for Modeling the Local Coherence of Discourse",
    author = "Grosz, Barbara J.  and
      Joshi, Aravind K.  and
      Weinstein, Scott",
    journal = "Computational Linguistics",
    volume = "21",
    number = "2",
    year = "1995",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J95-2003",
    pages = "203--225",
}

@inproceedings{mohiuddin-etal-2021-rethinking,
    title = "Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks",
    author = "Mohiuddin, Tasnim  and
      Jwalapuram, Prathyusha  and
      Lin, Xiang  and
      Joty, Shafiq",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.308",
    doi = "10.18653/v1/2021.eacl-main.308",
    pages = "3528--3539",
    abstract = "Although coherence modeling has come a long way in developing novel models, their evaluation on downstream applications for which they are purportedly developed has largely been neglected. With the advancements made by neural approaches in applications such as machine translation (MT), summarization and dialog systems, the need for coherence evaluation of these tasks is now more crucial than ever. However, coherence models are typically evaluated only on synthetic tasks, which may not be representative of their performance in downstream applications. To investigate how representative the synthetic tasks are of downstream use cases, we conduct experiments on benchmarking well-known traditional and neural coherence models on synthetic sentence ordering tasks, and contrast this with their performance on three downstream applications: coherence evaluation for MT and summarization, and next utterance prediction in retrieval-based dialog. Our results demonstrate a weak correlation between the model performances in the synthetic tasks and the downstream applications, motivating alternate training and evaluation methods for coherence models.",
}




@article{mann1988rhetorical,
  title={Rhetorical Structure Theory: Toward a Functional Theory of Text Organization},
  author={Mann, William C. and Thompson, Sandra A.},
  journal={Text},
  volume={8},
  number={3},
  pages={243--281},
  year={1988},
  publisher={Walter de Gruyter GmbH}
}

@book{revuz2013continuous,
  title = {Continuous Martingales and Brownian Motion},
  author = {Revuz, Daniel and Yor, Marc},
  year = {2013},
  publisher = {Springer Science \& Business Media},
  volume = {293}
}


@article{attention-2017,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention Is All You Need},
  journal      = {CoRR},
  volume       = {abs/1706.03762},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03762},
  eprinttype    = {arXiv},
  eprint       = {1706.03762},
  timestamp    = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{kintsch1978toward,
  title={Toward a model of text comprehension and production},
  author={Kintsch, Walter and van Dijk, Teun A},
  journal={Psychological Review},
  volume={85},
  pages={363--394},
  year={1978},
  publisher={American Psychological Association}
}

@article{agar1982interpreting,
  title={Interpreting discourse: coherence and the analysis of ethnographic interview},
  author={Agar, Michael and Hobbs, Jerry R},
  journal={Discourse Processes},
  volume={5},
  pages={1--32},
  year={1982},
  publisher={Taylor \& Francis}
}


@article{glosser1992comparison,
  title={A comparison of changes in macrolinguistic and microlinguistic aspects of discourse production in normal aging},
  author={Glosser, Guila and Deser, Thalia},
  journal={Journal of Gerontology: Psychological Sciences},
  volume={47},
  pages={266--272},
  year={1992},
  publisher={Oxford University Press}
}



@misc{gao2022simcse,
      title={SimCSE: Simple Contrastive Learning of Sentence Embeddings}, 
      author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
      year={2022},
      eprint={2104.08821},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}




@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@misc{deng2022model,
      title={Model Criticism for Long-Form Text Generation}, 
      author={Yuntian Deng and Volodymyr Kuleshov and Alexander M. Rush},
      year={2022},
      eprint={2210.08444},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@misc{bowman2016generating,
      title={Generating Sentences from a Continuous Space}, 
      author={Samuel R. Bowman and Luke Vilnis and Oriol Vinyals and Andrew M. Dai and Rafal Jozefowicz and Samy Bengio},
      year={2016},
      eprint={1511.06349},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@misc{wang2023language,
      title={Language modeling via stochastic processes}, 
      author={Rose E Wang and Esin Durmus and Noah Goodman and Tatsunori Hashimoto},
      year={2023},
      eprint={2203.11370},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}




@article{kehler2022coherence,
  title={Coherence Establishment as a Source of Explanation in Linguistic Theory},
  author={Kehler, Andrew},
  journal={Annual Review of Linguistics},
  year={2022},
  volume={8},
  number={},
  pages={123-142},
  month={January},
  doi={10.1146/annurev-linguistics-011619-030357},
  url={https://doi.org/10.1146/annurev-linguistics-011619-030357},
}





@article{wang2014short,
  title={A short analysis of discourse coherence},
  author={Wang, Yuan and Guo, Minghe},
  journal={Journal of Language Teaching and Research},
  volume={5},
  number={2},
  pages={460},
  year={2014},
  publisher={Citeseer}
}



@inproceedings{John:2021,
	abstract = {The Brownian bridge is a method for probabilistically interpolating the location of a moving person, animal, or object between two measured points. This type of probabilistic interpolation is useful, because it represents the uncertainty of the interpolated points. It can be used to infer the probability of having visited a certain location, including possible exposure to disease. In the class of probabilistic interpolators, the Brownian bridge is attractive, because it has only a single adjustable parameter, the diffusion coefficient. This paper investigates the suitability of the Brownian bridge for interpolating human locations using mobility data from over 12 million people. One section looks at the consistency of the diffusion coefficient from person to person. As part of this, the paper presents, for the first time, a closed form solution for the maximum likelihood estimate of this parameter. The paper also presents statistical tests aimed at evaluating the accuracy of the Brownian bridge for interpolating human location.},
	address = {New York, NY, USA},
	author = {Krumm, John},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	date-added = {2023-04-10 23:46:18 -0500},
	date-modified = {2023-04-10 23:46:27 -0500},
	doi = {10.1145/3474717.3483942},
	isbn = {9781450386647},
	keywords = {location interpolation, probabilistic location, brownian bridge, human mobility},
	location = {Beijing, China},
	numpages = {9},
	pages = {175--183},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Brownian Bridge Interpolation for Human Mobility?},
	url = {https://doi.org/10.1145/3474717.3483942},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1145/3474717.3483942}}

@article{Horne:2007aa,
	abstract = {By studying animal movements, researchers can gain insight into many of the ecological characteristics and processes important for understanding population-level dynamics. We developed a Brownian bridge movement model (BBMM) for estimating the expected movement path of an animal, using discrete location data obtained at relatively short time intervals. The BBMM is based on the properties of a conditional random walk between successive pairs of locations, dependent on the time between locations, the distance between locations, and the Brownian motion variance that is related to the animal's mobility. We describe two critical developments that enable widespread use of the BBMM, including a derivation of the model when location data are measured with error and a maximum likelihood approach for estimating the Brownian motion variance. After the BBMM is fitted to location data, an estimate of the animal's probability of occurrence can be generated for an area during the time of observation. To illustrate potential applications, we provide three examples: estimating animal home ranges, estimating animal migration routes, and evaluating the influence of fine-scale resource selection on animal movement patterns.},
	address = {University of Idaho, Department of Fish and Wildlife, Moscow, Idaho 83844, USA. jhorne@uidaho.edu},
	author = {Horne, Jon S and Garton, Edward O and Krone, Stephen M and Lewis, Jesse S},
	crdt = {2007/10/09 09:00},
	date = {2007 Sep},
	date-added = {2023-04-10 23:45:05 -0500},
	date-modified = {2023-04-10 23:45:05 -0500},
	dcom = {20071126},
	doi = {10.1890/06-0957.1},
	edat = {2007/10/09 09:00},
	issn = {0012-9658 (Print); 0012-9658 (Linking)},
	jid = {0043541},
	journal = {Ecology},
	jt = {Ecology},
	language = {eng},
	lr = {20220409},
	mh = {*Animal Migration; Animals; *Animals, Wild; *Ecosystem; Likelihood Functions; *Models, Biological; Population Density; Population Dynamics; *Spatial Behavior; Stochastic Processes},
	mhda = {2007/12/06 09:00},
	month = {Sep},
	number = {9},
	own = {NLM},
	pages = {2354--2363},
	phst = {2007/10/09 09:00 {$[$}pubmed{$]$}; 2007/12/06 09:00 {$[$}medline{$]$}; 2007/10/09 09:00 {$[$}entrez{$]$}},
	pl = {United States},
	pmid = {17918412},
	pst = {ppublish},
	pt = {Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.},
	sb = {IM},
	status = {MEDLINE},
	title = {Analyzing animal movements using Brownian bridges.},
	volume = {88},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1890/06-0957.1}}

@article{chow2009brownian,
  title={Brownian bridge},
  author={Chow, Winston C},
  journal={Wiley interdisciplinary reviews: computational statistics},
  volume={1},
  number={3},
  pages={325--332},
  year={2009},
  publisher={Wiley Online Library}
}



@article{van1985semantic,
  title={Semantic discourse analysis},
  author={Van Dijk, Teun A},
  journal={Handbook of discourse analysis},
  volume={2},
  pages={103--136},
  year={1985},
  publisher={London: Academic Press}
}


@article{arnold-etal-2019-sector,
    title = "{SECTOR}: A Neural Model for Coherent Topic Segmentation and Classification",
    author = {Arnold, Sebastian  and
      Schneider, Rudolf  and
      Cudr{\'e}-Mauroux, Philippe  and
      Gers, Felix A.  and
      L{\"o}ser, Alexander},
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1011",
    doi = "10.1162/tacl_a_00261",
    pages = "169--184",
    abstract = "When searching for information, a human reader first glances over a document, spots relevant sections, and then focuses on a few sentences for resolving her intention. However, the high variance of document structure complicates the identification of the salient topic of a given section at a glance. To tackle this challenge, we present SECTOR, a model to support machine reading systems by segmenting documents into coherent sections and assigning topic labels to each section. Our deep neural network architecture learns a latent topic embedding over the course of a document. This can be leveraged to classify local topics from plain text and segment a document at topic shifts. In addition, we contribute WikiSection, a publicly available data set with 242k labeled sections in English and German from two distinct domains: diseases and cities. From our extensive evaluation of 20 architectures, we report a highest score of 71.6{\%} F1 for the segmentation and classification of 30 topics from the English city domain, scored by our SECTOR long short-term memory model with Bloom filter embeddings and bidirectional segmentation. This is a significant improvement of 29.5 points F1 over state-of-the-art CNN classifiers with baseline segmentation.",
}
@inproceedings{barzilay-lapata-2005-modeling,
    title = "Modeling Local Coherence: An Entity-Based Approach",
    author = "Barzilay, Regina  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05)",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P05-1018",
    doi = "10.3115/1219840.1219858",
    pages = "141--148",
}
@inproceedings{moon-etal-2019-unified,
    title = "A Unified Neural Coherence Model",
    author = "Moon, Han Cheol  and
      Mohiuddin, Tasnim  and
      Joty, Shafiq  and
      Xu, Chi",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1231",
    doi = "10.18653/v1/D19-1231",
    pages = "2262--2272",
    abstract = "Recently, neural approaches to coherence modeling have achieved state-of-the-art results in several evaluation tasks. However, we show that most of these models often fail on harder tasks with more realistic application scenarios. In particular, the existing models underperform on tasks that require the model to be sensitive to local contexts such as candidate ranking in conversational dialogue and in machine translation. In this paper, we propose a unified coherence model that incorporates sentence grammar, inter-sentence coherence relations, and global coherence patterns into a common neural framework. With extensive experiments on local and global discrimination tasks, we demonstrate that our proposed model outperforms existing models by a good margin, and establish a new state-of-the-art.",
}
@inproceedings{lai-tetreault-2018-discourse,
    title = "Discourse Coherence in the Wild: A Dataset, Evaluation and Methods",
    author = "Lai, Alice  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5023",
    doi = "10.18653/v1/W18-5023",
    pages = "214--223",
    abstract = "To date there has been very little work on assessing discourse coherence methods on real-world data. To address this, we present a new corpus of real-world texts (GCDC) as well as the first large-scale evaluation of leading discourse coherence algorithms. We show that neural models, including two that we introduce here (SentAvg and ParSeq), tend to perform best. We analyze these performance differences and discuss patterns we observed in low coherence texts in four domains.",
}
@inproceedings{tien-nguyen-joty-2017-neural,
    title = "A Neural Local Coherence Model",
    author = "Tien Nguyen, Dat  and
      Joty, Shafiq",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1121",
    doi = "10.18653/v1/P17-1121",
    pages = "1320--1330",
    abstract = "We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.",
}
@inproceedings{mesgar-strube-2018-neural,
    title = "A Neural Local Coherence Model for Text Quality Assessment",
    author = "Mesgar, Mohsen  and
      Strube, Michael",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1464",
    doi = "10.18653/v1/D18-1464",
    pages = "4328--4339",
    abstract = "We propose a local coherence model that captures the flow of what semantically connects adjacent sentences in a text. We represent the semantics of a sentence by a vector and capture its state at each word of the sentence. We model what relates two adjacent sentences based on the two most similar semantic states, each of which is in one of the sentences. We encode the perceived coherence of a text by a vector, which represents patterns of changes in salient information that relates adjacent sentences. Our experiments demonstrate that our approach is beneficial for two downstream tasks: Readability assessment, in which our model achieves new state-of-the-art results; and essay scoring, in which the combination of our coherence vectors and other task-dependent features significantly improves the performance of a strong essay scorer.",
}
@inproceedings{mesgar-etal-2021-neural-graph,
    title = "A Neural Graph-based Local Coherence Model",
    author = "Mesgar, Mohsen  and
      Ribeiro, Leonardo F. R.  and
      Gurevych, Iryna",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.199",
    doi = "10.18653/v1/2021.findings-emnlp.199",
    pages = "2316--2321",
    abstract = "Entity grids and entity graphs are two frameworks for modeling local coherence. These frameworks represent entity relations between sentences and then extract features from such representations to encode coherence. The benefits of convolutional neural models for extracting informative features from entity grids have been recently studied. In this work, we study the benefits of Relational Graph Convolutional Networks (RGCN) to encode entity graphs for measuring local coherence. We evaluate our neural graph-based model for two benchmark coherence evaluation tasks: sentence ordering (SO) and summary coherence rating (SCR). The results show that our neural graph-based model consistently outperforms the neural grid-based model for both tasks. Our model performs competitively with a strong baseline coherence model, while our model uses 50{\%} fewer parameters. Our work defines a new, efficient, and effective baseline for local coherence modeling.",
}
@inproceedings{jeon-strube-2022-entity,
    title = "Entity-based Neural Local Coherence Modeling",
    author = "Jeon, Sungho  and
      Strube, Michael",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.537",
    doi = "10.18653/v1/2022.acl-long.537",
    pages = "7787--7805",
    abstract = "In this paper, we propose an entity-based neural local coherence model which is linguistically more sound than previously proposed neural coherence models. Recent neural coherence models encode the input document using large-scale pretrained language models. Hence their basis for computing local coherence are words and even sub-words. The analysis of their output shows that these models frequently compute coherence on the basis of connections between (sub-)words which, from a linguistic perspective, should not play a role. Still, these models achieve state-of-the-art performance in several end applications. In contrast to these models, we compute coherence on the basis of entities by constraining the input to noun phrases and proper names. This provides us with an explicit representation of the most important items in sentences leading to the notion of focus. This brings our model linguistically in line with pre-neural models of computing coherence. It also gives us better insight into the behaviour of the model thus leading to better explainability. Our approach is also in accord with a recent study (O{'}Connor and Andreas, 2021), which shows that most usable information is captured by nouns and verbs in transformer-based language models. We evaluate our model on three downstream tasks showing that it is not only linguistically more sound than previous models but also that it outperforms them in end applications.",
}
@inproceedings{elsner-charniak-2011-extending,
    title = "Extending the Entity Grid with Entity-Specific Features",
    author = "Elsner, Micha  and
      Charniak, Eugene",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-2022",
    pages = "125--129",
}
@article{barzilay-lapata-2008-modeling,
    title = "Modeling Local Coherence: An Entity-Based Approach",
    author = "Barzilay, Regina  and
      Lapata, Mirella",
    journal = "Computational Linguistics",
    volume = "34",
    number = "1",
    year = "2008",
    url = "https://aclanthology.org/J08-1001",
    doi = "10.1162/coli.2008.34.1.1",
    pages = "1--34",
}
@inproceedings{guinaudeau-strube-2013-graph,
    title = "Graph-based Local Coherence Modeling",
    author = "Guinaudeau, Camille  and
      Strube, Michael",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-1010",
    pages = "93--103",
}
@inproceedings{adewoyin-etal-2022-rstgen,
    title = "{RSTG}en: Imbuing Fine-Grained Interpretable Control into Long-{F}orm{T}ext Generators",
    author = "Adewoyin, Rilwan  and
      Dutta, Ritabrata  and
      He, Yulan",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.133",
    doi = "10.18653/v1/2022.naacl-main.133",
    pages = "1822--1835",
    abstract = "In this paper, we study the task of improving the cohesion and coherence of long-form text generated by language models.To this end, we propose RSTGen, a framework that utilises Rhetorical Structure Theory (RST), a classical language theory, to control the discourse structure, semantics and topics of generated text. Firstly, we demonstrate our model{'}s ability to control structural discourse and semantic features of generated text in open generation evaluation. Then we experiment on the two challenging long-form text tasks of argument generation and story generation. Evaluation using automated metrics and a metric with high correlation to human evaluation, shows that our model performs competitively against existing models, while offering significantly more controls over generated text than alternative methods.",
}

@inproceedings{laban-etal-2021-transformer,
    title = "Can Transformer Models Measure Coherence In Text: Re-Thinking the Shuffle Test",
    author = "Laban, Philippe  and
      Dai, Luke  and
      Bandarkar, Lucas  and
      Hearst, Marti A.",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.134",
    doi = "10.18653/v1/2021.acl-short.134",
    pages = "1058--1064",
    abstract = "The Shuffle Test is the most common task to evaluate whether NLP models can measure coherence in text. Most recent work uses direct supervision on the task; we show that by simply finetuning a RoBERTa model, we can achieve a near perfect accuracy of 97.8{\%}, a state-of-the-art. We argue that this outstanding performance is unlikely to lead to a good model of text coherence, and suggest that the Shuffle Test should be approached in a Zero-Shot setting: models should be evaluated without being trained on the task itself. We evaluate common models in this setting, such as Generative and Bi-directional Transformers, and find that larger architectures achieve high-performance out-of-the-box. Finally, we suggest the k-Block Shuffle Test, a modification of the original by increasing the size of blocks shuffled. Even though human reader performance remains high (around 95{\%} accuracy), model performance drops from 94{\%} to 78{\%} as block size increases, creating a conceptually simple challenge to benchmark NLP models.",
}

@article{hearst-1997-text,
    title = "Text Tiling: Segmenting Text into Multi-paragraph Subtopic Passages",
    author = "Hearst, Marti A.",
    journal = "Computational Linguistics",
    volume = "23",
    number = "1",
    year = "1997",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J97-1003",
    pages = "33--64",
}
