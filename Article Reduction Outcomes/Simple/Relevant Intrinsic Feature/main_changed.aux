\relax 
\bibstyle{aaai24}
\citation{long2015fully}
\citation{zhu2020deformable,xie2021segformer,yuan2021hrformer}
\citation{shaban2017one,liu2020part}
\citation{kang2022integrative}
\citation{tian2020prior,wang2019panet,zhang2019canet,zhang2020sg}
\citation{min2021hypercorrelation}
\citation{hong2022cost}
\citation{xiong2022doubly}
\citation{liu2020part}
\citation{tian2020prior}
\citation{zhang2021few}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{introduction}{{1}{1}{Comparison of RiFeNet and other works. (a) Two foreground-related issues that limit the effectiveness of previous research. (b) Schedule of previous prototype-based methods. The two-branch structure uses a single global prototype as the only medium for information interaction. (c) Framework of RiFeNet. An additional unlabeled branch is used for feature mining during training, with multi-granularity prototypes extracted from different branches. }{}{}}
\citation{long2015fully}
\citation{qin2022activation,qin2022multi,qin2023freeseg}
\citation{zhao2017pyramid,chen2018encoder,yang2018denseaspp,he2019adaptive}
\citation{fu2019dual,yuan2018ocnet,li2019expectation,tao2020hierarchical,zhu2019asymmetric,zhang2019acfnet}
\citation{lin2017refinenet,zhang2018context,yu2020context,jin2021mining}
\citation{dosovitskiy2020image}
\citation{yuan2021hrformer,wang2021pyramid,lee2022mpvit,xie2021segformer,strudel2021segmenter}
\citation{min2021hypercorrelation}
\citation{kang2022integrative}
\citation{hong2022cost}
\citation{xiong2022doubly}
\citation{dong2018few}
\citation{zhang2019canet,wang2019panet}
\citation{zhang2020sg}
\citation{tian2020prior}
\citation{liu2020part}
\citation{li2021adaptive}
\citation{yang2020prototype}
\citation{liu2022intermediate}
\citation{fan2022self}
\citation{liu2020part}
\citation{kim2023uncertainty}
\newlabel{overall}{{2}{3}{Overall architecture of RiFeNet. An additional branch using unlabeled inputs is attached to the traditional two-branch structure, as shown in green. The forward process of RiFeNet consists of three main blocks. The multi-level prototype generation block generates global and local prototypes. The multi-level prototype interaction module allows for interaction and integration between prototypes. A feature activation block is used consequently for obtaining final segmentation results.}{}{}}
\citation{milletari2016v}
\citation{fan2022self}
\newlabel{pre-process}{{1}{4}{}{}{}}
\newlabel{generation}{{3}{4}{Visual illustration of the multi-level prototype generation block that extracts global and local prototypes. }{}{}}
\newlabel{interaction}{{4}{4}{Schedule of the multi-level prototype interaction merging relevant intrinsic information for enhancement. }{}{}}
\citation{tian2020prior}
\citation{zhang2021few}
\citation{milletari2016v}
\citation{liu2020part}
\citation{yang2020prototype}
\citation{tian2020prior}
\citation{zhang2021few}
\citation{tao2020hierarchical}
\citation{li2021adaptive}
\citation{fan2022self}
\citation{shi2022dense}
\citation{wang2020few}
\citation{yang2020prototype}
\citation{tian2020prior}
\citation{zhang2021few}
\citation{tao2020hierarchical}
\citation{li2021adaptive}
\citation{fan2022self}
\citation{shi2022dense}
\citation{liu2020part}
\citation{yang2020prototype}
\citation{yang2020prototype}
\citation{zhang2021few}
\citation{tao2020hierarchical}
\citation{fan2022self}
\citation{shi2022dense}
\citation{he2016deep}
\newlabel{sotapascal}{{1}{6}{Performance comparison on PASCAL-$5^i$ in terms of mIoU ($\%$).}{}{}}
\newlabel{sotacoco}{{2}{6}{Performance comparison on COCO in terms of mIoU ($\%$).}{}{}}
\newlabel{segresult}{{5}{7}{Qualitative segmentation results on novel classes on PASCAL-$5^i$. From \textit  {left} to \textit  {right}: support image with mask, query input, query ground-truth mask, query prediction of the baseline, and prediction of RiFeNet.}{}{}}
\newlabel{ablation}{{3}{7}{Ablation studies on the key components of RiFeNet. ``Un'' and ``MP'' denote the use of the unlabeled branch and the multi-level prototypes, respectively.}{}{}}
\newlabel{design1}{{4}{7}{Ablation studies on multi-level prototypes. ``gp'' and ``lp'' denote global and local prototypes, respectively. That is, ``gp+gp'' means extracting both query and support prototypes globally. ``CA'' refers to channel-wise attention.}{}{}}
\newlabel{design2}{{5}{7}{Ablation studies on the unlabeled branch. ``w/ guide'' refers to the use of query local prototypes in the unlabeled branch for guidance, while ``w/o guide'' means using prototypes generated from the unlabeled branch itself.}{}{}}
\newlabel{unlabelnum}{{6}{7}{Ablation studies of different numbers of unlabeled images in the single meta-training process.}{}{}}
\bibdata{aaai24}
\gdef \@abspage@last{8}
