\relax 
\bibstyle{aaai24}
\citation{8719904,9412588}
\citation{10.1145/3077136.3080834,8830456}
\citation{BUSTOS2020101797}
\citation{Cherman2019,10.1145/3379504}
\citation{REYES2018494,10.1145/3379504}
\citation{Zhang2018}
\citation{mmc2009,adaptive2013,Cherman2019}
\citation{Su2021}
\citation{Wu_Lyu_Ghanem_2016}
\citation{Zhang2018}
\providecommand \oddpage@label [2]{}
\newlabel{sec:Intro}{{}{1}{}{}{}}
\citation{settles2009active,BEMPS_Wei_NEURIPS2011}
\citation{ren2020survey}
\citation{holub2008entropy}
\citation{ash2019deep}
\citation{ren2020survey,TanDuBun-IEEEPAMI23}
\citation{ash2019deep,TanDuBun-IEEEPAMI23}
\citation{Cherman2019,REYES2018494}
\citation{pmlr-v97-shi19b}
\citation{mmc2009}
\citation{pmlr-v20-hung11}
\citation{10.1007/978-3-319-97304-3_73,REYES2018494}
\citation{adaptive2013}
\citation{REYES2018494}
\citation{10.1007/978-3-319-93034-3_12}
\citation{gaupb2_2021}
\citation{roy2001toward}
\citation{settles2009active}
\citation{zhao2020uncertainty}
\citation{doi:10.1198/016214506000001437}
\citation{TanDuBun-IEEEPAMI23}
\citation{doi:10.1198/016214506000001437}
\citation{TanDuBun-IEEEPAMI23}
\citation{pmlr-v20-hung11}
\citation{TanDuBun-IEEEPAMI23}
\citation{dawid2014theory}
\newlabel{sec:Related}{{}{2}{}{}{}}
\newlabel{sec:Method}{{}{2}{}{}{}}
\newlabel{eq-QSS}{{1}{2}{}{}{}}
\newlabel{eq-DQS}{{2}{2}{}{}{}}
\citation{TanDuBun-IEEEPAMI23}
\citation{dawid2014theory}
\citation{Buja2005Loss}
\citation{merkle2013choosing}
\citation{merkle2013choosing,doi:10.1198/016214506000001437}
\citation{9319440}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:betagp_blt}{{1}{3}{ The graph depicts the Expected Score gp (green) and Scoring Functions from Eq\nobreakspace  {}(\ref {eq-Sbeta}) for the Beta family in blue (sp(., 0) when $y=0$ and orange (sp(., 1) when $y=1$). It covers six scenarios: three specific to Brier Score, Logarithmic Score, and total error approximations, and three emphasizing asymmetry with varied Beta values. }{}{}}
\newlabel{eq-3}{{3}{3}{}{}{}}
\newlabel{eq-4}{{4}{3}{}{}{}}
\newlabel{eq-Sbeta}{{5}{3}{}{}{}}
\citation{CHARTE20153}
\citation{wang2023imbalanced}
\citation{CHARTE20153}
\citation{LozaMenca2010}
\citation{10.5555/1005332.1005345}
\citation{Katakis2008MultilabelTC}
\citation{Tsoumakas2008EffectiveAE}
\citation{10.5555/2968618.2968710}
\citation{1559692}
\citation{kim-2014-convolutional}
\citation{8632592}
\citation{devlin-etal-2019-bert}
\citation{frankle2018lottery}
\citation{gal2017deep}
\citation{nguyen-etal-2022-hardness,Nguyen_Tan_Du_Buntine_Beare_Chen_2023,nguyen2023lowresource}
\citation{10.5555/3295222.3295387}
\newlabel{alg:alg-ensemble}{{1}{4}{Beta Scoring Rules For Deep Active Learning}{}{}}
\newlabel{tab:syndataset-table}{{1}{4}{Five datasets (including the test set) are created using the MeanIR values that represent the imbalance level of the multi-label datasets. Higher values suggest increased sparsity and imbalance.}{}{}}
\newlabel{sec:experiment}{{}{4}{}{}{}}
\newlabel{tab:dataset-table}{{2}{4}{Six benchmark datasets with their corresponding imbalance level MeanIR statistics. }{}{}}
\citation{mmc2009}
\citation{adaptive2013}
\citation{6729601}
\citation{REYES2018494}
\citation{pmlr-v20-hung11}
\citation{10.1007/978-3-319-93034-3_12}
\citation{gaupb2_2021}
\citation{Buja2005Loss,merkle2013choosing}
\citation{pmlr-v20-hung11}
\newlabel{fig:syn_rcv1meanIR50_mircof1}{{2}{5}{The average Micro F1-score of AL models with acquisition size 100 on BERT, which were run with 5 different random seeds on various synthetic datasets.}{}{}}
\citation{TanDuBun-IEEEPAMI23,NEURIPS2019_95323660}
\newlabel{fig:textBERT_mircof1_6}{{3}{6}{The average Micro F1-score of AL models with acquisition size 100 on BERT, which were run with 5 different random seeds on various datasets.}{}{}}
\newlabel{sec:ablation}{{}{6}{}{}{}}
\newlabel{fig:modelagnostic_mircof1_3}{{4}{6}{The average Micro F1-score of AL models with acquisition size 100 on TextCNN and TextRNN, which were run with 5 different random seeds on RCV1.}{}{}}
\newlabel{fig:ablationTextBERT_mircof1_6}{{5}{6}{The average Micro F1-score of AL models with acquisition size 100 on BERT, which were run with 5 different random seeds on for Bibtex and Yahoo (health).}{}{}}
\bibdata{aaai24}
\newlabel{fig:batchsize_mircof1_2}{{6}{7}{ Left: Learning curves of ALs with batch size 50 on RCV1. Right: Learning curves for BESRA with batch sizes $B \in \{50, 100\}$ on RCV1. All results were run with 5 different random seeds}{}{}}
\gdef \@abspage@last{8}
