\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\def\year{2020}\relax
\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{graphicx}  % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}



\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

\setlength\titlebox{2.5in} % If your paper contains an overfull \vbox too high warning at the beginning of the document, use this
\title{A Knowledge Driven Approach to Adaptive Assistance Using Preference Reasoning and Explanation}
\author{Jason R. Wilson\\Franklin \& Marshall College\\Lancaster, Pennsylvania\\jrw@fandm.edu \And Leilani Gilpin\\Massachusetts Institute of Technology\\Cambridge, Massachusetts\\lgilpin@mit.edu \And Irina Rabkina\\Occidental College\\Los Angeles, California\\irabkina@oxy.edu}

\begin{document}

\maketitle

\begin{abstract}
There is a need for socially assistive robots (SARs) to provide
transparency in their behavior by explaining their reasoning. Additionally,
the reasoning and explanation should represent the users preferences and
goals.
To work towards satisfying this need for interpretable
reasoning and representations, we propose the robot uses Analogical
Theory of Mind to infer what the user is trying to do and uses the
Hint Engine to find an appropriate assistance based on what the user
is trying to do.  If the user is unsure or confused, the robot
provides the user with an explanation, generated by the Explanation
Synthesizer.  The explanation helps the user understand what the robot
inferred about the users preferences and why the robot decided to
provide the assistance it gave.  A knowledge-driven approach provides
transparency to reasoning about preferences, assistance, and
explanations, thereby facilitating the incorporation
of user feedback and allowing the robot to learn and adapt to the user.
\end{abstract}

\section{Introduction}
Socially assistive robots (SARs) can aid humans in a variety of tasks.
One of the most compelling assistive tasks is in medication
management, where a SAR can instruct, record, and oversee a patient's
medication usage.  However, since this is a medical application, it is
important that a robot is robust, transparent, and open to user
feedback; especially for corrections.

However, SARs, like other social robots, are complex to understand.  Robots are built of many parts, with underlying language tools (e.g., for NLP, NLU, or NLG) that are  not inherently interpretable.  Therefore, SARs cannot
effectively communicate and collaborate with humans on tasks without \emph{explainability}.  This is
troublesome when the robot fails, or when the assistive application is
critical, like healthcare or medical applications.  In this paper, we
make two distinct contributions towards explainable SARs: (1) we
contribute a complex cognitive model for incorporating user feedback,
and (2) we show a proof-of-concept on a real-life medication case
study.  Our approach combines three components:
\begin{enumerate}
\item Preference Reasoning: The robot considers what it knows or can infer about the user's preferences, and how these affect possible actions.
\item Assistance: The robot interacts with the user and aids
them if deemed necessary.
\item Explanation: The robot explains its reasoning and validates its
recommendation and conclusion with a user.
\end{enumerate}
In this paper, we show the capabilities of a SAR with knowledge-driven
adaptive assistance.  We start with a detailed overview of the
medication sorting task.  We present the approach and initial results,
and conclude with future work, discussion, and a reiteration of the
contributions.

\section{Task Description}
\label{sec:task-description}
We consider a SAR that provides social assistance in a medication sorting task.  In this task, a person organizes a set of medications, vitamins, and other supplements.  Each has constraints, provided in the form of a prescription, doctor recommendation, or personal preference.  In the example we use in this paper, there is a vitamin to be taken each morning and a medication that needs to be taken prior to any physical activity.

The role of the robot is to observe the person while they are organizing the pills into a sorting grid, a device that has compartments for each day of the week and multiple time of day (see Figure~\ref{fig:grid}).  Our example uses a sorting grid for four times in the day: morning, noon, evening, and bedtime.

\begin{figure}[hb]
    \centering
    \includegraphics[width=0.8\columnwidth]{grid.jpg}
    \caption{An example of a sorting grid.}
    \label{fig:grid}
\end{figure}

Consider the scenario in which a person is to take a vitamin each morning and then Levodopa before any physical activity.  The robot knows that the person has a physical therapy appointment at 1pm on Wednesday and a dance class at 6pm on Friday.  The person has a preference that Levodopa is taken enough time before the activity for it to take effect.

After the person has placed one vitamin in the morning compartment for each day, they begin to figure out where to place some Levodopa.  The person hesitates, and the robot interjects with a suggestion:

\begin{itemize}[left=30pt]
    \item[Robot:] Try placing a Levodopa pill in the morning on Wednesday.
    \item[User:] Why?
    \item[Robot:] Levodopa needs to be taken before any physical activity, and you have a physical therapy appointment at 1pm on Wednesday.  Since you prefer to take it a few hours before activity, you should take it in the morning.
    \item[User:] Oh, right.  Thank you.
\end{itemize}

Alternatively, consider the case in which the person prefers to take Levodopa closer to when the activity is to occur.  In this case, the robot would explain:

\begin{itemize}[left=30pt]
    \item[Robot:] A Levodopa pill needs to be taken before any physical activity, and you have a physical therapy appointment at 1pm on Wednesday.  Since you prefer to take it immediately before activity, you should take it in the afternoon.
\end{itemize}

While previous work demonstrated the robot adapting its assistance based on how much assistance the person needed \cite{wilson2020challenges}, the current work looks at how the robot can reason about the person's preferences, thus adapting its assistance and explanation.

















\section{Approach}


For the social robot to provide assistance and explanations to the user, we propose an
architecture consisting of Preference Reasoning, Adaptive Assistance, and Explanation
components, as shown in Figure~\ref{fig:arch}.  The Adaptive Assistance component uses updates in the task and social cues conveyed by the user to generate a plan, which is used in determining what action the robot is to take to assist the user.  The plan would be sent to the Explanation component, along with the user preferences from the Preference Reasoning component, to generate an explanation for the robot's method of assistance. Each component is functional individually, but full integration is a work in progress

\begin{figure}[h]
\centering
\includegraphics[width=1.0\columnwidth]{architecture.png}
\caption{User preferences are sent to the Adaptive Assistance and Explanation components.  The Adaptive Assistance component uses the preference in generating a plan for completing the task, and the Explanation component uses the preference and plan to explain the robot's assistance.}
\label{fig:arch}
\end{figure}

\subsection{Preference Reasoning}
In the context of pill sorting, user preferences can take two forms: preferences about how to sort specific pills (e.g., take Levodopa directly before an activity vs. several hours before) and preferences about the sorting task as a whole (e.g., sort all of one type of pill for the week vs. each day in order). These preferences take the form:
 \small{
 \begin{verbatim}
(prefers user
    (medicationBeforeActivityBy
        medtype
        distance))
 \end{verbatim}}
 \noindent
 and

 \small{
 \begin{verbatim}(prefers user (sortOrder order))
  \end{verbatim}}
 \noindent
 respectively.

 Note that all preferences use the \texttt{prefers} predicate and take the user as the first argument. This representation allows us to generate preferences through both inference and user input. More importantly, it allows us to use the preferences for further reasoning, including in the Adaptive Assistance and Explanation components.

 In the present work, we assume that preferences are given by the user. This may be in the form of correcting the robot (e.g., "No, don't put that pill in the morning. I want to take it in the afternoon") or stated out right (e.g., "Let's start by sorting the green pills."). However, such preferences can also be inferred. We are working on integrating the Analogical Theory of Mind (AToM) ~\cite{rabkina2017towards} model into the architecture to do so.

 AToM is a computational cognitive model of the processes by which people learn to reason about others' preferences, goals, beliefs, desires, etc. (called theory of mind reasoning). It has successfully modeled children's learning in two developmental studies ~\cite{rabkina2017towards,rabkina2018bootstrapping} and has previously been used to recognize the goals and intentions of simulated agents in multiagent interactions ~\cite{rabkina2019analogical,rabkina2020acs}.

 We plan to use AToM because its reasoning is human-like, and therefore easy for people to understand. Furthermore, AToM can learn from just a handful of examples and can incorporate user feedback to improve its reasoning on the fly.

\subsection{Adaptive Assistance}

The Adaptive Assistance component uses a Hint Engine to generate an appropriate assistance to help the user complete the task~\cite{wilson2018general}.  The Hint Engine integrates information from three models (need, assistance, and domain) to determine how and when the robot should assist \cite{wilson2019developing}.  The need model is used to infer how much assistance the person needs based on progress in the task and social cues (e.g., verbal requests, eye gaze patterns).  The assistance model represents the relations between the types of actions the user can take to complete the task, the types of actions the robot can take to assist, and the amount of assistance provided in any robot action.

The domain model represents the task that the user is performing and is used to infer a plan, the actions the user can take to complete the task.  When information from the need model indicates that the user has a sufficient level of need, the Hint Engine generates a plan for completing the task.  The first action in the plan indicates where the robot should focus its assistance.  Based on the action type and how much assistance the user needs, the Hint Engine uses the assistance model to determine the appropriate assistive action for the robot to perform.

To generate a plan, the Hint Engine uses its domain model, which is represented with a hierarchical task network (HTN) \cite{nau1999shop}.  The HTN used for medication sorting is shown in Figure~\ref{fig:htn}.  At the top level, the user is working toward sorting the pills for all of the medications.  To complete the task, the user needs to go through each medication, sorting the pills for each. For a given medication, a user may go through the days of the week, adding and removing pills as they go.  If no mistakes have been made, the user is just missing pills (leading to \texttt{addPill} actions).  If a pill has been placed at the wrong time of the day, then the action pair \texttt{removePill} and \texttt{addPill} are included in the plan to indicate the moving of a pill.

\begin{figure}[h]
\centering
\includegraphics[width=0.99\columnwidth]{medsorting_htn.png}
\caption{Hierarchical task network representing a medication sorting task. It assumes task is done by sorting all pills of one medication before doing the next one. White boxes are tasks, and shaded boxes are operators.  }
\label{fig:htn}
\end{figure}

To determine whether there is a missing pill, an extra pill, or a pill placed at a wrong time, the planner % maybe mention the pyhop planner being used?
checks the preconditions of the method, which includes the constraints defined for the given medication.  For example, each medication defines the maximum number to be taken in a day.  If the number of pills for that medication exceed the maximum, allowed, then the \texttt{extraPill} condition is satisfied.

We extend the precondition checks to also consider user preferences.
For example, a vitamin could be taken at any time, but a user may prefer to take it in the morning.  Similarly, a medication like Levodopa might be taken before physical activity, and the user may have preferences regarding how long before the activity.  In this case, there is a constraint of the form \texttt{(beforeActivity pill row col activity)} that can be inferred with a rule like the following:

\small{
\begin{verbatim}
(beforeActivity pill row col activity) <-
    (activityAt activity rowX col)
    (isa pill med)
    (medicationBeforeActivityBy med
        distance)
    (difference rowX row distance)
\end{verbatim}}

In this example, the HTN represents blocks of time as they relate to the sorting grid (i.e., morning, noon, etc.) and not specific times in the day.











\subsection{Explanation}
The explanations are generated from an existing
system \cite{gilpin2018monitoring,leilanithesis} that incorporates commonsense
knowledge, rules, and constraint checking towards an explanation of
intended behavior.  The Explanation Synthesizer proceeds in 3 steps:
\begin{enumerate}
\item Parsing and aggregation: The input query is parsed for key \emph{concepts}.  Those
concepts are used as search terms in the commonsense knowledge base.  A list of facts (symbolic triples) is returned.
\item Constraint Checking: Commonsense rules are triggered to generate
new facts and evidence.
\item Synthesizing: Once all the facts are aggregated, an explanation
synthesizer constructs the most plausible chain of reasoning
towards an explanation.
\end{enumerate}

For example, consider the query \texttt{(pill onDate Friday)} which justifies that the user can take the pill on Friday.  The query is parsed and
the key concepts are \texttt{pill} and \texttt{Friday}, which are search terms for
the commonsense knowledge base (KB).  The KB returns facts like
\texttt{(Friday IsA 'business day')}.  The relation \texttt{onDate} is
used as a constraint in the system.

The constraints are a combination of commonsense rules and user
preferences.  These constraints are application dependent.   In the
medication sorting domain, they are rules related to the requirements
for each type of pill, as well as user preferences.  For example, the
user may prefer to take pills in the morning, or users may be
\emph{instructed} to ingest pills with meals or food.  The facts are
forward chained against these rules to generate new facts and
evidence.

After this process, there may be more than one plausible explanation
supporting the query.  The explanation synthesizer starts from the
query and constructs a goal tree to satisfy the query
\cite{leilanithesis}.  For this paper, we only examine one
explanation.  Choosing the best explanation may be explored in future
work.










\section{Proof of Concept}


To demonstrate our components adapting to and explaining with user preferences, we consider the scenario in which the user has already correctly placed all of their Vitamin D and is now working to sort their Levodopa, which is to be taken before activity.  The user has two activities planned for the week, Wednesday at 1pm and Friday at 8pm.  The user just placed one Levodopa pill in the space for Monday midday.

We assume that the user has a previously stated preference to take Levodopa during time slot prior to an activity.  When the user misplaces the Levodopa, the Adaptive Assistance component recognizes that the user needs assistance and generates a plan to
move the Monday pill to an earlier time slot:

\small{
\begin{verbatim}
(planFor state8
  ((preference beforeActivity 1))
  ((removePill Levodopa 3 1)
   (addPill Levodopa 3 0)
   (addPill Levodopa 5 2)))
\end{verbatim}}

The first action, \texttt{(removePill Levodopa 3 1)}, is used along with an inference of a level of assistance to determine that the robot should provide direct assistance, which has the robot clearly stating what should be done next.  In this case, the pill needs to be removed and the robot would say ``Try removing a Levodopa from Wednesday''.

Additionally, the Adaptive Assistance component considers alternative plans, a counterfactual reasoning over different preferences.  The alternative plans do not affect how the robot assists but would be sent to the Explanation component.  An alternative plan for taking Levodopa at the same time as the activity is shown below.
The plan indicates that the Monday pill is in the correct location and the only action remaining is to place the Friday pill:

\small{
\begin{verbatim}
(alternativePlanFor state8
  ((preference beforeActivity 0))
  ((addPill Levodopa 5 3)))
\end{verbatim}}


Finally, the user can inquire about the robot's actions.  For example, the user can ask why the robot said to ``Try removing a Levodopa from Wednesday.''  This question is parsed into an intermediate representation: \texttt{(onDate Levodopa Wednesday)}, which is passed to the Explanation Synthesizer along with the associated \emph{preference}; the user prefers to take the medication before an activity.
The following is a trace of the reasoning of the Explanation Synthesizer

\small{
\begin{verbatim}
[(IsA Levodopa pill), 'Given']
[(AtLocation pill cabinet), 'ConceptNet']
...
[(IsA Wednesday weekday), 'ConceptNet']
[(IsA Wednesday day), 'ConceptNet']
...
[(prefers user (before pill activity)),
    'Given preference']
[(IsA appt activity), 'Given knowledge']
[(atTime appt '1pm'), 'calendar']
[(onDay appt Wednesday), 'calendar']
[(atTime appt afternoon), 'Rule fired']
\end{verbatim}}

The justification is that \texttt{(onDay pill Wednesday) (beforeTime pill
afternoon)}.  And the final explanation reads in a series of symbolic
triples: \texttt{(prefers user (before pill activity)) (IsA user activity) (atTime appt '1pm') (onDay appt Wednesday) (IsA '1pm' afternoon)}.

\section{Future Work}
The work we describe here sets the foundation for a whole line of work in
designing social robots to adapt to users, adhere to user preferences, and
provide explanations.  The most immediate next step is to build upon the
proof of concept we have demonstrated here by integrating the individual
parts and evaluating the system on more complex scenarios.

Once we have a fully integrated system, the critical next step is to
incorporate feedback from the user.
One of the greatest advantages of taking a knowledge-driven approach is
that the entire system is inspectable, which will facilitate integrating
the user feedback.
The user may provide feedback after the robot provides an explanation to
the user.  An explanation synthesizer extracts key terms from the
feedback, interprets the feedback, and determines which component(s)
need adjustment.  The explanation synthesizer also \emph{validates} its
conclusion by verifying with the user.

In this ongoing work, if the explanation synthesizer identifies that the feedback is related
to the user preferences, the Preference Reasoner will construct a new
case that is used by AToM to update the model of the user.  Even a single
piece of feedback from the user can be sufficient for AToM to learn the
user's preferences because analogical learning, which is used by AToM, is
data efficient and capable of learning from only a few examples
\cite{chen2019human,wilson2019analogical}.


\section{Related Work}
Consideration of \textit{user preferences} is an important aspect of human-robot interaction, as it allows the robot to modify its behaviors according to its understanding of the user. Hiatt, Harrison, and Trafton (\citeyear{hiatt2011accommodating}) found that people prefer collaborating with robots that adapt their behaviors in this way. However, most approaches to recognizing users' preferences use statistical techniques like reinforcement learning \cite{woodworth2018preference} and Markov Decision Processes \cite{munzer2017preference} to predict preferences. This means that they require large amounts of training data, are not responsive to user feedback, and are not explainable. That is, once trained, such systems predict preferences based on their built-up statistical models; a user cannot state a preference directly or inspect why the robot predicts a particular preference. By incorporating stated user preferences, and moving toward learning preferences by analogy, we attempt to avoid these pitfalls.

There are many forms of \textit{adaptive assistance} in robotics.  One approach is
shared autonomy, in which the system infers human intentions and adapts how
much assistance in provided in controlling a robot \cite{Nikolaidis2017,Jain2019}.
This work is focused on assisting people in physically controlling robots,
whereas we are working towards an autonomous robot that provides social assistance.

Other work has looked at adapting a robot's behavior based on user preferences.
For example, a recursive neural network was used
to learn weights pertaining to user preferences, which influence the plans for
a robot \cite{Bacciu2014}.  While the preferences did affect the plans used by a robot,
the plans are used to improve the
robot's navigation.  Thus, the user preferences do not relate to assistance
provided to the user.

A model of Theory of Mind (ToM) has been proposed to adapt the assistance provided
by a social robot \cite{Gorur2017}.  A stochastic model is used to infer what action a person could
be executing.  Based on this estimate, they generate a plan to determine with which
action the robot should help.
While they use ToM to estimate a user's intent (via a set of possible actions),
they do not represent a user's preference for how the task should be completed.


One way to understand complex decision making systems is with
\emph{interpretable} or \emph{explainable} parts.  Explanations can describe
proxy methods \cite{why-trust,grad-cam,visualizing}, representations
\cite{netdissect2017,cavs}, or be inherently explanation-producing
\cite{multimodal}.
In the context
of human-robot interaction, explanations can help to communicate and
build trust \cite{wang2016trust}, justify the robot's actions
\cite{stange2020effects} or motions \cite{dragan2013legibility}, or
describe \emph{unreasonable} perceptions \cite{gilpin-hri}.  But most of
these explanations are generated \emph{after-the-fact} and cannot be used
to improve the completion of tasks moving forward.

We propose to use explanations as \emph{feedback} to augment assistive
robots.  This has been explored for agents playing games, especially
\emph{when} to provide explanations \cite{li2020reasoning}.  This approach
builds on Rainbow, a self-adaptive system that can correct itself and
reuse the same baseline framework \cite{rainbow}.  To our knowledge,
this is the first work to propose a knowledge-driven architecture that could use explanations to \emph{improve} robotic reasoning and inference.
\section{Contributions}

In this paper, we motivate a knowledge-driven architecture for adaptive assistance. We demonstrate the functionality of the components of this architecture in a task for socially assistive robots (SARs). In future work, we will expand the architecture to incorporate and process feedback and learn user preferences. This paper opens a new area of research in adaptable and interpretable SARs.

\bibliographystyle{aaai}
Molestias unde ut totam quaerat eum numquam maxime consequatur aperiam similique fugit, sunt rem tempora architecto dicta minima ipsum magnam adipisci accusamus vero eos, dolorum porro deserunt excepturi asperiores blanditiis, numquam nulla deleniti dicta perspiciatis.Expedita ullam quas quos hic rerum dignissimos maiores, unde numquam sed nihil eligendi asperiores illo, blanditiis cupiditate tempore perferendis suscipit ab tenetur pariatur molestias nisi quaerat consequuntur, laudantium cumque eligendi rem doloremque optio animi quo quae, rem ullam totam nobis sint esse assumenda consectetur ex quasi unde vitae?Aut soluta consequatur aliquid officiis quas dicta eum inventore, porro molestias reprehenderit dolorum perspiciatis fugiat impedit assumenda aspernatur, possimus eum iste nisi amet dolor iusto?\clearpage
\bibliography{pgaef}
\end{document}