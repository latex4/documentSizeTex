\relax 
\bibstyle{aaai21}
\citation{ll_ratio_nlp_facebook,larson2019evaluation,zheng2020out}
\citation{Hendrycks2020PretrainedTI}
\citation{larson2019evaluation}
\citation{kamath2020selective}
\citation{hendrycks2018deep_OE}
\citation{rostd_indomain}
\citation{msp}
\citation{odin}
\citation{yilmaz2020kloos}
\citation{nalisnick2018do,ll_ratio_google}
\citation{zheng2020out}
\citation{Mandelbaum2017DistancebasedCS,Gu2019StatisticalAO,Lee2018ASU}
\citation{NIPS2018_7936,blundell2015weight}
\citation{ghosal2018investigating}
\citation{tan2019out}
\citation{msp}
\citation{ll_ratio_nlp_facebook}
\citation{de2000mahalanobis}
\citation{msp}
\citation{odin}
\citation{ll_ratio_nlp_facebook}
\citation{Lee2018ASU}
\newlabel{eq:OOD_descion_rule}{{1}{2}{}{}{}}
\newlabel{eq:msp}{{2}{2}{}{}{}}
\newlabel{eq:LLR}{{3}{2}{}{}{}}
\newlabel{eq:Maha}{{4}{2}{}{}{}}
\citation{clinc150}
\citation{rostd_indomain,ll_ratio_nlp_facebook}
\citation{ll_ratio_nlp_facebook}
\citation{intent_detection_margin_loss}
\citation{haris_bow}
\citation{pennington2014glove}
\citation{zheng2020out}
\citation{devlin2019bert}
\citation{liu2019roberta}
\citation{sanh2019distilbert}
\citation{paszke2019pytorch}
\citation{falcon2019pytorch}
\citation{wolf2019huggingface}
\citation{liu2020simple}
\citation{ll_ratio_nlp_facebook}
\citation{zheng2020out}
\citation{liu2020simple}
\citation{ll_ratio_nlp_facebook}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab: data_stats}{{1}{3}{Dataset statistics}{}{}}
\newlabel{sec: experiments}{{}{3}{}{}{}}
\newlabel{tab:all_results}{{2}{4}{Comparison of OOD detection performance. Each result is an average of 10 runs. $\uparrow $ -- greater is better, $\downarrow $ -- lower is better}{}{}}
\citation{hendrycks_pretraining19,orhan2019robustness}
\citation{Hendrycks2020PretrainedTI}
\newlabel{tab:center_stats}{{3}{5}{Descriptive statistics of embedding space. Both spaces are derived from fine-tuned models with ID supervision. We show statistics for only one of the SNIPS splits}{}{}}
\newlabel{fig:tsne}{{1}{5}{t-SNE visualization of CLINC150 ID classes. Embeddings are derived from fine-tuned RoBERTa for ID classification. ID classes are easily separated}{}{}}
\newlabel{fig:model_size_aupr_ood}{{2}{5}{Comparison of models of different sizes on ROSTD and CLINC150. Maha stands for Mahalanobis distance}{}{}}
\citation{Kamoi2020WhyIT}
\citation{Kamoi2020WhyIT}
\citation{murphy2012machine}
\newlabel{fig:maha_components}{{3}{6}{These heatmaps represent each utterance from the CLINC150 test set as the vector of Mahalanobis distance terms, computed according to Eq.\nobreakspace  {}\ref {eq:Maha_via_PCA} and sorted in the decreasing order of explained variance. Each row stands for an utterance. The horizontal solid line separates the OOD utterances (above the line) from the ID ones (below the line). The vertical solid line splits each heatmap into two parts: to the left are components numbered lower than 150, to the right are components numbered above 150. 150 is the number of classes in the CLINC150 dataset. Only fine-tuned RoBERTa-based vectors clearly distinguish ID and OOD utterances. The difference between ID and OOD is less evident in (c) and almost indistinguishable in (a). However, in (b), the values of the components, starting from the 150$^{th}$ one (in yellow), are lower than those of ID ones (in red). }{}{}}
\newlabel{eq:Maha_via_PCA}{{5}{6}{}{}{}}
\newlabel{eq:M_Maha}{{6}{6}{}{}{}}
\newlabel{eq:P_Maha}{{7}{6}{}{}{}}
\citation{reimers2019sentence}
\bibdata{references}
\newlabel{fig:maha_var_comp}{{4}{7}{Comparison of different distances. Mahalanobis distance and its variants outperform Euclidean distance by a wide margin.}{}{}}
\newlabel{fig:dataset_size_var}{{5}{7}{OX: fraction of train data used, CLINC150, OY: performance of OOD detection score. Mahalanobis distance and its variants need less data for OOD detection.}{}{}}
\gdef \@abspage@last{8}
