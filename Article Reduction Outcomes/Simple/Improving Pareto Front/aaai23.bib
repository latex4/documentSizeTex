@misc{chen2018gradnorm,
      title={GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks}, 
      author={Zhao Chen and Vijay Badrinarayanan and Chen-Yu Lee and Andrew Rabinovich},
      year={2018},
      eprint={1711.02257},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{le2021efficient,
  title={Efficient Retrieval of Matrix Factorization-Based Top-k Recommendations: A Survey of Recent Approaches},
  author={Le, Dung D and Lauw, Hady},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1441--1479},
  year={2021}
}

@inproceedings{le2020stochastically,
  title={Stochastically robust personalized ranking for lsh recommendation retrieval},
  author={Le, Dung D and Lauw, Hady W},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={4594--4601},
  year={2020}
}

@misc{kendall2018multitask,
      title={Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics}, 
      author={Alex Kendall and Yarin Gal and Roberto Cipolla},
      year={2018},
      eprint={1705.07115},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{sener2019multitask,
      title={Multi-Task Learning as Multi-Objective Optimization}, 
      author={Ozan Sener and Vladlen Koltun},
      year={2019},
      eprint={1810.04650},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{lin2019pareto,
  title={Pareto multi-task learning},
  author={Lin, Xi and Zhen, Hui-Ling and Li, Zhenhua and Zhang, Qing-Fu and Kwong, Sam},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@InProceedings{pmlr-v119-mahapatra20a,
  title = 	 {Multi-Task Learning with User Preferences: Gradient Descent with Controlled Ascent in Pareto Optimization},
  author =       {Mahapatra, Debabrata and Rajan, Vaibhav},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6597--6607},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/mahapatra20a/mahapatra20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/mahapatra20a.html},
  abstract = 	 {Multi-Task Learning (MTL) is a well established paradigm for jointly learning models for multiple correlated tasks. Often the tasks conflict, requiring trade-offs between them during optimization. In such cases, multi-objective optimization based MTL methods can be used to find one or more Pareto optimal solutions. A common requirement in MTL applications, that cannot be addressed by these methods, is to find a solution satisfying userspecified preferences with respect to task-specific losses. We advance the state-of-the-art by developing the first gradient-based multi-objective MTL algorithm to solve this problem. Our unique approach combines multiple gradient descent with carefully controlled ascent to traverse the Pareto front in a principled manner, which also makes it robust to initialization. The scalability of our algorithm enables its use in large-scale deep networks for MTL. Assuming only differentiability of the task-specific loss functions, we provide theoretical guarantees for convergence. Our experiments show that our algorithm outperforms the best competing methods on benchmark datasets.}
}

@misc{navon2021learning,
      title={Learning the Pareto Front with Hypernetworks}, 
      author={Aviv Navon and Aviv Shamsian and Gal Chechik and Ethan Fetaya},
      year={2021},
      eprint={2010.04104},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{ruchte2021scalable,
      title={Scalable Pareto Front Approximation for Deep Multi-Objective Learning}, 
      author={Michael Ruchte and Josif Grabocka},
      year={2021},
      eprint={2103.13392},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{deist2021multiobjective,
      title={Multi-Objective Learning to Predict Pareto Fronts Using Hypervolume Maximization}, 
      author={Timo M. Deist and Monika Grewal and Frank J. W. M. Dankers and Tanja Alderliesten and Peter A. N. Bosman},
      year={2021},
      eprint={2102.04523},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@ARTICLE{6600851,
  author={Deb, Kalyanmoy and Jain, Himanshu},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={An Evolutionary Many-Objective Optimization Algorithm Using Reference-Point-Based Nondominated Sorting Approach, Part I: Solving Problems With Box Constraints}, 
  year={2014},
  volume={18},
  number={4},
  pages={577-601},
  doi={10.1109/TEVC.2013.2281535}}
@inproceedings{inproceedings,
  title={Multiple objective optimization with vector evaluated genetic algorithms},
  author={Schaffer, J David},
  booktitle={Proceedings of the First International Conference of Genetic Algorithms and Their Application},
  pages={93--100},
  year={1985}
}
@INPROCEEDINGS{257666,
  author={Fonseca, C.M. and Fleming, P.J.},
  booktitle={IEE Colloquium on Genetic Algorithms for Control Systems Engineering}, 
  title={Multiobjective genetic algorithms}, 
  year={1993},
  volume={},
  number={},
  pages={6/1-6/5},
  doi={}}
@INPROCEEDINGS{350037,  author={Horn, J. and Nafpliotis, N. and Goldberg, D.E.},  booktitle={Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence},   title={A niched Pareto genetic algorithm for multiobjective optimization},   year={1994},  volume={},  number={},  pages={82-87 vol.1},  doi={10.1109/ICEC.1994.350037}}
@INPROCEEDINGS{781913,  author={Knowles, J. and Corne, D.},  booktitle={Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)},   title={The Pareto archived evolution strategy: a new baseline algorithm for Pareto multiobjective optimisation},   year={1999},  volume={1},  number={},  pages={98-105 Vol. 1},  doi={10.1109/CEC.1999.781913}}
@article{article,
author = {Steuer, Ralph and Choo, Eng},
year = {1983},
month = {10},
pages = {326-344},
title = {An Interactive Weighted Tchebycheff Procedure for Multiple Objective Programming},
volume = {26},
journal = {Mathematical Programming},
doi = {10.1007/BF02591870}
}
@article{gradientbased1,
  title={Steepest descent methods for multicriteria optimization},
  author={Fliege, J{\"o}rg and Svaiter, Benar Fux},
  journal={Mathematical methods of operations research},
  volume={51},
  number={3},
  pages={479--494},
  year={2000},
  publisher={Springer}
}
@article{gradientbased2,
author = {Schäffler, S. and Schultz, R. and Weinzierl, Klaus},
year = {2002},
month = {01},
pages = {209-222},
title = {Stochastic Method for the Solution of Unconstrained Vector Optimization Problems},
volume = {114},
journal = {Journal of Optimization Theory and Applications},
doi = {10.1023/A:1015472306888}
}
@article{gradientbased3,
author = {Désidéri, Jean-Antoine},
year = {2012},
month = {03},
pages = {313–318},
title = {Multiple-gradient descent algorithm (MGDA) for multiobjective optimization},
volume = {350},
journal = {Comptes Rendus Mathematique},
doi = {10.1016/j.crma.2012.03.014}
}
@inproceedings{kuhn1951nonlinear,
  added-at = {2013-06-17T14:15:18.000+0200},
  address = {Berkeley and Los Angeles},
  author = {Kuhn, H. W. and Tucker, A. W.},
  biburl = {https://www.bibsonomy.org/bibtex/2d009df996edeb353842306e3e70d9c86/thoni},
  booktitle = {Proceedings of the {S}econd {B}erkeley {S}ymposium on              {M}athematical {S}tatistics and {P}robability, 1950},
  description = {MR: Publications results for "MR Number=(47303)"},
  interhash = {7fb836ca2faa555996a5eae04e99cce2},
  intrahash = {d009df996edeb353842306e3e70d9c86},
  keywords = {conditions karush kkt kuhn svm tucker},
  mrclass = {90.0X},
  mrnumber = {0047303 (13,855f)},
  mrreviewer = {D. Gale},
  pages = {481--492},
  publisher = {University of California Press},
  timestamp = {2016-09-06T08:23:07.000+0200},
  title = {Nonlinear programming},
  year = 1951
}
@inproceedings{Guo2018DynamicTP,
  title={Dynamic Task Prioritization for Multitask Learning},
  author={Michelle Guo and Albert Haque and De-An Huang and Serena Yeung and Li Fei-Fei},
  booktitle={ECCV},
  year={2018}
}
@misc{liu2019endtoend,
      title={End-to-End Multi-Task Learning with Attention}, 
      author={Shikun Liu and Edward Johns and Andrew J. Davison},
      year={2019},
      eprint={1803.10704},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{Frank1956AnAF,
  title={An algorithm for quadratic programming},
  author={Marguerite Frank and Philip Wolfe},
  journal={Naval Research Logistics Quarterly},
  year={1956},
  volume={3},
  pages={95-110}
}
@article{10.1007/s10589-017-9921-x,
author = {Poirion, Fabrice and Mercier, Quentin and D\'{e}sid\'{e}ri, Jean-Antoine},
title = {Descent Algorithm for Nonsmooth Stochastic Multiobjective Optimization},
year = {2017},
issue_date = {November 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {68},
number = {2},
issn = {0926-6003},
url = {https://doi.org/10.1007/s10589-017-9921-x},
doi = {10.1007/s10589-017-9921-x},
abstract = {An algorithm for solving the expectation formulation of stochastic nonsmooth multiobjective optimization problems is proposed. The proposed method is an extension of the classical stochastic gradient algorithm to multiobjective optimization using the properties of a common descent vector defined in the deterministic context. The mean square and the almost sure convergence of the algorithm are proven. The algorithm efficiency is illustrated and assessed on an academic example.},
journal = {Comput. Optim. Appl.},
month = {nov},
pages = {317–331},
numpages = {15},
keywords = {Nonsmooth, Almost sure convergence, Stochastic, Multiobjective optimization}
}
@misc{yu2020gradient,
      title={Gradient Surgery for Multi-Task Learning}, 
      author={Tianhe Yu and Saurabh Kumar and Abhishek Gupta and Sergey Levine and Karol Hausman and Chelsea Finn},
      year={2020},
      eprint={2001.06782},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{ma2020efficient,
      title={Efficient Continuous Pareto Exploration in Multi-Task Learning}, 
      author={Pingchuan Ma and Tao Du and Wojciech Matusik},
      year={2020},
      eprint={2006.16434},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{10.1007/978-3-540-70928-2_64,
author="Zitzler, Eckart
and Brockhoff, Dimo
and Thiele, Lothar",
editor="Obayashi, Shigeru
and Deb, Kalyanmoy
and Poloni, Carlo
and Hiroyasu, Tomoyuki
and Murata, Tadahiko",
title="The Hypervolume Indicator Revisited: On the Design of Pareto-compliant Indicators Via Weighted Integration",
booktitle="Evolutionary Multi-Criterion Optimization",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="862--876",
abstract="The design of quality measures for approximations of the Pareto-optimal set is of high importance not only for the performance assessment, but also for the construction of multiobjective optimizers. Various measures have been proposed in the literature with the intention to capture different preferences of the decision maker. A quality measure that possesses a highly desirable feature is the hypervolume measure: whenever one approximation completely dominates another approximation, the hypervolume of the former will be greater than the hypervolume of the latter. Unfortunately, this measure---as any measure inducing a total order on the search space---is biased, in particular towards convex, inner portions of the objective space. Thus, an open question in this context is whether it can be modified such that other preferences such as a bias towards extreme solutions can be obtained. This paper proposes a methodology for quality measure design based on the hypervolume measure and demonstrates its usefulness for three types of preferences.",
isbn="978-3-540-70928-2"
}

@incollection{hypervolumegrad,
  title={Time complexity and zeros of the hypervolume indicator gradient field},
  author={Emmerich, Michael and Deutz, Andr{\'e}},
  booktitle={EVOLVE-a bridge between probability, set oriented numerics, and evolutionary computation III},
  pages={169--193},
  year={2014},
  publisher={Springer}
}

@misc{miranda2016singlesolution,
      title={Single-Solution Hypervolume Maximization and its use for Improving Generalization of Neural Networks}, 
      author={Conrado S. Miranda and Fernando J. Von Zuben},
      year={2016},
      eprint={1602.01164},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{rs3,
  title={Multi-Gradient Descent for Multi-Objective Recommender Systems},
  author={Nikola Milojkovi{\'c} and Diego Antognini and Giancarlo Bergamin and Boi Faltings and Claudiu Cristian Musat},
  journal={ArXiv},
  year={2019},
  volume={abs/2001.00846}
}


@inbook{rs1,
author = {Masthoff, Judith},
year = {2011},
month = {01},
pages = {677-702},
title = {Group Recommender Systems: Combining Individual Models},
journal = {Recommender Systems Handbook},
doi = {10.1007/978-0-387-85820-3_21}
}

@article{rs2,
  author    = {Guy Hadash and
               Oren Sar Shalom and
               Rita Osadchy},
  title     = {Rank and Rate: Multi-task Learning for Recommender Systems},
  journal   = {CoRR},
  volume    = {abs/1807.11698},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.11698},
  eprinttype = {arXiv},
  eprint    = {1807.11698},
  timestamp = {Mon, 13 Aug 2018 16:49:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-11698.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2110-03461,
  author    = {Simyung Chang and
               KiYoon Yoo and
               Jiho Jang and
               Nojun Kwak},
  title     = {Self-Evolutionary Optimization for Pareto Front Learning},
  journal   = {CoRR},
  volume    = {abs/2110.03461},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.03461},
  eprinttype = {arXiv},
  eprint    = {2110.03461},
  timestamp = {Thu, 21 Oct 2021 16:20:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-03461.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cv1,
author = {Lapin, Maksim and Schiele, Bernt and Hein, Matthias},
year = {2014},
month = {06},
pages = {1434-1441},
title = {Scalable Multitask Representation Learning for Scene Classification},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.186}
}
@article{cv2,
author = {Yuan, Xiao-Tong and Liu, Xiaobai and Yan, Shuicheng},
year = {2012},
month = {06},
pages = {4349-60},
title = {Visual Classification With Multitask Joint Sparse Representation},
volume = {21},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
doi = {10.1109/TIP.2012.2205006}
}

@inproceedings{cv3,
  title={Tracking via robust multi-task multi-view joint sparse representation},
  author={Hong, Zhibin and Mei, Xue and Prokhorov, Danil and Tao, Dacheng},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={649--656},
  year={2013}
}


@article{bh1,
author = {Qi, Yanjun and Tastan, Oznur and Carbonell, Jaime and Klein-Seetharaman, Judith and Weston, Jason},
year = {2010},
month = {09},
pages = {i645-52},
title = {Semi-supervised multi-task learning for predicting interactions between HIV-1 and human proteins},
volume = {26},
journal = {Bioinformatics (Oxford, England)},
doi = {10.1093/bioinformatics/btq394}
}

@article{bh2,
    author = {Puniyani, Kriti and Kim, Seyoung and Xing, Eric P.},
    title = "{Multi-population GWA mapping via multi-task regularized regression}",
    journal = {Bioinformatics},
    volume = {26},
    number = {12},
    pages = {i208-i216},
    year = {2010},
    month = {06},
    abstract = "{Motivation: Population heterogeneity through admixing of different founder populations can produce spurious associations in genome- wide association studies that are linked to the population structure rather than the phenotype. Since samples from the same population generally co-evolve, different populations may or may not share the same genetic underpinnings for the seemingly common phenotype. Our goal is to develop a unified framework for detecting causal genetic markers through a joint association analysis of multiple populations.Results: Based on a multi-task regression principle, we present a multi-population group lasso algorithm using L1/L2-regularized regression for joint association analysis of multiple populations that are stratified either via population survey or computational estimation. Our algorithm combines information from genetic markers across populations, to identify causal markers. It also implicitly accounts for correlations between the genetic markers, thus enabling better control over false positive rates. Joint analysis across populations enables the detection of weak associations common to all populations with greater power than in a separate analysis of each population. At the same time, the regression-based framework allows causal alleles that are unique to a subset of the populations to be correctly identified. We demonstrate the effectiveness of our method on HapMap-simulated and lactase persistence datasets, where we significantly outperform state of the art methods, with greater power for detecting weak associations and reduced spurious associations.Availability: Software will be available at http://www.sailing.cs.cmu.edu/Contact:epxing@cs.cmu.edu}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btq191},
    url = {https://doi.org/10.1093/bioinformatics/btq191},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/26/12/i208/795455/btq191.pdf},
}

@InProceedings{bh3,
  title = 	 {Multitask Learning for Brain-Computer Interfaces},
  author = 	 {Alamgir, Morteza and Grosse–Wentrup, Moritz and Altun, Yasemin},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {17--24},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/alamgir10a/alamgir10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/alamgir10a.html},
  abstract = 	 {Brain-computer interfaces (BCIs) are limited in their applicability in everyday settings by the current necessity to record subject-specific calibration data prior to actual use of the BCI for communication. In this paper, we utilize the framework of multitask learning to construct a BCI that can be used without any subject-specific calibration process. We discuss how this out-of-the-box BCI can be further improved in a computationally efficient manner as subject-specific data becomes available. The feasibility of the approach is demonstrated on two sets of experimental EEG data recorded during a standard two-class motor imagery paradigm from a total of 19 healthy subjects. Specifically, we show that satisfactory classification results can be achieved with zero training data, and combining prior recordings with subject-specific calibration data substantially outperforms using subject-specific data only. Our results further show that transfer between recordings under slightly different experimental setups is feasible.}
}
@inproceedings{sn1,
  title={Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks},
  author={Chen, Zhuo and Watanabe, Shinji and Erdogan, Hakan and Hershey, John R},
  booktitle={Sixteenth Annual Conference of the International Speech Communication Association},
  year={2015}
}


@inproceedings{sn2,
author = {Zhao, Liang and Sun, Qian and Ye, Jieping and Chen, Feng and Lu, Chang-Tien and Ramakrishnan, Naren},
title = {Multi-Task Learning for Spatio-Temporal Event Forecasting},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783377},
doi = {10.1145/2783258.2783377},
abstract = {Spatial event forecasting from social media is an important problem but encounters critical challenges, such as dynamic patterns of features (keywords) and geographic heterogeneity (e.g., spatial correlations, imbalanced samples, and different populations in different locations). Most existing approaches (e.g., LASSO regression, dynamic query expansion, and burst detection) are designed to address some of these challenges, but not all of them. This paper proposes a novel multi-task learning framework which aims to concurrently address all the challenges. Specifically, given a collection of locations (e.g., cities), we propose to build forecasting models for all locations simultaneously by extracting and utilizing appropriate shared information that effectively increases the sample size for each location, thus improving the forecasting performance. We combine both static features derived from a predefined vocabulary by domain experts and dynamic features generated from dynamic query expansion in a multi-task feature learning framework; we investigate different strategies to balance homogeneity and diversity between static and dynamic terms. Efficient algorithms based on Iterative Group Hard Thresholding are developed to achieve efficient and effective model training and prediction. Extensive experimental evaluations on Twitter data from four different countries in Latin America demonstrated the effectiveness of our proposed approach.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1503–1512},
numpages = {10},
keywords = {multi-task learning, event forecasting, hard thresholding, dynamic query expansion, lasso},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}

@INPROCEEDINGS{sn3,  author={Wu, Fangzhao and Huang, Yongfeng},  booktitle={2015 IEEE International Conference on Data Mining},   title={Collaborative Multi-domain Sentiment Classification},   year={2015},  volume={},  number={},  pages={459-468},  doi={10.1109/ICDM.2015.68}}
@misc{sabour2017,
  doi = {10.48550/ARXIV.1710.09829},
  
  url = {https://arxiv.org/abs/1710.09829},
  
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Dynamic Routing Between Capsules},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@ARTICLE{Lecun1998,  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},  journal={Proceedings of the IEEE},   title={Gradient-based learning applied to document recognition},   year={1998},  volume={86},  number={11},  pages={2278-2324},  doi={10.1109/5.726791}}
@misc{Xiao2017,
  doi = {10.48550/ARXIV.1708.07747},
  
  url = {https://arxiv.org/abs/1708.07747},
  
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{Goovaerts1997,
  title={Geostatistics for natural resources evaluation},
  author={Goovaerts, Pierre and others},
  year={1997},
  publisher={Oxford University Press on Demand}
}

@misc{Sethu2000,
  author = {Sethu Vijayakumar},  
  url = {\url{http://www.gaussianprocess.org/gpml/data}},
  year = {2000},
  title = {The sarcos dataset},
  howpublished = "\url{ http://www.gaussianprocess.org/gpml/data}",
  note = {Accessed: 2023-03-09}
}

@inproceedings{Dominated-front1,
  title={Hypervolume indicator gradient ascent multi-objective optimization},
  author={Wang, Hao and Deutz, Andr{\'e} and B{\"a}ck, Thomas and Emmerich, Michael},
  booktitle={International conference on evolutionary multi-criterion optimization},
  pages={654--669},
  year={2017},
  organization={Springer}
}
@incollection{Deist_2020,
	doi = {10.1007/978-3-030-58115-2_13},
  
	url = {https://doi.org/10.1007%2F978-3-030-58115-2_13},
  
	year = 2020,
	publisher = {Springer International Publishing},
  
	pages = {186--200},
  
	author = {Timo M. Deist and Stefanus C. Maree and Tanja Alderliesten and Peter A. N. Bosman},
  
	title = {Multi-objective Optimization by Uncrowded Hypervolume Gradient Ascent},
  
	booktitle = {Parallel Problem Solving from Nature {\textendash} {PPSN} {XVI}
}
}
@inproceedings{Felix2018,
author = {Gr\"{a}\ss{}er, Felix and Kallumadi, Surya and Malberg, Hagen and Zaunseder, Sebastian},
title = {Aspect-Based Sentiment Analysis of Drug Reviews Applying Cross-Domain and Cross-Data Learning},
year = {2018},
isbn = {9781450364935},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194658.3194677},
doi = {10.1145/3194658.3194677},
abstract = {Online review sites and opinion forums contain a wealth of information regarding user preferences and experiences over multiple product domains. This information can be leveraged to obtain valuable insights using data mining approaches such as sentiment analysis. In this work we examine online user reviews within the pharmaceutical field. Online user reviews in this domain contain information related to multiple aspects such as effectiveness of drugs and side effects, which make automatic analysis very interesting but also challenging. However, analyzing sentiments concerning the various aspects of drug reviews can provide valuable insights, help with decision making and improve monitoring public health by revealing collective experience. In this preliminary work we perform multiple tasks over drug reviews with data obtained by crawling online pharmaceutical review sites. We first perform sentiment analysis to predict the sentiments concerning overall satisfaction, side effects and effectiveness of user reviews on specific drugs. To meet the challenge of lacking annotated data we further investigate the transferability of trained classification models among domains, i.e. conditions, and data sources. In this work we show that transfer learning approaches can be used to exploit similarities across domains and is a promising approach for cross-domain sentiment analysis.},
booktitle = {Proceedings of the 2018 International Conference on Digital Health},
pages = {121–125},
numpages = {5},
keywords = {text classification, health recommender system, sentiment analysis, clinical decision support system (cdss)},
location = {Lyon, France},
series = {DH '18}
}


@article{jura2,
author = {\'{A}lvarez, Mauricio A. and Lawrence, Neil D.},
title = {Computationally Efficient Convolved Multiple Output Gaussian Processes},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {Recently there has been an increasing interest in regression methods that deal with multiple outputs. This has been motivated partly by frameworks like multitask learning, multisensor networks or structured output data. From a Gaussian processes perspective, the problem reduces to specifying an appropriate covariance function that, whilst being positive semi-definite, captures the dependencies between all the data points and across all the outputs. One approach to account for non-trivial correlations between outputs employs convolution processes. Under a latent function interpretation of the convolution transform we establish dependencies between output variables. The main drawbacks of this approach are the associated computational and storage demands. In this paper we address these issues. We present different efficient approximations for dependent output Gaussian processes constructed through the convolution formalism. We exploit the conditional independencies present naturally in the model. This leads to a form of the covariance similar in spirit to the so called PITC and FITC approximations for a single output. We show experimental results with synthetic and real data, in particular, we show results in school exams score prediction, pollution prediction and gene expression data.},
journal = {J. Mach. Learn. Res.},
month = {jul},
pages = {1459–1500},
numpages = {42}
}
@article{das,
author = {Das, Indraneel and Dennis, J.},
year = {2000},
month = {07},
pages = {},
title = {Normal-Boundary Intersection: A New Method for Generating the Pareto Surface in Nonlinear Multicriteria Optimization Problems},
volume = {8},
journal = {SIAM Journal on Optimization},
doi = {10.1137/S1052623496307510}
}

@article{implicitfunc,
author = {Krantz, Steven and Parks, Harold},
year = {2003},
month = {01},
pages = {},
title = {The Implicit Function Theorem : History, Theory, and Applications / S.G. Krantz, H.R. Parks.},
isbn = {978-1-4612-6593-1},
doi = {10.1007/978-1-4612-0059-8}
}
@article{koski1987norm,
  title={Norm methods and partial weighting in multicriterion optimization of structures},
  author={Koski, Juhani and Silvennoinen, Risto},
  journal={International Journal for Numerical Methods in Engineering},
  volume={24},
  number={6},
  pages={1101--1121},
  year={1987},
  publisher={Wiley Online Library}
}

@article{citeulike:3561150,
  abstract = {{In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.}},
  added-at = {2012-03-02T03:39:18.000+0100},
  author = {Cybenko, G.},
  biburl = {https://www.bibsonomy.org/bibtex/2be85c56ae384216b2e35bdf79b7fb477/baby9992006},
  citeulike-article-id = {3561150},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/BF02551274},
  citeulike-linkout-1 = {http://www.springerlink.com/content/n873j15736072427},
  day = 1,
  doi = {10.1007/BF02551274},
  interhash = {96aecb02daa11041489259a8edb54070},
  intrahash = {be85c56ae384216b2e35bdf79b7fb477},
  issn = {0932-4194},
  journal = {Mathematics of Control, Signals, and Systems (MCSS)},
  keywords = {approximation, control, duckling, free, lunch, no, theorem, theory, ugly, universal},
  month = dec,
  number = 4,
  pages = {303--314},
  posted-at = {2012-02-28 13:17:08},
  priority = {2},
  publisher = {Springer London},
  timestamp = {2012-03-02T03:39:20.000+0100},
  title = {{Approximation by superpositions of a sigmoidal function}},
  url = {http://dx.doi.org/10.1007/BF02551274},
  volume = 2,
  year = 1989
}

@inproceedings{TextCNN,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}


@article{zitzler1999multiobjective,
  title={Multiobjective evolutionary algorithms: a comparative case study and the strength Pareto approach},
  author={Zitzler, Eckart and Thiele, Lothar},
  journal={IEEE transactions on Evolutionary Computation},
  volume={3},
  number={4},
  pages={257--271},
  year={1999},
  publisher={IEEE}
}
@inproceedings{hv2,
  title={The measure of Pareto optima applications to multi-objective metaheuristics},
  author={Fleischer, Mark},
  booktitle={International conference on evolutionary multi-criterion optimization},
  pages={519--533},
  year={2003},
  organization={Springer}
}

@inproceedings{
hyper_origin,
title={HyperNetworks},
author={David Ha and Andrew M. Dai and Quoc V. Le},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rkpACe1lx}
}

@article{Edelsbrunner1985VoronoiDA,
  title={Voronoi diagrams and arrangements},
  author={Herbert Edelsbrunner and Raimund Seidel},
  journal={Discrete \& Computational Geometry},
  year={1985},
  volume={1},
  pages={25-44}
}




@inproceedings{moosvgd,
 author = {Liu, Xingchao and Tong, Xin and Liu, Qiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {14721--14733},
 publisher = {Curran Associates, Inc.},
 title = {Profiling Pareto Front With Multi-Objective Stein Variational  Gradient Descent},
 url = {https://proceedings.neurips.cc/paper/2021/file/7bb16972da003e87724f048d76b7e0e1-Paper.pdf},
 volume = {34},
 year = {2021}
}


@inproceedings{svgd,
 author = {Liu, Qiang and Wang, Dilin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm},
 url = {https://proceedings.neurips.cc/paper/2016/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf},
 volume = {29},
 year = {2016}
}




@article{controllable,
  author    = {Xi Lin and
               Zhiyuan Yang and
               Qingfu Zhang and
               Sam Kwong},
  title     = {Controllable Pareto Multi-Task Learning},
  journal   = {CoRR},
  volume    = {abs/2010.06313},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.06313},
  eprinttype = {arXiv},
  eprint    = {2010.06313},
  timestamp = {Fri, 22 Jul 2022 10:31:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-06313.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@phdthesis{fonseca1995multiobjective,
  title={Multiobjective genetic algorithms with application to control engineering problems.},
  author={Fonseca, Carlos Manuel Mira da},
  year={1995},
  school={University of Sheffield}
}

@article{thang2020monotonic,
  title={A monotonic optimization approach for solving strictly quasiconvex multiobjective programming problems},
  author={Thang, Tran Ngoc and Solanki, Vijender Kumar and Dao, Tuan Anh and Thi Ngoc Anh, Nguyen and Van Hai, Pham},
  journal={Journal of Intelligent \& Fuzzy Systems},
  volume={38},
  number={5},
  pages={6053--6063},
  year={2020},
  publisher={IOS Press}
}


@incollection{anh2022multi,
  title={Multi Deep Learning Model for Building Footprint Extraction from High Resolution Remote Sensing Image},
  author={{\'A}nh, Ho Trong and Tuan, Tran Anh and Long, Ho{\`a}ng Phi and H{\`a}, L{\^e} Hai and Thang, Tran Ngoc},
  booktitle={Intelligent Systems and Networks},
  pages={246--252},
  year={2022},
  publisher={Springer}
}

@incollection{pytorch, 
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith}, 
booktitle = {Advances in Neural Information Processing Systems 32}, 
pages = {8024--8035}, 
year = {2019}, 
publisher = {Curran Associates, Inc.}, 
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf} 
}
