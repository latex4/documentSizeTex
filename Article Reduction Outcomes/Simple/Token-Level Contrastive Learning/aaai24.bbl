\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Baevski et~al.(2020)Baevski, Zhou, Mohamed, and Auli}]{NEURIPS2020_92d1e1eb}
Baevski, A.; Zhou, Y.; Mohamed, A.; and Auli, M. 2020.
\newblock wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations.
\newblock In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., \emph{Advances in Neural Information Processing Systems}, volume~33, 12449--12460. Curran Associates, Inc.

\bibitem[{Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal, Bojanowski, and Joulin}]{caron2021emerging}
Caron, M.; Touvron, H.; Misra, I.; J{\'e}gou, H.; Mairal, J.; Bojanowski, P.; and Joulin, A. 2021.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, 9650--9660.

\bibitem[{Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton}]{chen2020simple}
Chen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020.
\newblock A simple framework for contrastive learning of visual representations.
\newblock In \emph{International conference on machine learning}, 1597--1607. PMLR.

\bibitem[{Chen and He(2021)}]{chen2021exploring}
Chen, X.; and He, K. 2021.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 15750--15758.

\bibitem[{Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei}]{5206848}
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L. 2009.
\newblock ImageNet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern Recognition}, 248--255.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{devlin2018bert}
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}.

\bibitem[{Dong et~al.(2022)Dong, Fu, Zhou, Li, and Wang}]{dong2022improving}
Dong, J.; Fu, J.; Zhou, P.; Li, H.; and Wang, X. 2022.
\newblock Improving spoken language understanding with cross-modal contrastive learning.
\newblock \emph{Interspeech. ISCA}.

\bibitem[{Gan et~al.(2023)Gan, Bai, Lou, Ma, Zhang, Shi, and Luo}]{gan2023decorate}
Gan, Y.; Bai, Y.; Lou, Y.; Ma, X.; Zhang, R.; Shi, N.; and Luo, L. 2023.
\newblock Decorate the newcomers: Visual domain prompt for continual test time adaptation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, 7595--7603.

\bibitem[{Graves et~al.(2006)Graves, Fern\'{a}ndez, Gomez, and Schmidhuber}]{10.1145/1143844.1143891}
Graves, A.; Fern\'{a}ndez, S.; Gomez, F.; and Schmidhuber, J. 2006.
\newblock Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks.
\newblock In \emph{Proceedings of the 23rd International Conference on Machine Learning}, ICML '06, 369–376. New York, NY, USA: Association for Computing Machinery.
\newblock ISBN 1595933832.

\bibitem[{Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond, Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar et~al.}]{grill2020bootstrap}
Grill, J.-B.; Strub, F.; Altch{\'e}, F.; Tallec, C.; Richemond, P.; Buchatskaya, E.; Doersch, C.; Avila~Pires, B.; Guo, Z.; Gheshlaghi~Azar, M.; et~al. 2020.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock \emph{Advances in neural information processing systems}, 33: 21271--21284.

\bibitem[{Han et~al.(2021)Han, Chen, Gelbukh, Zadeh, Morency, and Poria}]{han2021bi}
Han, W.; Chen, H.; Gelbukh, A.; Zadeh, A.; Morency, L.-p.; and Poria, S. 2021.
\newblock Bi-bimodal modality fusion for correlation-controlled multimodal sentiment analysis.
\newblock In \emph{Proceedings of the 2021 International Conference on Multimodal Interaction}, 6--15.

\bibitem[{Han, Chen, and Poria(2021)}]{han2021improving}
Han, W.; Chen, H.; and Poria, S. 2021.
\newblock Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis.
\newblock \emph{arXiv preprint arXiv:2109.00412}.

\bibitem[{Hazarika, Zimmermann, and Poria(2020)}]{10.1145/3394171.3413678}
Hazarika, D.; Zimmermann, R.; and Poria, S. 2020.
\newblock MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis.
\newblock In \emph{Proceedings of the 28th ACM International Conference on Multimedia}, MM '20, 1122–1131. New York, NY, USA: Association for Computing Machinery.
\newblock ISBN 9781450379885.

\bibitem[{He et~al.(2020)He, Fan, Wu, Xie, and Girshick}]{he2020momentum}
He, K.; Fan, H.; Wu, Y.; Xie, S.; and Girshick, R. 2020.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 9729--9738.

\bibitem[{Hou et~al.(2019)Hou, Tang, Zhang, Kong, and Zhao}]{hou2019deep}
Hou, M.; Tang, J.; Zhang, J.; Kong, W.; and Zhao, Q. 2019.
\newblock Deep multimodal multilinear fusion with high-order polynomial pooling.
\newblock \emph{Advances in Neural Information Processing Systems}, 32.

\bibitem[{Li et~al.(2022)Li, Li, Li, Niebles, and Hoi}]{Li_2022_CVPR}
Li, D.; Li, J.; Li, H.; Niebles, J.~C.; and Hoi, S.~C. 2022.
\newblock Align and Prompt: Video-and-Language Pre-Training With Entity Prompts.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 4953--4963.

\bibitem[{Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo}]{Liu_2021_ICCV}
Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B. 2021.
\newblock Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, 10012--10022.

\bibitem[{Liu et~al.(2018)Liu, Shen, Lakshminarasimhan, Liang, Zadeh, and Morency}]{liu2018efficient}
Liu, Z.; Shen, Y.; Lakshminarasimhan, V.~B.; Liang, P.~P.; Zadeh, A.; and Morency, L.-P. 2018.
\newblock Efficient low-rank multimodal fusion with modality-specific factors.
\newblock \emph{arXiv preprint arXiv:1806.00064}.

\bibitem[{Loshchilov and Hutter(2017)}]{loshchilov2017decoupled}
Loshchilov, I.; and Hutter, F. 2017.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}.

\bibitem[{maintainers and contributors(2016)}]{torchvision2016}
maintainers, T.; and contributors. 2016.
\newblock TorchVision: PyTorch's Computer Vision library.
\newblock \url{https://github.com/pytorch/vision}.

\bibitem[{Paraskevopoulos, Georgiou, and Potamianos(2022)}]{paraskevopoulos2022mmlatch}
Paraskevopoulos, G.; Georgiou, E.; and Potamianos, A. 2022.
\newblock Mmlatch: Bottom-up top-down fusion for multimodal sentiment analysis.
\newblock In \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 4573--4577. IEEE.

\bibitem[{Rahman et~al.(2020)Rahman, Hasan, Lee, Zadeh, Mao, Morency, and Hoque}]{rahman2020integrating}
Rahman, W.; Hasan, M.~K.; Lee, S.; Zadeh, A.; Mao, C.; Morency, L.-P.; and Hoque, E. 2020.
\newblock Integrating multimodal information in large pretrained transformers.
\newblock In \emph{Proceedings of the conference. Association for Computational Linguistics. Meeting}, volume 2020, 2359. NIH Public Access.

\bibitem[{Rao et~al.(2022)Rao, Zhao, Chen, Tang, Zhu, Huang, Zhou, and Lu}]{rao2022denseclip}
Rao, Y.; Zhao, W.; Chen, G.; Tang, Y.; Zhu, Z.; Huang, G.; Zhou, J.; and Lu, J. 2022.
\newblock Denseclip: Language-guided dense prediction with context-aware prompting.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 18082--18091.

\bibitem[{Saha et~al.(2020)Saha, Patra, Saha, and Bhattacharyya}]{saha-etal-2020-towards}
Saha, T.; Patra, A.; Saha, S.; and Bhattacharyya, P. 2020.
\newblock Towards Emotion-aided Multi-modal Dialogue Act Classification.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, 4361--4372. Online: Association for Computational Linguistics.

\bibitem[{Sohn(2016)}]{sohn2016improved}
Sohn, K. 2016.
\newblock Improved deep metric learning with multi-class n-pair loss objective.
\newblock \emph{Advances in neural information processing systems}, 29.

\bibitem[{Tian, Krishnan, and Isola(2020)}]{tian2020contrastive}
Tian, Y.; Krishnan, D.; and Isola, P. 2020.
\newblock Contrastive multiview coding.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XI 16}, 776--794. Springer.

\bibitem[{Tsai et~al.(2019)Tsai, Bai, Liang, Kolter, Morency, and Salakhutdinov}]{tsai2019multimodal}
Tsai, Y.-H.~H.; Bai, S.; Liang, P.~P.; Kolter, J.~Z.; Morency, L.-P.; and Salakhutdinov, R. 2019.
\newblock Multimodal transformer for unaligned multimodal language sequences.
\newblock In \emph{Proceedings of the conference. Association for Computational Linguistics. Meeting}, volume 2019, 6558. NIH Public Access.

\bibitem[{Wang et~al.(2022)Wang, Zhang, Lee, Zhang, Sun, Ren, Su, Perot, Dy, and Pfister}]{Wang_2022_CVPR}
Wang, Z.; Zhang, Z.; Lee, C.-Y.; Zhang, H.; Sun, R.; Ren, X.; Su, G.; Perot, V.; Dy, J.; and Pfister, T. 2022.
\newblock Learning To Prompt for Continual Learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 139--149.

\bibitem[{Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz et~al.}]{wolf2019huggingface}
Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.; Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; et~al. 2019.
\newblock Huggingface's transformers: State-of-the-art natural language processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}.

\bibitem[{Wu et~al.(2018)Wu, Xiong, Yu, and Lin}]{wu2018unsupervised}
Wu, Z.; Xiong, Y.; Yu, S.~X.; and Lin, D. 2018.
\newblock Unsupervised feature learning via non-parametric instance discrimination.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, 3733--3742.

\bibitem[{Ye et~al.(2019)Ye, Zhang, Yuen, and Chang}]{ye2019unsupervised}
Ye, M.; Zhang, X.; Yuen, P.~C.; and Chang, S.-F. 2019.
\newblock Unsupervised embedding learning via invariant and spreading instance feature.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 6210--6219.

\bibitem[{Yu et~al.(2023)Yu, Gao, Lin, Yang, Wu, Ma, Wang, Huang, and Li}]{yu-etal-2023-speech}
Yu, T.; Gao, H.; Lin, T.-E.; Yang, M.; Wu, Y.; Ma, W.; Wang, C.; Huang, F.; and Li, Y. 2023.
\newblock Speech-Text Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, 7900--7913. Toronto, Canada: Association for Computational Linguistics.

\bibitem[{Zadeh et~al.(2017)Zadeh, Chen, Poria, Cambria, and Morency}]{zadeh2017tensor}
Zadeh, A.; Chen, M.; Poria, S.; Cambria, E.; and Morency, L.-P. 2017.
\newblock Tensor fusion network for multimodal sentiment analysis.
\newblock \emph{arXiv preprint arXiv:1707.07250}.

\bibitem[{Zadeh et~al.(2018)Zadeh, Liang, Mazumder, Poria, Cambria, and Morency}]{zadeh2018memory}
Zadeh, A.; Liang, P.~P.; Mazumder, N.; Poria, S.; Cambria, E.; and Morency, L.-P. 2018.
\newblock Memory fusion network for multi-view sequential learning.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~32.

\bibitem[{Zhang et~al.(2021{\natexlab{a}})Zhang, Li, Xu, Zhang, Zhao, and Gao}]{zhang2021textoir}
Zhang, H.; Li, X.; Xu, H.; Zhang, P.; Zhao, K.; and Gao, K. 2021{\natexlab{a}}.
\newblock TEXTOIR: An Integrated and Visualized Platform for Text Open Intent Recognition.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations}, 167--174.

\bibitem[{Zhang, Xu, and Lin(2021)}]{zhang2021deep}
Zhang, H.; Xu, H.; and Lin, T.-E. 2021.
\newblock Deep open intent classification with adaptive decision boundary.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, 14374--14382.

\bibitem[{Zhang et~al.(2021{\natexlab{b}})Zhang, Xu, Lin, and Lyu}]{zhang2021discovering}
Zhang, H.; Xu, H.; Lin, T.-E.; and Lyu, R. 2021{\natexlab{b}}.
\newblock Discovering new intents with deep aligned clustering.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, 14365--14373.

\bibitem[{Zhang et~al.(2023{\natexlab{a}})Zhang, Xu, Wang, Long, and Gao}]{10349963}
Zhang, H.; Xu, H.; Wang, X.; Long, F.; and Gao, K. 2023{\natexlab{a}}.
\newblock A Clustering Framework for Unsupervised and Semi-supervised New Intent Discovery.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 1--14.

\bibitem[{Zhang et~al.(2022)Zhang, Xu, Wang, Zhou, Zhao, and Teng}]{10.1145/3503161.3547906}
Zhang, H.; Xu, H.; Wang, X.; Zhou, Q.; Zhao, S.; and Teng, J. 2022.
\newblock MIntRec: A New Dataset for Multimodal Intent Recognition.
\newblock In \emph{Proceedings of the 30th ACM International Conference on Multimedia}, MM '22, 1688–1697. New York, NY, USA: Association for Computing Machinery.
\newblock ISBN 9781450392037.

\bibitem[{Zhang et~al.(2023{\natexlab{b}})Zhang, Xu, Zhao, and Zhou}]{10097558}
Zhang, H.; Xu, H.; Zhao, S.; and Zhou, Q. 2023{\natexlab{b}}.
\newblock Learning Discriminative Representations and Decision Boundaries for Open Intent Detection.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 31: 1611--1623.

\bibitem[{Zhou et~al.(2022{\natexlab{a}})Zhou, Yang, Loy, and Liu}]{zhou2022conditional}
Zhou, K.; Yang, J.; Loy, C.~C.; and Liu, Z. 2022{\natexlab{a}}.
\newblock Conditional prompt learning for vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 16816--16825.

\bibitem[{Zhou et~al.(2022{\natexlab{b}})Zhou, Yang, Loy, and Liu}]{zhou2022learning}
Zhou, K.; Yang, J.; Loy, C.~C.; and Liu, Z. 2022{\natexlab{b}}.
\newblock Learning to prompt for vision-language models.
\newblock \emph{International Journal of Computer Vision}, 130(9): 2337--2348.

\end{thebibliography}
