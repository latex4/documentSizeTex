\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{kingma:14:iclr}
\citation{pmlr-v80-alemi18a}
\citation{Lucas2019}
\citation{Higgins:17:iclr}
\citation{burgess2018understanding}
\citation{8253482}
\citation{NIPS2018_7692}
\citation{bengio:AAAI:17}
\citation{Lucas2019}
\citation{Higgins:17:iclr}
\citation{burgess2018understanding}
\citation{oord:17:nips}
\@writefile{toc}{\contentsline {section}{Introduction}{1}{section*.1}\protected@file@percent }
\citation{Henter2018}
\citation{bengio:AAAI:17}
\citation{DBLP:journals/corr/abs-1811-07557}
\citation{pmlr-v32-rezende14}
\@writefile{toc}{\contentsline {section}{Bottleneck Vector Quantizer}{2}{section*.2}\protected@file@percent }
\newlabel{quantization}{{}{2}{Bottleneck Vector Quantizer}{section*.2}{}}
\@writefile{toc}{\contentsline {subsection}{Vector Quantization in Autoencoders}{2}{section*.3}\protected@file@percent }
\newlabel{subsec:vq-vae}{{}{2}{Vector Quantization in Autoencoders}{section*.3}{}}
\newlabel{eq:assgn1}{{1}{2}{Vector Quantization in Autoencoders}{equation.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{Bottleneck Vector Quantizer as a Latent Parameter Estimator}{2}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Vector Quantizer as a Regularizer}{2}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Soft VQ-VAE}{2}{section*.7}\protected@file@percent }
\newlabel{noise_injection}{{}{2}{Soft VQ-VAE}{section*.7}{}}
\@writefile{toc}{\contentsline {subsection}{Noisy Latent Codes}{2}{section*.8}\protected@file@percent }
\citation{DBLP:journals/corr/Goodfellow17}
\citation{8588399}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: q1}{{1a}{3}{Vanilla autoencoders}{figure.caption.6}{}}
\newlabel{sub@fig: q1}{{a}{3}{Vanilla autoencoders}{figure.caption.6}{}}
\newlabel{fig: q2}{{1b}{3}{Autoencoders with quantized bottleneck}{figure.caption.6}{}}
\newlabel{sub@fig: q2}{{b}{3}{Autoencoders with quantized bottleneck}{figure.caption.6}{}}
\newlabel{fig:sub3}{{1c}{3}{The quantizer enforces a\\ similarity-preserving mapping at the encoder}{figure.caption.6}{}}
\newlabel{sub@fig:sub3}{{c}{3}{The quantizer enforces a\\ similarity-preserving mapping at the encoder}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The quantizer behaves as a regularizer that encourages a similarity-preserving mapping}}{3}{figure.caption.6}\protected@file@percent }
\newlabel{fig:test}{{1}{3}{The quantizer behaves as a regularizer that encourages a similarity-preserving mapping}{figure.caption.6}{}}
\newlabel{eq:gmm_generative}{{4}{3}{Noisy Latent Codes}{equation.0.4}{}}
\newlabel{eq:componentGaussian}{{5}{3}{Noisy Latent Codes}{equation.0.5}{}}
\@writefile{toc}{\contentsline {subsection}{Bayesian Estimator}{3}{section*.9}\protected@file@percent }
\newlabel{eq: estimator}{{6}{3}{Bayesian Estimator}{equation.0.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Description of the soft VQ-VAE.}}{3}{figure.caption.10}\protected@file@percent }
\newlabel{fig:estimator}{{2}{3}{Description of the soft VQ-VAE}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{Optimal Estimator}{3}{section*.11}\protected@file@percent }
\citation{bregman}
\newlabel{eq:elboalter}{{10}{4}{Optimal Estimator}{equation.0.10}{}}
\newlabel{eq:klgoal}{{11}{4}{Optimal Estimator}{equation.0.11}{}}
\newlabel{propostion1}{{1}{4}{}{theorem.1}{}}
\newlabel{eq: optiesti}{{12}{4}{}{equation.0.12}{}}
\newlabel{eq:setp1}{{13}{4}{Optimal Estimator}{equation.0.13}{}}
\newlabel{eq:setp2}{{17}{4}{Optimal Estimator}{equation.0.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The relation of variables in the soft VQ-VAE model. The symbol $\nleftrightarrow $ is used to indicate that we cannot directly use paths that connected to the unobserved $\mathbf  {z_e}$ for probabilistic inference.}}{4}{figure.caption.12}\protected@file@percent }
\newlabel{fig:markov_soft}{{3}{4}{The relation of variables in the soft VQ-VAE model. The symbol $\nleftrightarrow $ is used to indicate that we cannot directly use paths that connected to the unobserved $\mathbf {z_e}$ for probabilistic inference}{figure.caption.12}{}}
\newlabel{eq:berg_obj}{{20}{4}{Optimal Estimator}{equation.0.20}{}}
\citation{roy:18}
\citation{Henter2018}
\citation{theis:17:iclr}
\citation{Balle:17:iclr}
\citation{agustsson:17:nips}
\citation{agustsson:17:nips}
\citation{sonderbypoole2017}
\citation{DBLP:journals/corr/abs-1807-07543}
\citation{NIPS2018_7692}
\citation{alemi:17:iclr}
\citation{pmlr-v80-alemi18a}
\citation{tishby:15:itw}
\citation{8253482}
\citation{DBLP:journals/corr/abs-1811-07557}
\citation{NIPS2017_7181}
\newlabel{eq: convex_obj}{{24}{5}{Optimal Estimator}{equation.0.24}{}}
\newlabel{eq: minimizer}{{26}{5}{Optimal Estimator}{equation.0.26}{}}
\@writefile{toc}{\contentsline {section}{Related Work}{5}{section*.13}\protected@file@percent }
\newlabel{Background}{{}{5}{Related Work}{section*.13}{}}
\@writefile{toc}{\contentsline {section}{Experimental Results}{5}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Model Implementation}{5}{section*.15}\protected@file@percent }
\citation{pmlr-v9-glorot10a}
\citation{kingma:15:iclr}
\citation{tSNE}
\citation{DBLP:journals/corr/abs-1807-07543}
\citation{xie:16:icml}
\citation{Somvae}
\citation{DBLP:journals/corr/abs-1801-07648}
\newlabel{eq:loss1}{{27}{6}{Model Implementation}{equation.0.27}{}}
\@writefile{toc}{\contentsline {subsection}{Training Setup}{6}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Visualization of Latent Representation}{6}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Representation Learning Tasks}{6}{section*.19}\protected@file@percent }
\newlabel{sb: downstreamtasks}{{}{6}{Representation Learning Tasks}{section*.19}{}}
\newlabel{fig:latents1}{{4a}{6}{}{figure.caption.18}{}}
\newlabel{sub@fig:latents1}{{a}{6}{}{figure.caption.18}{}}
\newlabel{fig:latents2}{{4b}{6}{}{figure.caption.18}{}}
\newlabel{sub@fig:latents2}{{b}{6}{}{figure.caption.18}{}}
\newlabel{fig:latents3}{{4c}{6}{}{figure.caption.18}{}}
\newlabel{sub@fig:latents3}{{c}{6}{}{figure.caption.18}{}}
\newlabel{fig:latents4}{{4d}{6}{}{figure.caption.18}{}}
\newlabel{sub@fig:latents4}{{d}{6}{}{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Two-dimensional learned representations  of MNIST. Each color indicates one digit class.}}{6}{figure.caption.18}\protected@file@percent }
\newlabel{fig:latent}{{4}{6}{Two-dimensional learned representations \\of MNIST. Each color indicates one digit class}{figure.caption.18}{}}
\citation{DBLP:journals/corr/abs-1807-07543}
\citation{DBLP:journals/corr/abs-1901-03416}
\bibstyle{aaai}
\bibdata{fine3}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Accuracy of downstream tasks of MNIST.}}{7}{table.caption.20}\protected@file@percent }
\newlabel{MNIST_table}{{1}{7}{Accuracy of downstream tasks of MNIST}{table.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Accuracy of downstream tasks of SVHN and CIFAR-10.}}{7}{table.caption.21}\protected@file@percent }
\newlabel{sample-table}{{2}{7}{Accuracy of downstream tasks of SVHN and CIFAR-10}{table.caption.21}{}}
\@writefile{toc}{\contentsline {section}{Conclusion}{7}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Acknowledgements}{7}{section*.23}\protected@file@percent }
\gdef \@abspage@last{7}
