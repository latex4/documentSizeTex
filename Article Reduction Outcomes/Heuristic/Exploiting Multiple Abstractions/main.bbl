\begin{thebibliography}{26}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Abel, Hershkowitz, and Littman(2016)}]{abel_2016_OptimalBehavior}
Abel, D.; Hershkowitz, D.; and Littman, M. 2016.
\newblock Near Optimal Behavior via Approximate State Abstraction.
\newblock In \emph{ICML 2016}, volume~48, 2915--2923. {PMLR}.

\bibitem[{Abel et~al.(2020)Abel, Umbanhowar, Khetarpal, Arumugam, Precup, and
  Littman}]{abel_2020_ValuePreserving}
Abel, D.; Umbanhowar, N.; Khetarpal, K.; Arumugam, D.; Precup, D.; and Littman,
  M. 2020.
\newblock Value Preserving State-Action Abstractions.
\newblock In \emph{AISTATS}, volume 108, 1639--1650. {PMLR}.

\bibitem[{Brafman, De~Giacomo, and Patrizi(2018)}]{brafman_2018_LTLfLDLf}
Brafman, R.~I.; De~Giacomo, G.; and Patrizi, F. 2018.
\newblock {{LTLf}} / {{LDLf}} Non-Markovian Rewards.
\newblock \emph{AAAI 2018}, 1771--1778.

\bibitem[{Devlin and Kudenko(2012)}]{devlin2012}
Devlin, S.; and Kudenko, D. 2012.
\newblock Dynamic Potential-Based Reward Shaping.
\newblock In \emph{{{AAMAS}} 2012}, 433--440. {IFAAMAS}.

\bibitem[{Dietterich(2000)}]{dietterich2000hierarchical}
Dietterich, T.~G. 2000.
\newblock Hierarchical reinforcement learning with the MAXQ value function
  decomposition.
\newblock \emph{JAIR}, 13: 227--303.

\bibitem[{Gao and Toni(2015)}]{gao2015potential}
Gao, Y.; and Toni, F. 2015.
\newblock Potential Based Reward Shaping for Hierarchical Reinforcement
  Learning.
\newblock In \emph{{{IJCAI}} 2015}, 3504--3510. {AAAI Press}.

\bibitem[{Grzes(2017)}]{grzes2017reward}
Grzes, M. 2017.
\newblock Reward Shaping in Episodic Reinforcement Learning.
\newblock In \emph{{{AAMAS}} 2017}, 565--573. {ACM}.

\bibitem[{Grzes and Kudenko(2008)}]{grzes2008multigrid}
Grzes, M.; and Kudenko, D. 2008.
\newblock Multigrid Reinforcement Learning with Reward Shaping.
\newblock In \emph{{ICANN} 2008, Part {I}}, volume 5163, 357--366. Springer.

\bibitem[{Hutsebaut-Buysse, Mets, and
  Latr\'e(2022)}]{hutsebaut-buysse_2022_HierarchicalReinforcementb}
Hutsebaut-Buysse, M.; Mets, K.; and Latr\'e, S. 2022.
\newblock Hierarchical Reinforcement Learning: {{A}} Survey and Open Research
  Challenges.
\newblock \emph{Machine Learning and Knowledge Extraction}, 4(1): 172--221.

\bibitem[{Icarte et~al.(2018)Icarte, Klassen, Valenzano, and
  McIlraith}]{icarte2018using}
Icarte, R.~T.; Klassen, T.; Valenzano, R.; and McIlraith, S. 2018.
\newblock Using reward machines for high-level task specification and
  decomposition in reinforcement learning.
\newblock In \emph{ICML}, volume~80, 2107--2116. {PMLR}.

\bibitem[{Jothimurugan, Bastani, and
  Alur(2021)}]{jothimurugan_2021_AbstractValue}
Jothimurugan, K.; Bastani, O.; and Alur, R. 2021.
\newblock Abstract Value Iteration for Hierarchical Reinforcement Learning.
\newblock In \emph{AISTATS}, volume 130, 1162--1170. {PMLR}.

\bibitem[{Li, Walsh, and Littman(2006)}]{li06towards}
Li, L.; Walsh, T.~J.; and Littman, M.~L. 2006.
\newblock Towards a Unified Theory of State Abstraction for MDPs.
\newblock In \emph{ISAIM 2006}, 531--539.

\bibitem[{Liaw et~al.(2018)Liaw, Liang, Nishihara, Moritz, Gonzalez, and
  Stoica}]{liaw2018tune}
Liaw, R.; Liang, E.; Nishihara, R.; Moritz, P.; Gonzalez, J.~E.; and Stoica, I.
  2018.
\newblock Tune: {{A}} Research Platform for Distributed Model Selection and
  Training.
\newblock \emph{CoRR}, abs/1807.05118.

\bibitem[{Marthi(2007)}]{Marthi07automatic-shaping}
Marthi, B. 2007.
\newblock Automatic shaping and decomposition of reward functions.
\newblock In \emph{ICML 2007}, volume 227, 601--608. {ACM}.

\bibitem[{Ng, Harada, and Russell(1999)}]{ng1999policy}
Ng, A.~Y.; Harada, D.; and Russell, S.~J. 1999.
\newblock Policy {{Invariance Under Reward Transformations}}: {{Theory}} and
  {{Application}} to {{Reward Shaping}}.
\newblock In \emph{{{ICML}} 1999}, 278--287. {Morgan Kaufmann}.

\bibitem[{Parr and Russell(1998)}]{parr1998reinforcement}
Parr, R.; and Russell, S. 1998.
\newblock Reinforcement learning with hierarchies of machines.
\newblock \emph{Advances in neural information processing systems}, 1043--1049.

\bibitem[{Puterman(1994)}]{puterman1994}
Puterman, M.~L. 1994.
\newblock \emph{Markov Decision Processes: {{Discrete}} Stochastic Dynamic
  Programming}.
\newblock {Wiley}.
\newblock ISBN 978-0-471-61977-2.

\bibitem[{Ravindran and Barto(2002)}]{ravindran_model_2002}
Ravindran, B.; and Barto, A.~G. 2002.
\newblock Model Minimization in Hierarchical Reinforcement Learning.
\newblock In \emph{{SARA} 2002}, volume 2371, 196--211. Springer.

\bibitem[{Schubert, Oguz, and Toussaint(2021)}]{schubert_2021_PlanbasedRelaxed}
Schubert, I.; Oguz, O.~S.; and Toussaint, M. 2021.
\newblock Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks.
\newblock In \emph{{{ICLR}} 2021}. {OpenReview.net}.

\bibitem[{Steccanella, Totaro, and
  Jonsson(2021)}]{steccanella_hierarchical_2021}
Steccanella, L.; Totaro, S.; and Jonsson, A. 2021.
\newblock Hierarchical Representation Learning for Markov Decision Processes.
\newblock \emph{CoRR}, abs/2106.01655.

\bibitem[{Strehl et~al.(2006)Strehl, Li, Wiewiora, Langford, and
  Littman}]{strehl2006pac}
Strehl, A.~L.; Li, L.; Wiewiora, E.; Langford, J.; and Littman, M.~L. 2006.
\newblock PAC model-free reinforcement learning.
\newblock In \emph{ICML 2006}, 881--888.

\bibitem[{Sutton, Precup, and Singh(1999)}]{sutton1999between}
Sutton, R.~S.; Precup, D.; and Singh, S. 1999.
\newblock Between MDPs and semi-MDPs: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artif. Intell.}, 112(1-2): 181--211.

\bibitem[{Wang et~al.(2016)Wang, Schaul, Hessel, van Hasselt, Lanctot, and
  de~Freitas}]{wang_dueling_2016}
Wang, Z.; Schaul, T.; Hessel, M.; van Hasselt, H.; Lanctot, M.; and de~Freitas,
  N. 2016.
\newblock Dueling Network Architectures for Deep Reinforcement Learning.
\newblock In \emph{{ICML}}, volume~48, 1995--2003. JMLR.org.

\bibitem[{Watkins and Dayan(1992)}]{watkins1992q}
Watkins, C.~J.; and Dayan, P. 1992.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8(3-4): 279--292.

\bibitem[{Wiewiora(2003)}]{wiewiora2003potential}
Wiewiora, E. 2003.
\newblock Potential-{{Based Shaping}} and {{Q-Value Initialization}} Are
  {{Equivalent}}.
\newblock \emph{Journal of Artificial Intelligence Research}, 19: 205--208.

\bibitem[{Wiewiora, Cottrell, and Elkan(2003)}]{wiewiora2003principled}
Wiewiora, E.; Cottrell, G.~W.; and Elkan, C. 2003.
\newblock Principled {{Methods}} for {{Advising Reinforcement Learning
  Agents}}.
\newblock In \emph{{{ICML}} 2003}, 792--799. {AAAI Press}.

\end{thebibliography}
