@STRING{AI    = "Artif. Intell."}
@STRING{AICT  = "IFIP AICT"}
@STRING{AIM   = "AI Magazine"}
@STRING{AInf  = "Acta Inf."}
@STRING{ALC   = "$\mathcal{ALC}$"}
@STRING{AMAI  = "Ann. Math. Artif. Intell."}
@STRING{AMST  = "American Mathematical Society Translations"}
@STRING{APhF  = "Acta Philos. Fenn."}
@STRING{APIN  = "Appl. Intell."}
@STRING{BAMS  = "Bull. Austral. Math. Soc."}
@STRING{BEATCS = "Bull. EATCS"}
@STRING{CI    = "Comput. Intell."}
@STRING{CJ    = "Comput. J."}

@STRING{Conf = "Conf."}
@STRING{CSUR  = "{ACM} Comput. Surv."}
@STRING{DKE   = "Data Knowl. Eng."}
@STRING{ENTCS = "Electr. Notes Theor. Comput. Sci."}
@STRING{FI    = "Fund. Inform."}
@STRING{FM    = "Fund. Math."}
@STRING{IC    = "Inf. Comput."}
@STRING{IGPL  = "L. J. IGPL"}
@STRING{IJFCS = "Int. J. Found. Comput. Sci."}
@STRING{Intl = "Intl."}
@STRING{IPL   = "Information Processing Letters"}
@STRING{IST   = "Information and Software Technology"}
@STRING{JACM  = "J. ACM"}
@STRING{JAIR  = "J. Artif. Intell. Res."}
@STRING{JAL   = "J. Appl. Log."}
@STRING{JANCL = "J. Appl. Non-Class. Log."}
@STRING{JAR   = "J. Autom. Reasoning"}
@STRING{JCSS  = "J. Comput. System Sci."}
@STRING{JLAP   = "J. Log. Algebr. Program."}
@STRING{JLC   = "J. Log. Comput."}
@STRING{JLP   = "J. Log. Program."}
@STRING{JoLLI = "J. Log. Lang. Inf."}
@STRING{JPhL  = "J. Philos. Log."}
@STRING{JS    = "J. Semantics"}
@STRING{JSC   = "J. Symb. Comput."}
@STRING{JSL   = "J. Symb. Log."}
@STRING{JWS   = "J. Web Sem."}
@STRING{LA    = "Logique et Analyse"}
@STRING{LMCS  = "Log. Methods Comput. Sci."}
@STRING{LNAI  = "LNCS (LNAI)"}
@STRING{LNCS  = "LNCS"}
@STRING{MZ    = "Math. Z." }
@STRING{NDJFL = "Notre Dame J. Form. Log."}
@STRING{PHIL  = "Philosophia"}
@STRING{SCP   = "Sci. Comput. Programming"}

@STRING{SHIQ = "$\mathcal{SHIQ}$"}
@STRING{SHOIQ = "$\mathcal{SHOIQ}$"}
@STRING{SHOQD = "$\mathcal{SHOQ}(\mathrm{D})$"}
@STRING{SIAM  = "{SIAM} J. Comput."}
@STRING{SL    = "Stud. Log."}
@STRING{SROIQ = "$\mathcal{SROIQ}$"}
@STRING{SYNTH = "Synthese"}
@STRING{TCS   = "Theor. Comput. Sci."}
@STRING{THEO  = "Theoria"}
@STRING{TOCL  = "ACM TOCL"}
@STRING{TOCL  = "ACM Trans. Comput. Log."}
@STRING{TODS  = "ACM TODS"}
@STRING{TODS  = "ACM Trans. Database Syst."}
@STRING{TOPLAS = "ACM TOPLAS"}
@STRING{ZMLGM = "Z. Math. Logik Grundlagen Math."}

@STRING{VLDBJ = "VLDB J."}
@STRING{PVLDB = "PVLDB"}
@STRING{TKDE  = "IEEE TKDE"}
@STRING{IEEEC = "IEEE Computer"}
@STRING{IEEETSMC = "IEEE Trans. Syst. Man Cybern."}
@STRING{TPLP  = "Theory Pract. Log. Program."}
@STRING{LNCS = "Lecture Notes in Comput. Sci."}
@STRING{BS = "Bell Syst."}
@STRING{IBMJRD = "IBM J. Res. Dev."}


@book{puterman1994,
  title = {Markov Decision Processes: {{Discrete}} Stochastic Dynamic Programming},
  author = {Puterman, Martin L.},
  year = {1994},
  publisher = {{Wiley}},
  doi = {10.1002/9780470316887},
  isbn = {978-0-471-61977-2}
}

@inproceedings{bacchus1996,
  author =        {Bacchus, Fahiem and Boutilier, Craig and Grove, Adam},
  booktitle =     {{AAAI}},
  pages =         {1160--1167},
  title =         {Rewarding Behaviors},
  year =          {1996},
}

@article{thiebaux2006,
  author =        {Sylvie Thi{\'{e}}baux and Charles Gretton and
                   John K. Slaney and David Price and Froduald Kabanza},
  journal =       JAIR,
  pages =         {17--74},
  title =         {Decision-Theoretic Planning with non-{M}arkovian
                   Rewards},
  volume =        {25},
  year =          {2006},
}

@inproceedings{icarte2018-qrm,
  author =        {Toro Icarte, Rodrigo and Klassen, Toryn and
                   Valenzano, Richard and McIlraith, Sheila},
  booktitle =     {ICML},
  pages =         {2107--2116},
  title =         {Using Reward Machines for High-Level Task
                   Specification and Decomposition in Reinforcement
                   Learning},
  year =          {2018},
}

@article{quint2019,
  author =        {Eleanor Quint and Dong Xu and Haluk Dogan and
                   Zeynep Hakguder and Stephen Scott and
                   Matthew B. Dwyer},
  journal =       {CoRR},
  title =         {Formal Language Constraints for Markov Decision
                   Processes},
  volume =        {abs/1910.01074},
  year =          {2019},
}

@article{brafman_2018_LTLfLDLf,
  title = {{{LTLf}} / {{LDLf}} Non-Markovian Rewards},
  author = {Brafman, Ronen I. and De Giacomo, Giuseppe and Patrizi, Fabio},
  year = {2018},
  journal = {AAAI 2018},
  pages = {1771--1778},
  isbn = {9781577358008}
}

@inproceedings{DeGiacomo2019,
  author =        {Giuseppe {De Giacomo} and Luca Iocchi and
                   Marco Favorito and Fabio Patrizi},
  booktitle =     {ICAPS},
  pages =         {128--136},
  title =         {Foundations for Restraining Bolts: Reinforcement
                   Learning with {LTLf/LDLf} Restraining Specifications},
  year =          {2019},
}

@inproceedings{DeGiacomo2013,
  author =        {De Giacomo, Giuseppe and Vardi, Moshe Y.},
  booktitle =     {IJCAI},
  pages =         {854--860},
  title =         {Linear Temporal Logic and Linear Dynamic Logic on
                   Finite Traces},
  year =          {2013},
}

@inproceedings{DeGiacomo2015,
  author =        {De Giacomo, Giuseppe and Vardi, Moshe Y.},
  booktitle =     {IJCAI},
  pages =         {1558--1564},
  title =         {Synthesis for {LTL} and {LDL} on Finite Traces},
  year =          {2015},
}

@inproceedings{DeGiacomo2016,
  author =        {De Giacomo, Giuseppe and Vardi, Moshe Y.},
  booktitle =     {IJCAI},
  pages =         {1044--1050},
  title =         {{LTLf} and {LDLf} Synthesis Under Partial
                   Observability},
  year =          {2016},
}

@article{rabin1959,
  author =        {Michael O. Rabin and Dana S. Scott},
  journal =       IBMJRD,
  number =        {2},
  pages =         {114--125},
  title =         {Finite Automata and Their Decision Problems},
  volume =        {3},
  year =          {1959},
}

@book{sutton-barto-2018,
  author =        {Sutton, Richard S. and Barto, Andrew G.},
  publisher =     {A Bradford Book},
  title =         {Reinforcement Learning: An Introduction},
  year =          {2018},
}

@article{bauer2010RVLTL,
  author =        {Andreas Bauer and Martin Leucker and
                   Christian Schallhart},
  journal =       JLC,
  number =        {3},
  pages =         {651--674},
  title =         {Comparing {LTL} Semantics for Runtime Verification},
  volume =        {20},
  year =          {2010},
}

@inproceedings{ly2013monitoring,
  author =        {Linh Thao Ly and Fabrizio Maria Maggi and
                   Marco Montali and Stefanie Rinderle{-}Ma and
                   Wil M. P. van der Aalst},
  booktitle =     {EDOC},
  pages =         {7--16},
  title =         {A Framework for the Systematic Comparison and
                   Evaluation of Compliance Monitoring Approaches},
  year =          {2013},
}

@inproceedings{degiacomo2014monitoring,
  author =        {Giuseppe {De Giacomo} and Riccardo {De Masellis} and
                   Marco Grasso and Fabrizio Maria Maggi and
                   Marco Montali},
  booktitle =     {BPM},
  pages =         {1--17},
  series =        LNCS,
  title =         {Monitoring Business Metaconstraints Based on {LTL}
                   and {LDL} for Finite Traces},
  volume =        {8659},
  year =          {2014},
}

@article{moore1956,
  author =        {Moore, Edward F.},
  journal =       {Automata Studies},
  month =         {12},
  pages =         {129--154},
  title =         {Gedanken-Experiments on Sequential Machines},
  year =          {1956},
}

@article{mealy1955,
  author =        {Mealy, George H.},
  journal =       BS,
  month =         {9},
  pages =         {1045--1079},
  title =         {A method for synthesizing sequential circuits},
  volume =        {34},
  year =          {1955},
}

@book{linz2006,
  author =        {Peter Linz},
  publisher =     {Jones and Bartlett Publishers},
  title =         {An introduction to formal languages and automata},
  year =          {2006},
}

@inproceedings{Pnueli77,
  author =        {Amir Pnueli},
  booktitle =     {FOCS},
  pages =         {46--57},
  title =         {The Temporal Logic of Programs},
  year =          {1977},
}

@article{levesque1997,
  author =        {Hector J. Levesque and Raymond Reiter and
                   Yves Lesp{\'{e}}rance and Fangzhen Lin and
                   Richard B. Scherl},
  journal =       JLP,
  number =        {1-3},
  pages =         {59--83},
  title =         {{GOLOG:} {A} Logic Programming Language for Dynamic
                   Domains},
  volume =        {31},
  year =          {1997},
}

@inproceedings{baier2008,
  author =        {Jorge A. Baier and Christian Fritz and
                   Meghyn Bienvenu and Sheila A. McIlraith},
  booktitle =     {{AAAI}},
  title =         {Beyond Classical Planning: Procedural Control
                   Knowledge and Preferences in State-of-the-Art
                   Planners},
  year =          {2008},
}

@article{slaney2005,
  author =        {John K. Slaney},
  journal =       IGPL,
  number =        {2},
  pages =         {211--229},
  title =         {Semipositive {LTL} with an Uninterpreted Past
                   Operator},
  volume =        {13},
  year =          {2005},
}

@inproceedings{gretton2007,
  author =        {Gretton, Charles},
  booktitle =     {ICAPS},
  pages =         {168–175},
  title =         {Gradient-Based Relational Reinforcement Learning of
                   Temporally Extended Policies},
  year =          {2007},
}

@inproceedings{gretton2014,
  author =        {Charles Gretton},
  booktitle =     {PRICAI},
  pages =         {13--25},
  series =        LNCS,
  title =         {A More Expressive Behavioral Logic for
                   Decision-Theoretic Planning},
  volume =        {8862},
  year =          {2014},
}

@inproceedings{lacerda2014,
  author =        {Bruno Lacerda and David Parker and Nick Hawes},
  booktitle =     {IROS},
  pages =         {1511--1516},
  title =         {Optimal and dynamic planning for Markov decision
                   processes with co-safe {LTL} specifications},
  year =          {2014},
}

@inproceedings{lacerda2015policy,
  author = {Lacerda, Bruno and Parker, David and Hawes, Nick},
  title = {Optimal Policy Generation for Partially Satisfiable Co-Safe {LTL} Specifications},
  year = {2015},
  booktitle = {IJCAI},
  pages = {1587–1593},
}
  

@inproceedings{CamachoCSM17,
  author =        {Camacho, Alberto and Chen, Oscar and Sanner, Scott and
                   McIlraith, Sheila A.},
  booktitle =     {RLDM},
  pages =         {279-283},
  title =         {Decision-Making with Non-{M}arkovian Rewards: From
                   {LTL} to automata-based reward shaping},
  year =          {2017},
}

@unpublished{littman2015IJCAItalk,
  author =        {Michael Lederman Littman},
  note =          {Invited talk at IJCAI},
  title =         {Programming agent via rewards.},
  year =          {2015},
}

@article{littman2017,
  author =        {Michael L. Littman and Ufuk Topcu and Jie Fu and
                   Charles Lee Isbell Jr. and Min Wen and
                   James MacGlashan},
  journal =       {CoRR},
  title =         {Environment-Independent Task Specifications via
                   {GLTL}},
  volume =        {abs/1704.04341},
  year =          {2017},
}

@inproceedings{degiacomo2018planning,
  author =        {Giuseppe {De Giacomo} and Sasha Rubin},
  booktitle =     {IJCAI},
  pages =         {4729--4735},
  title =         {Automata-Theoretic Foundations of {FOND} Planning for
                   {LTLf} and {LDLf} Goals},
  year =          {2018},
}

@article{chandra1981alternation,
  author =        {Ashok K. Chandra and Dexter Kozen and
                   Larry J. Stockmeyer},
  journal =       JACM,
  number =        {1},
  pages =         {114--133},
  title =         {Alternation},
  volume =        {28},
  year =          {1981},
}

@inproceedings{moore1991,
  author =        {Andrew W. Moore},
  booktitle =     {ML91},
  pages =         {333--337},
  title =         {Variable Resolution Dynamic Programming},
  year =          {1991},
}

@inproceedings{camacho2019,
  author =        {Camacho, Alberto and Toro Icarte, Rodrigo and
                   Klassen, Toryn Q. and Valenzano, Richard Anthony and
                   McIlraith, Sheila A.},
  booktitle =     {IJCAI},
  pages =         {6065--6073},
  title =         {{LTL} and Beyond: Formal Languages for Reward
                   Function Specification in Reinforcement Learning},
  year =          {2019},
}

@book{gamma1995,
  author =        {Gamma, Erich and Helm, Richard and Johnson, Ralph and
                   Vlissides, John},
  publisher =     {Addison-Wesley},
  title =         {Design Patterns: Elements of Reusable Object-Oriented
                   Software},
  year =          {1995},
}

@inproceedings{grzes2017,
  author =        {Grze\'{s}, Marek},
  booktitle =     {AAMAS},
  pages =         {565--573},
  title =         {Reward Shaping in Episodic Reinforcement Learning},
  year =          {2017},
}

@techreport{manna1990tools,
  author =        {Manna, Z and Pnueli, Amir},
  publisher =     {Stanford University},
  title =         {Tools and Rules for the Practicing Verifier},
  year =          {1990},
}

@article{barto1983,
  author =        {Barto, Andrew G. and Sutton, Richard S. and
                   Anderson, Charles W.},
  journal =       IEEETSMC,
  month =         {9},
  pages =         {834--846},
  xpublisher =     {Institute of Electrical and Electronics Engineers
                   (IEEE)},
  title =         {Neuronlike adaptive elements that can solve difficult
                   learning control problems},
  volume =        {13},
  year =          {1983},
  xdoi =           {10.1109/tsmc.1983.6313077},
}

@book{norvig2010,
  author =        {Stuart J. Russell and Peter Norvig},
  publisher =     {Pearson Education},
  title =         {Artificial Intelligence: {A} Modern Approach},
  year =          {2010},
}

@article{dietterich2000,
  author =        {Dietterich, Thomas G.},
  journal =       JAIR,
  number =        {1},
  pages =         {227--303},
  title =         {Hierarchical Reinforcement Learning with the {MAXQ}
                   Value Function Decomposition},
  volume =        {13},
  year =          {2000},
  xpublisher =     {AI Access Foundation},
  xaddress =       {El Segundo, CA, USA},
}

@article{gerevini2009pddl,
  author =        {Alfonso Gerevini and Patrik Haslum and Derek Long and
                   Alessandro Saetti and Yannis Dimopoulos},
  journal =       AI,
  number =        {5-6},
  pages =         {619--668},
  title =         {Deterministic planning in the fifth international
                   planning competition: {PDDL3} and experimental
                   evaluation of the planners},
  volume =        {173},
  year =          {2009},
}

@inproceedings{devlin2012,
  title = {Dynamic Potential-Based Reward Shaping},
  booktitle = {{{AAMAS}} 2012},
  author = {Devlin, Sam and Kudenko, Daniel},
  year = {2012},
  pages = {433--440},
  publisher = {{IFAAMAS}}
}

@misc{faulty-reward-function,
  title = {Faulty Reward Functions in the Wild},
  author = {Clark, Jack and Amodei, Dario},
  howpublished = {\url{https://openai.com/blog/faulty-reward-functions/}},
  note = {Accessed: 15-03-2020},
  year = {2016}
}


@inproceedings{li06towards,
  author = {Li, Lihong and Walsh, Thomas J. and Littman, Michael L.},
  booktitle = {ISAIM 2006},
  pages = {531--539},
  title = {Towards a Unified Theory of State Abstraction for MDPs},
  year = {2006},
}


@article{amodei2016,
  author    = {Dario Amodei and
               Chris Olah and
               Jacob Steinhardt and
               Paul F. Christiano and
               John Schulman and
               Dan Man{\'{e}}},
  title     = {Concrete Problems in {AI} Safety},
  journal   = {CoRR},
  volume    = {abs/1606.06565},
  year      = {2016},
  xarchivePrefix = {arXiv},
  xeprint    = {1606.06565},
}


@inproceedings{andreas2017modular,
  title={Modular multitask reinforcement learning with policy sketches},
  author={Andreas, Jacob and Klein, Dan and Levine, Sergey},
  booktitle={ICML},
  pages={166--175},
  year={2017},
  xorganization={JMLR. org}
}

@misc{karpathy2015,
	author = {Andrej Karpathy},
	title = {{REINFORCEjs}: {WaterWorld} demo},
    note = {Accessed: 15-03-2020},
    howpublished =
    {\url{https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html}},
	year = {2015}
}

@article{kupferman2005from,
   author = {Kupferman, Orna and Vardi, Moshe Y.},
   title = {From Linear Time to Branching Time},
   year = {2005},
   volume = {6},
   number = {2},
   journal = TOCL,
   pages = {273–294},
}
  
@inproceedings{schewe2006synthesis,
   author = {Schewe, Sven},
   title = {Synthesis for Probabilistic Environments},
   year = {2006},
   booktitle = {ATVA},
   pages = {245–259},
}
  
@inproceedings{alshiekh2018safe,
  title={Safe reinforcement learning via shielding},
  author={Alshiekh, Mohammed and Bloem, Roderick and Ehlers, R{\"u}diger and K{\"o}nighofer, Bettina and Niekum, Scott and Topcu, Ufuk},
  booktitle={AAAI},
  pages= {2669--2678},
  year={2018}
}

@article{degiacomo2020monitoring,
  author    = {Giuseppe {De Giacomo} and
               Riccardo {De Masellis} and
               Fabrizio Maria Maggi and
               Marco Montali},
  title     = {Monitoring Constraints and Metaconstraints with Temporal Logics
               on Finite Traces},
  journal   = {CoRR},
  volume    = {abs/2004.01859},
  year      = {2020},
  xurl       = {https://arxiv.org/abs/2004.01859},
  xarchivePrefix = {arXiv},
  xeprint    = {2004.01859},
}

@inproceedings{toroicarte2019learning,
  author = {Toro Icarte, Rodrigo and 
            Waldie, Ethan and 
            Klassen, Toryn and 
            Valenzano, Rick and 
            Castro, Margarita and 
            McIlraith, Sheila},
  title = {Learning Reward Machines for Partially Observable 
           Reinforcement Learning},
  booktitle = {NIPS},
  pages = {15523--15534},
  year = {2019},
}

@misc{frozenlake,
    author = {OpenAI},
    year = {2016},
    title = {FrozenLake-v0},
    howpublished = {\url{https://gym.openai.com/envs/FrozenLake-v0/}},
    note = {Accessed: 30-06-2020}
}

@inproceedings{degiacomo2020imitation,
  title={Imitation Learning over Heterogeneous Agents with Restraining Bolts},
  author={De Giacomo, Giuseppe and Favorito, Marco and Iocchi, Luca and Patrizi, Fabio},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={30},
  pages={517--521},
  year={2020}
}

@inproceedings{degiacomo2020temporal,
  title={Temporal Logic Monitoring Rewards via Transducers},
  author={De Giacomo, Giuseppe and Favorito, Marco and Iocchi, Luca and Patrizi, Fabio and Ronca, Alessandro},
  year={2020},
  booktitle={Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning},
  volume={17},
}

@incollection{mataric1994reward,
  title={Reward functions for accelerated learning},
  author={Mataric, Maja J},
  booktitle={Machine learning proceedings 1994},
  pages={181--189},
  year={1994},
  publisher={Elsevier}
}


@inproceedings{ng1999policy,
  title = {Policy {{Invariance Under Reward Transformations}}: {{Theory}} and {{Application}} to {{Reward Shaping}}},
  booktitle = {{{ICML}} 1999},
  author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
  year = {1999},
  pages = {278--287},
  publisher = {{Morgan Kaufmann}}
}

@inproceedings{grzes2017reward,
  title = {Reward Shaping in Episodic Reinforcement Learning},
  booktitle = {{{AAMAS}} 2017},
  author = {Grzes, Marek},
  year = {2017},
  pages = {565--573},
  publisher = {{ACM}}
}

@misc{bousmalis2017using,
      title={Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping}, 
      author={Konstantinos Bousmalis and Alex Irpan and Paul Wohlhart and Yunfei Bai and Matthew Kelcey and Mrinal Kalakrishnan and Laura Downs and Julian Ibarz and Peter Pastor and Kurt Konolige and Sergey Levine and Vincent Vanhoucke},
      year={2017},
      eprint={1709.07857},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{tobin2017domain,
  title={Domain randomization for transferring deep neural networks from simulation to the real world},
  author={Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={23--30},
  year={2017},
  organization={IEEE}
}


@article{silver2016mastering,
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  doi = {10.1038/nature16961},
  issue = {7587},
  journal = {Nature},
  language = {en},
  month = {1},
  pages = {484--489},
  publisher = {Springer Science and Business Media LLC},
  title = {Mastering the game of Go with deep neural networks and tree search},
  type = {article},
  url = {http://dx.doi.org/10.1038/nature16961},
  volume = {529},
  year = {2016},
}

@article{mnih2015human,
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  doi = {10.1038/nature14236},
  issue = {7540},
  journal = {Nature},
  language = {en},
  month = {2},
  pages = {529--533},
  publisher = {Springer Science and Business Media LLC},
  title = {Human-level control through deep reinforcement learning},
  type = {article},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = {518},
  year = {2015},
}


@article{vinyals2019grandmaster,
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha\"{e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R\'{e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W\"{u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  doi = {10.1038/s41586-019-1724-z},
  issue = {7782},
  journal = {Nature},
  language = {en},
  month = {11},
  pages = {350--354},
  publisher = {Springer Science and Business Media LLC},
  title = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  type = {article},
  url = {http://dx.doi.org/10.1038/s41586-019-1724-z},
  volume = {575},
  year = {2019},
}

@inproceedings{camacho2019ltl,
  author = {Camacho, Alberto and Toro Icarte, Rodrigo and Klassen, Toryn Q. and Valenzano, Richard and McIlraith, Sheila A.},
  booktitle = {Twenty-Eighth International Joint Conference on Artificial Intelligence IJCAI-19},
  journal = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence},
  title = {LTL and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning},
  year = {2019},
}

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}


@article{sadeghi2017cad2rl,
  author = {Sadeghi, Fereshteh and Levine, Sergey},
  eprint = {1611.04201v4},
  month = {Nov},
  title = {CAD2RL: Real Single-Image Flight without a Single Real Image},
  type = {article},
  url = {http://arxiv.org/abs/1611.04201v4},
  year = {2016},
}

@misc{zhu2020ingredients,
      title={The Ingredients of Real-World Robotic Reinforcement Learning}, 
      author={Henry Zhu and Justin Yu and Abhishek Gupta and Dhruv Shah and Kristian Hartikainen and Avi Singh and Vikash Kumar and Sergey Levine},
      year={2020},
      eprint={2004.12570},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@phdthesis{kakade2003sample,
  title={On the sample complexity of reinforcement learning},
  author={Kakade, Sham Machandranath and others},
  year={2003},
  school={University of London London, England}
}

@article{kearns2002near,
  author = {Kearns, Michael and Singh, Satinder},
  journal = {Machine learning},
  number = {2-3},
  pages = {209--232},
  publisher = {Springer},
  title = {Near-optimal reinforcement learning in polynomial time},
  type = {article},
  volume = {49},
  year = {2002},
}

@inproceedings{devlin2011theoretical,
  title={Theoretical considerations of potential-based reward shaping for multi-agent systems},
  author={Devlin, Sam and Kudenko, Daniel},
  booktitle={The 10th International Conference on Autonomous Agents and Multiagent Systems},
  pages={225--232},
  year={2011},
  organization={ACM}
}

@inproceedings{wiewiora2003principled,
  title = {Principled {{Methods}} for {{Advising Reinforcement Learning Agents}}},
  booktitle = {{{ICML}} 2003},
  author = {Wiewiora, Eric and Cottrell, Garrison W. and Elkan, Charles},
  year = {2003},
  pages = {792--799},
  publisher = {{AAAI Press}}
}

@inproceedings{marom2018belief,
  title={Belief reward shaping in reinforcement learning},
  author={Marom, Ofir and Rosman, Benjamin S},
  year={2018},
  organization={AAAI}
}

@inproceedings{koenig1993complexity,
  title={Complexity analysis of real-time reinforcement learning},
  author={Koenig, Sven and Simmons, Reid G},
  booktitle={AAAI},
  pages={99--107},
  year={1993}
}

@article{koenig1996effect,
  title={The effect of representation and knowledge on goal-directed exploration with reinforcement-learning algorithms},
  author={Koenig, Sven and Simmons, Reid G},
  journal={Machine Learning},
  volume={22},
  number={1},
  pages={227--250},
  year={1996},
  publisher={Springer}
}


@InProceedings{brunskill14,
  title = 	 {PAC-inspired Option Discovery in Lifelong Reinforcement Learning},
  author = 	 {Brunskill, Emma and Li, Lihong},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {316--324},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
}

@inproceedings{gao2015potential,
  title = {Potential Based Reward Shaping for Hierarchical Reinforcement Learning},
  booktitle = { {{IJCAI}} 2015},
  author = {Gao, Yang and Toni, Francesca},
  year = {2015},
  pages = {3504--3510},
  publisher = {{AAAI Press}}
}


@inproceedings{grzes2008multigrid,
  author    = {Marek Grzes and
               Daniel Kudenko},
  title     = {Multigrid Reinforcement Learning with Reward Shaping},
  booktitle = {{ICANN} 2008, Part {I}},
  volume    = {5163},
  pages     = {357--366},
  publisher = {Springer},
  year      = {2008},
}



@article{haj2019view,
  title={A view on deep reinforcement learning in system optimization},
  author={Haj-Ali, Ameer and Ahmed, Nesreen K and Willke, Ted and Gonzalez, Joseph and Asanovic, Krste and Stoica, Ion},
  journal={arXiv preprint arXiv:1908.01275},
  year={2019}
}

@article{polydoros2017survey,
  author = {Polydoros, Athanasios S and Nalpantidis, Lazaros},
  journal = {Journal of Intelligent \textbackslash{}\& Robotic Systems},
  number = {2},
  pages = {153--173},
  publisher = {Springer Netherlands},
  title = {Survey of Model-Based Reinforcement Learning: Applications on Robotics},
  type = {article},
  volume = {86},
  year = {2017},
}

@article{moerland2020model,
  title={Model-based reinforcement learning: A survey},
  author={Moerland, Thomas M and Broekens, Joost and Jonker, Catholijn M},
  journal={arXiv preprint arXiv:2006.16712},
  year={2020}
}

@article{grzes2010online,
  title={Online learning of shaping rewards in reinforcement learning},
  author={Grze{\'s}, Marek and Kudenko, Daniel},
  journal={Neural Networks},
  volume={23},
  number={4},
  pages={541--550},
  year={2010},
  publisher={Elsevier}
}

@incollection{sutton1990integrated,
  title={Integrated architectures for learning, planning, and reacting based on approximating dynamic programming},
  author={Sutton, Richard S},
  booktitle={Machine learning proceedings 1990},
  pages={216--224},
  year={1990},
  publisher={Elsevier}
}

@article{sutton1991dyna,
  title={Dyna, an integrated architecture for learning, planning, and reacting},
  author={Sutton, Richard S},
  journal={ACM Sigart Bulletin},
  volume={2},
  number={4},
  pages={160--163},
  year={1991},
  publisher={ACM New York, NY, USA}
}

@inproceedings{gu2016continuous,
  title={Continuous deep q-learning with model-based acceleration},
  author={Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={2829--2838},
  year={2016}
}

@inproceedings{nagabandi2018neural,
  title={Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning},
  author={Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={7559--7566},
  year={2018},
  organization={IEEE}
}

@article{shah2020deep,
  title={Deep R-Learning for Continual Area Sweeping},
  author={Shah, Rishi and Jiang, Yuqian and Hart, Justin and Stone, Peter},
  journal={arXiv preprint arXiv:2006.00589},
  year={2020}
}

@inproceedings{schwartz1993reinforcement,
  author = {Schwartz, Anton},
  booktitle = {Proceedings of the tenth international conference on machine learning},
  pages = {298--305},
  title = {A reinforcement learning method for maximizing undiscounted rewards},
  type = {inproceedings},
  volume = {298},
  year = {1993},
}


@article{mahadevan1996average,
  title={Average reward reinforcement learning: Foundations, algorithms, and empirical results},
  author={Mahadevan, Sridhar},
  journal={Machine learning},
  volume={22},
  number={1-3},
  pages={159--195},
  year={1996},
  publisher={Springer}
}

@article{naik2019discounted,
  title={Discounted reinforcement learning is not an optimization problem},
  author={Naik, Abhishek and Shariff, Roshan and Yasui, Niko and Yao, Hengshuai and Sutton, Richard S},
  journal={arXiv preprint arXiv:1910.02140},
  year={2019}
}

@misc{wan2020learning,
      title={Learning and Planning in Average-Reward Markov Decision Processes}, 
      author={Yi Wan and Abhishek Naik and Richard S. Sutton},
      year={2020},
      eprint={2006.16318},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Hwangbo_2019,
   title={Learning agile and dynamic motor skills for legged robots},
   volume={4},
   ISSN={2470-9476},
   url={http://dx.doi.org/10.1126/scirobotics.aau5872},
   DOI={10.1126/scirobotics.aau5872},
   number={26},
   journal={Science Robotics},
   publisher={American Association for the Advancement of Science (AAAS)},
   author={Hwangbo, Jemin and Lee, Joonho and Dosovitskiy, Alexey and Bellicoso, Dario and Tsounis, Vassilios and Koltun, Vladlen and Hutter, Marco},
   year={2019},
   month={Jan},
   pages={eaau5872}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@INPROCEEDINGS{grzes2009,  
  author={M. {Grzes} and D. {Kudenko}},
  booktitle={ICMLA},
  title={Theoretical and Empirical Analysis of Reward Shaping in Reinforcement Learning},
  year={2009},
  volume={},
  number={},
  pages={337-344},
  doi={10.1109/ICMLA.2009.33}
}

@phdthesis{grzes2010improving,
  title={Improving exploration in reinforcement learning through domain knowledge and parameter analysis},
  author={Grzes, Marek},
  year={2010},
  school={University of York}
}

@book{bertsekas1995dynamic,
  title={Dynamic programming and optimal control},
  author={Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P},
  volume={1},
  year={1995},
  publisher={Athena scientific Belmont, MA}
}

@inproceedings{wang_dueling_2016,
  author    = {Ziyu Wang and
               Tom Schaul and
               Matteo Hessel and
               Hado van Hasselt and
               Marc Lanctot and
               Nando de Freitas},
  title     = {Dueling Network Architectures for Deep Reinforcement Learning},
  booktitle = {{ICML}},
  volume    = {48},
  pages     = {1995--2003},
  publisher = {JMLR.org},
  year      = {2016}
}



@phdthesis{ravindran2004algebraic,
  title={An algebraic approach to abstraction in reinforcement learning},
  author={Ravindran, Balaraman and Barto, Andrew G},
  year={2004},
  school={University of Massachusetts at Amherst}
}

@inproceedings{Iocchi_ICAPS16,
        Address = {London, UK},
        Author = {L. Iocchi and L. Jeanpierre and M. T. L\'azaro and A.-I. Mouaddib},
        Booktitle = {ICAPS},
        Month = {June 12-17},
        Pages = {486-494},
        Title = {A practical framework for robust decision-theoretic planning and execution for service robots},
        Year = {2016}
}

@inproceedings{Iocchi_ICAPS17a,
        Author = {V. Sanelli and M. Cashmore and D. Magazzeni and L. Iocchi},
        Booktitle = {Proc. of International Conference on Automated Planning and Scheduling (ICAPS)},
        Title = {Short-Term Human Robot Interaction through Conditional Planning and Execution},
         url = {https://www.diag.uniroma1.it/iocchi/publications/sanelli-icaps2017.pdf},
        Year = {2017}
}

@article{wiewiora2003potential,
  title = {Potential-{{Based Shaping}} and {{Q-Value Initialization}} Are {{Equivalent}}},
  author = {Wiewiora, E.},
  year = {2003},
  journal = {Journal of Artificial Intelligence Research},
  volume = {19},
  pages = {205--208},
  publisher = {{AI Access Foundation}},
  issn = {1076-9757},
  doi = {10.1613/jair.1190}
}

@inproceedings{whitehead1991complexity,
  title={A Complexity Analysis of Cooperative Mechanisms in Reinforcement Learning.},
  author={Whitehead, Steven D},
  booktitle={AAAI},
  pages={607--613},
  year={1991}
}

@techreport{koenig1992complexity,
  author = {Koenig, Sven and Simmons, Reid G},
  institution = {CARNEGIE-MELLON UNIV PITTSBURGH PA SCHOOL OF COMPUTER SCIENCE},
  title = {Complexity analysis of real-time reinforcement learning applied to finding shortest paths in deterministic domains},
  type = {techreport},
  year = {1992},
}


@article{geffner2013concise,
  title={A concise introduction to models and methods for automated planning},
  author={Geffner, Hector and Bonet, Blai},
  journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume={8},
  number={1},
  pages={1--141},
  year={2013},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{asmuth2008potential,
  title={Potential-based Shaping in Model-based Reinforcement Learning.},
  author={Asmuth, John and Littman, Michael L and Zinkov, Robert},
  booktitle={AAAI},
  pages={604--609},
  year={2008}
}

@article{strehl2009reinforcement,
  title={Reinforcement Learning in Finite MDPs: PAC Analysis.},
  author={Strehl, Alexander L and Li, Lihong and Littman, Michael L},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={11},
  year={2009}
}

@article{konidaris2019necessity,
  title={On the necessity of abstraction},
  author={Konidaris, George},
  journal={Current opinion in behavioral sciences},
  volume={29},
  pages={1--7},
  year={2019},
  publisher={Elsevier}
}

@article{brafman2002r,
  title={R-max-a general polynomial time algorithm for near-optimal reinforcement learning},
  author={Brafman, Ronen I and Tennenholtz, Moshe},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Oct},
  pages={213--231},
  year={2002}
}

@inproceedings{strehl2006pac,
  title={PAC model-free reinforcement learning},
  author={Strehl, Alexander L and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L},
  booktitle={ICML 2006},
  pages={881--888},
  year={2006}
}

@article{sutton1999between,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artif. Intell.},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@article{dietterich2000hierarchical,
  title={Hierarchical reinforcement learning with the MAXQ value function decomposition},
  author={Dietterich, Thomas G},
  journal={JAIR},
  volume={13},
  pages={227--303},
  year={2000}
}

@article{parr1998reinforcement,
  title={Reinforcement learning with hierarchies of machines},
  author={Parr, Ronald and Russell, Stuart},
  journal={Advances in neural information processing systems},
  pages={1043--1049},
  year={1998},
  publisher={Morgan Kaufmann}
}

@inproceedings{icarte2018using,
  title={Using reward machines for high-level task specification and decomposition in reinforcement learning},
  author={Icarte, Rodrigo Toro and Klassen, Toryn and Valenzano, Richard and McIlraith, Sheila},
  booktitle={ICML},
  volume = {80},
  pages={2107--2116},
  year={2018},
  publisher = {{PMLR}},
}

@inproceedings{illanes_2020_SymbolicPlansa,
  title = {Symbolic Plans as High-Level Instructions for Reinforcement Learning},
  booktitle = {ICAPS},
  author = {Illanes, Le\'on and Yan, Xi and Icarte, Rodrigo Toro and McIlraith, Sheila A.},
  year = {2020},
  pages = {540--550},
  publisher = {{AAAI Press}}
}


@article{efthymiadis2014comparison,
  title={A comparison of plan-based and abstract MDP reward shaping},
  author={Efthymiadis, Kyriakos and Kudenko, Daniel},
  journal={Connection Science},
  volume={26},
  number={1},
  pages={85--99},
  year={2014},
  publisher={Taylor \& Francis}
}

@article{leonetti2016synthesis,
  title={A synthesis of automated planning and reinforcement learning for efficient, robust decision-making},
  author={Leonetti, Matteo and Iocchi, Luca and Stone, Peter},
  journal={Artif. Intell.},
  volume={241},
  pages={103--130},
  year={2016},
  publisher={Elsevier}
}



@article{GundanaK21,
  author    = {David Gundana and
               Hadas Kress{-}Gazit},
  title     = {Event-Based Signal Temporal Logic Synthesis for Single and Multi-Robot
               Tasks},
  journal   = {{IEEE} Robotics Autom. Lett.},
  volume    = {6},
  number    = {2},
  pages     = {3687--3694},
  year      = {2021}
}

@inproceedings{he2019efficient,
  title={Efficient symbolic reactive synthesis for finite-horizon tasks},
  author={He, Keliang and Wells, Andrew M and Kavraki, Lydia E and Vardi, Moshe Y},
  booktitle={ICRA},
  pages={8993--8999},
  year={2019},
}


@inproceedings{ravindran_model_2002,
	title = {Model Minimization in Hierarchical Reinforcement Learning},
	volume = {2371},
	doi = {10.1007/3-540-45622-8_15},
	pages = {196--211},
	booktitle = {{SARA} 2002},
	publisher = {Springer},
	author = {Ravindran, Balaraman and Barto, Andrew G.},
	year = {2002},
}



@inproceedings{Jiang2012temporalshaping,
  author    = {Yuqian Jiang and
               Suda Bharadwaj and
               Bo Wu and
               Rishi Shah and
               Ufuk Topcu and
               Peter Stone},
  title     = {Temporal-Logic-Based Reward Shaping for Continuing Reinforcement Learning
               Tasks},
  booktitle = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2021, Thirty-Third Conference on Innovative Applications of Artificial
               Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances
               in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9,
               2021},
  pages     = {7995--8003},
  publisher = {{AAAI} Press},
  year      = {2021}
}

@inproceedings{Marthi07automatic-shaping,
  author    = {Bhaskara Marthi},
  title     = {Automatic shaping and decomposition of reward functions},
  booktitle = {ICML 2007},
  volume    = {227},
  pages     = {601--608},
  publisher = {{ACM}},
  year      = {2007},
  doi       = {10.1145/1273496.1273572},
}


@inproceedings{yu_2018_towards,
  author    = {Yang Yu},
  editor    = {J{\'{e}}r{\^{o}}me Lang},
  title     = {Towards Sample Efficient Reinforcement Learning},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on
               Artificial Intelligence, {IJCAI} 2018, July 13-19, 2018, Stockholm,
               Sweden},
  pages     = {5739--5743},
  publisher = {ijcai.org},
  year      = {2018},
  url       = {https://doi.org/10.24963/ijcai.2018/820},
  doi       = {10.24963/ijcai.2018/820},
}


@inproceedings{biza_2019_online,
  author    = {Ondrej Biza and
               Robert Platt Jr.},
  editor    = {Edith Elkind and
               Manuela Veloso and
               Noa Agmon and
               Matthew E. Taylor},
  title     = {Online Abstraction with {MDP} Homomorphisms for Deep Learning},
  booktitle = {Proceedings of the 18th International Conference on Autonomous Agents
               and MultiAgent Systems, {AAMAS} '19, Montreal, QC, Canada, May 13-17,
               2019},
  pages     = {1125--1133},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  year      = {2019},
}


@article{steccanella_hierarchical_2021,
  author    = {Lorenzo Steccanella and
               Simone Totaro and
               Anders Jonsson},
  title     = {Hierarchical Representation Learning for Markov Decision Processes},
  journal   = {CoRR},
  volume    = {abs/2106.01655},
  year      = {2021},
  eprinttype = {arXiv},
  eprint    = {2106.01655},
}

@inproceedings{schubert_2021_PlanbasedRelaxed,
  title = {Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks},
  booktitle = {{{ICLR}} 2021},
  author = {Schubert, Ingmar and Oguz, Ozgur S. and Toussaint, Marc},
  year = {2021},
  publisher = {{OpenReview.net}}
}


@inproceedings{abel_2016_OptimalBehavior,
	location = {New York, New York, {USA}},
	title = {Near Optimal Behavior via Approximate State Abstraction},
	volume = {48},
	pages = {2915--2923},
	booktitle = {ICML 2016},
	publisher = {{PMLR}},
	author = {Abel, David and Hershkowitz, David and Littman, Michael},
	year = {2016}
}

@inproceedings{bai_2016_MarkovianState,
	title = {Markovian State and Action Abstractions for {MDPs} via Hierarchical {MCTS}},
	pages = {3029--3039},
	booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, {IJCAI} 2016, New York, {NY}, {USA}, 9-15 July 2016},
	publisher = {{IJCAI}/{AAAI} Press},
	author = {Bai, Aijun and Srivastava, Siddharth and Russell, Stuart J.},
	year = {2016},
}

@inproceedings{abel_2020_ValuePreserving,
	title = {Value Preserving State-Action Abstractions},
	volume = {108},
	pages = {1639--1650},
	booktitle = {AISTATS},
	publisher = {{PMLR}},
	author = {Abel, David and Umbanhowar, Nate and Khetarpal, Khimya and Arumugam, Dilip and Precup, Doina and Littman, Michael},
	year = {2020},
}

@article{vanhasselt_2015_DeepReinforcement,
	title = {Deep Reinforcement Learning with Double Q-learning},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions afﬁrmatively. In particular, we ﬁrst show that the recent {DQN} algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a speciﬁc adaptation to the {DQN} algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	journal = {{arXiv}:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	date = {2015-12-08},
	year = {2015},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1509.06461},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{Hauskrecht_1998_HierarchicalSolution,
  author    = {Milos Hauskrecht and
               Nicolas Meuleau and
               Leslie Pack Kaelbling and
               Thomas L. Dean and
               Craig Boutilier},
  title     = {Hierarchical Solution of Markov Decision Processes using Macro-actions},
  booktitle = {{UAI} '98: Proceedings of the Fourteenth Conference on Uncertainty
               in Artificial Intelligence, University of Wisconsin Business School},
  pages     = {220--229},
  year      = {1998},
}

@inproceedings{Dayan_1992_Feudal,
  author    = {Peter Dayan and
               Geoffrey E. Hinton},
  editor    = {Stephen Jose Hanson and
               Jack D. Cowan and
               C. Lee Giles},
  title     = {Feudal Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems 5, NIPS Conference},
  pages     = {271--278},
  publisher = {Morgan Kaufmann},
  year      = {1992},
}

@inproceedings{Randlov_1998_Learning,
  author    = {Jette Randl{\o}v and
               Preben Alstr{\o}m},
  editor    = {Jude W. Shavlik},
  title     = {Learning to Drive a Bicycle Using Reinforcement Learning and Shaping},
  booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning
               {(ICML} 1998)},
  pages     = {463--471},
  publisher = {Morgan Kaufmann},
  year      = {1998},
}


@inproceedings{icarte_2018_advicebased,
  author    = {Rodrigo Toro Icarte and
               Toryn Q. Klassen and
               Richard Anthony Valenzano and
               Sheila A. McIlraith},
  editor    = {Ebrahim Bagheri and
               Jackie Chi Kit Cheung},
  title     = {Advice-Based Exploration in Model-Based Reinforcement Learning},
  booktitle = {Advances in Artificial Intelligence - 31st Canadian Conference on
               Artificial Intelligence, Canadian {AI} 2018, Toronto, ON, Canada,
               May 8-11, 2018, Proceedings},
  series    = {Lecture Notes in Computer Science},
  volume    = {10832},
  pages     = {72--83},
  publisher = {Springer},
  year      = {2018},
  url       = {https://doi.org/10.1007/978-3-319-89656-4\_6},
  doi       = {10.1007/978-3-319-89656-4\_6},
}

@article{lee_2021_AIPlanning,
  title = {{{AI Planning Annotation}} in {{Reinforcement Learning}}: {{Options}} and {{Beyond}}},
  author = {Lee, Junkyu and Katz, Michael and Agravante, Don Joven and Liu, Miao and Klinger, Tim and Campbell, Murray and Sohrabi, Shirin and Tesauro, Gerald},
  year = {2021},
  journal = {Planning and Reinforcement Learning PRL Workshop at ICAPS},
  langid = {english}
}

@inproceedings{gehring_2022_ReinforcementLearning,
  title = {Reinforcement Learning for Classical Planning: {{Viewing}} Heuristics as Dense Reward Generators},
  booktitle = {Proceedings of the Thirty-Second International Conference on Automated Planning and Scheduling, {{ICAPS}} 2022, Singapore (Virtual), June 13-24, 2022},
  author = {Gehring, Clement and Asai, Masataro and Chitnis, Rohan and Silver, Tom and Kaelbling, Leslie Pack and Sohrabi, Shirin and Katz, Michael},
  editor = {Kumar, Akshat and Thi\'ebaux, Sylvie and Varakantham, Pradeep and Yeoh, William},
  date = {2022},
  pages = {588--596},
  publisher = {{AAAI Press}}
}


@article{hutsebaut-buysse_2022_HierarchicalReinforcementb,
  title = {Hierarchical Reinforcement Learning: {{A}} Survey and Open Research Challenges},
  author = {Hutsebaut-Buysse, Matthias and Mets, Kevin and Latr\'e, Steven},
  year = {2022},
  journal = {Machine Learning and Knowledge Extraction},
  volume = {4},
  number = {1},
  pages = {172--221},
  issn = {2504-4990},
  doi = {10.3390/make4010009}
}

@inproceedings{jothimurugan_2021_AbstractValue,
  title = {Abstract Value Iteration for Hierarchical Reinforcement Learning},
  booktitle = {AISTATS},
  author = {Jothimurugan, Kishor and Bastani, Osbert and Alur, Rajeev},
  year = {2021},
  volume = {130},
  pages = {1162--1170},
  publisher = {{PMLR}}
}

@article{liaw2018tune,
  title = {Tune: {{A}} Research Platform for Distributed Model Selection and Training},
  author = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2018},
  journal = {CoRR},
  volume = {abs/1807.05118},
  eprint = {1807.05118},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}
