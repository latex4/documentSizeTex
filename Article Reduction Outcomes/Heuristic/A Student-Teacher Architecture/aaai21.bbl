\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi:\discretionary{}{}{}#1}\else
  \providecommand{\doi}{doi:\discretionary{}{}{}\begingroup
  \urlstyle{rm}\Url}\fi

\bibitem[{Bahdanau, Cho, and Bengio(2014)}]{bahdanau2014neural}
Bahdanau, D.; Cho, K.; and Bengio, Y. 2014.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473} .

\bibitem[{Bengio(2012)}]{bengio2012deep}
Bengio, Y. 2012.
\newblock Deep learning of representations for unsupervised and transfer
  learning.
\newblock In \emph{Proceedings of ICML Workshop on Unsupervised and Transfer
  Learning}, 17--36.

\bibitem[{Budzianowski and Vuli{\'c}(2019)}]{budzianowski2019hello}
Budzianowski, P.; and Vuli{\'c}, I. 2019.
\newblock Hello, It's GPT-2--How Can I Help You? Towards the Use of Pretrained
  Language Models for Task-Oriented Dialogue Systems.
\newblock \emph{arXiv preprint arXiv:1907.05774} .

\bibitem[{Budzianowski et~al.(2018)Budzianowski, Wen, Tseng, Casanueva, Ultes,
  Ramadan, and Ga{\v{s}}i{\'c}}]{budzianowski2018multiwoz}
Budzianowski, P.; Wen, T.-H.; Tseng, B.-H.; Casanueva, I.; Ultes, S.; Ramadan,
  O.; and Ga{\v{s}}i{\'c}, M. 2018.
\newblock Multiwoz-a large-scale multi-domain wizard-of-oz dataset for
  task-oriented dialogue modelling.
\newblock \emph{arXiv preprint arXiv:1810.00278} .

\bibitem[{Caruana(1997)}]{caruana1997multitask}
Caruana, R. 1997.
\newblock Multitask learning.
\newblock \emph{Machine learning} 28(1): 41--75.

\bibitem[{Cho et~al.(2014)Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau,
  Bougares, Schwenk, and Bengio}]{cho2014learning}
Cho, K.; Van~Merri{\"e}nboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.;
  Schwenk, H.; and Bengio, Y. 2014.
\newblock Learning phrase representations using RNN encoder-decoder for
  statistical machine translation.
\newblock \emph{arXiv preprint arXiv:1406.1078} .

\bibitem[{Dauphin et~al.(2014)Dauphin, T{\"u}r, Hakkani-T{\"u}r, and
  Heck}]{Dauphin2014ZeroShotLA}
Dauphin, Y.; T{\"u}r, G.; Hakkani-T{\"u}r, D.~Z.; and Heck, L.~P. 2014.
\newblock Zero-Shot Learning and Clustering for Semantic Utterance
  Classification.
\newblock \emph{CoRR} abs/1401.0509.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{Devlin2019BERTPO}
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
\newblock BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding.
\newblock \emph{ArXiv} abs/1810.04805.

\bibitem[{Fan et~al.(2018)Fan, Tian, Qin, Li, and Liu}]{Fan2018LearningTT}
Fan, Y.; Tian, F.; Qin, T.; Li, X.; and Liu, T.-Y. 2018.
\newblock Learning to Teach.
\newblock \emph{ArXiv} abs/1805.03643.

\bibitem[{Finn, Abbeel, and Levine(2017)}]{finn2017model}
Finn, C.; Abbeel, P.; and Levine, S. 2017.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, 1126--1135. JMLR. org.

\bibitem[{Genevay and Laroche(2016)}]{Genevay2016TransferLF}
Genevay, A.; and Laroche, R. 2016.
\newblock Transfer Learning for User Adaptation in Spoken Dialogue Systems.
\newblock In \emph{AAMAS}.

\bibitem[{Gu et~al.(2016)Gu, Lu, Li, and Li}]{Gu2016IncorporatingCM}
Gu, J.; Lu, Z.; Li, H.; and Li, V. O.~K. 2016.
\newblock Incorporating Copying Mechanism in Sequence-to-Sequence Learning.
\newblock \emph{CoRR} abs/1603.06393.

\bibitem[{Gu et~al.(2018)Gu, Wang, Chen, Cho, and Li}]{gu2018meta}
Gu, J.; Wang, Y.; Chen, Y.; Cho, K.; and Li, V.~O. 2018.
\newblock Meta-learning for low-resource neural machine translation.
\newblock \emph{arXiv preprint arXiv:1808.08437} .

\bibitem[{Henderson, Thomson, and Williams(2014)}]{Henderson2014TheTD}
Henderson, M.; Thomson, B.; and Williams, J.~D. 2014.
\newblock The third Dialog State Tracking Challenge.
\newblock \emph{2014 IEEE Spoken Language Technology Workshop (SLT)} 324--329.

\bibitem[{Hinton, Vinyals, and Dean(2015)}]{Hinton2015DistillingTK}
Hinton, G.~E.; Vinyals, O.; and Dean, J. 2015.
\newblock Distilling the Knowledge in a Neural Network.
\newblock \emph{ArXiv} abs/1503.02531.

\bibitem[{Ioffe and Szegedy(2015)}]{ioffe2015batch}
Ioffe, S.; and Szegedy, C. 2015.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167} .

\bibitem[{Kelley(1984)}]{Kelley1984AnID}
Kelley, J.~F. 1984.
\newblock An iterative design methodology for user-friendly natural language
  office information applications.
\newblock \emph{ACM Trans. Inf. Syst.} 2: 26--41.

\bibitem[{Kim and Rush(2016)}]{Kim2016SequenceLevelKD}
Kim, Y.; and Rush, A.~M. 2016.
\newblock Sequence-Level Knowledge Distillation.
\newblock \emph{ArXiv} abs/1606.07947.

\bibitem[{Kingma and Ba(2014)}]{kingma2014adam}
Kingma, D.~P.; and Ba, J. 2014.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980} .

\bibitem[{Lei et~al.(2018)Lei, Jin, Kan, Ren, He, and Yin}]{lei2018sequicity}
Lei, W.; Jin, X.; Kan, M.-Y.; Ren, Z.; He, X.; and Yin, D. 2018.
\newblock Sequicity: Simplifying task-oriented dialogue systems with single
  sequence-to-sequence architectures.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 1437--1447.

\bibitem[{Li et~al.(2019)Li, Qian, Shi, and Yu}]{Li2019EndtoEndTN}
Li, Y.; Qian, K.; Shi, W.; and Yu, Z. 2019.
\newblock End-to-End Trainable Non-Collaborative Dialog System.
\newblock \emph{ArXiv} abs/1911.10742.

\bibitem[{Liu et~al.(2017)Liu, Dai, Humayun, Tay, Yu, Smith, Rehg, and
  Song}]{Liu2017IterativeMT}
Liu, W.; Dai, B.; Humayun, A.; Tay, C.; Yu, C.; Smith, L.~B.; Rehg, J.~M.; and
  Song, L. 2017.
\newblock Iterative Machine Teaching.
\newblock \emph{ArXiv} abs/1705.10470.

\bibitem[{Mo et~al.(2018)Mo, Zhang, Li, Li, and Yang}]{Mo2018PersonalizingAD}
Mo, K.; Zhang, Y.; Li, S.; Li, J.; and Yang, Q. 2018.
\newblock Personalizing a Dialogue System With Transfer Reinforcement Learning.
\newblock In \emph{AAAI}.

\bibitem[{Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu}]{Papineni2002BleuAM}
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
\newblock Bleu: a Method for Automatic Evaluation of Machine Translation.

\bibitem[{Peng et~al.(2020)Peng, Li, chao Li, Shayandeh, Liden, and
  Gao}]{Peng2020SOLOISTFT}
Peng, B.; Li, C.; chao Li, J.; Shayandeh, S.; Liden, L.; and Gao, J. 2020.
\newblock SOLOIST: Few-shot Task-Oriented Dialog with A Single Pre-trained
  Auto-regressive Model.
\newblock \emph{ArXiv} abs/2005.05298.

\bibitem[{Peng et~al.(2019)Peng, Huang, Lin, Ji, Chen, and
  Zhang}]{Peng2019TeacherStudentFE}
Peng, S.; Huang, X.; Lin, Z.; Ji, F.; Chen, H.; and Zhang, Y. 2019.
\newblock Teacher-Student Framework Enhanced Multi-domain Dialogue Generation.
\newblock \emph{ArXiv} abs/1908.07137.

\bibitem[{Pennington, Socher, and Manning(2014)}]{pennington2014glove}
Pennington, J.; Socher, R.; and Manning, C.~D. 2014.
\newblock GloVe: Global Vectors for Word Representation.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  1532--1543.
\newblock \urlprefix\url{http://www.aclweb.org/anthology/D14-1162}.

\bibitem[{Qian and Yu(2019)}]{qian2019domain}
Qian, K.; and Yu, Z. 2019.
\newblock Domain Adaptive Dialog Generation via Meta Learning.
\newblock \emph{arXiv preprint arXiv:1906.03520} .

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever}]{radford2019language}
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog} 1(8): 9.

\bibitem[{Rastogi et~al.(2019)Rastogi, Zang, Sunkara, Gupta, and
  Khaitan}]{rastogi2019towards}
Rastogi, A.; Zang, X.; Sunkara, S.; Gupta, R.; and Khaitan, P. 2019.
\newblock Towards scalable multi-domain conversational agents: The
  schema-guided dialogue dataset.
\newblock \emph{arXiv preprint arXiv:1909.05855} .

\bibitem[{Rusu et~al.(2019)Rusu, Rao, Sygnowski, Vinyals, Pascanu, Osindero,
  and Hadsell}]{Rusu2019MetaLearningWL}
Rusu, A.~A.; Rao, D.; Sygnowski, J.; Vinyals, O.; Pascanu, R.; Osindero, S.;
  and Hadsell, R. 2019.
\newblock Meta-Learning with Latent Embedding Optimization.
\newblock \emph{ArXiv} abs/1807.05960.

\bibitem[{Shalyminov et~al.(2019)Shalyminov, Lee, Eshghi, and
  Lemon}]{shalyminov2019few}
Shalyminov, I.; Lee, S.; Eshghi, A.; and Lemon, O. 2019.
\newblock Few-Shot Dialogue Generation Without Annotated Data: A Transfer
  Learning Approach.
\newblock \emph{arXiv preprint arXiv:1908.05854} .

\bibitem[{Shalyminov et~al.(2020)Shalyminov, Sordoni, Atkinson, and
  Schulz}]{Shalyminov2020HybridGT}
Shalyminov, I.; Sordoni, A.; Atkinson, A.; and Schulz, H. 2020.
\newblock Hybrid Generative-Retrieval Transformers for Dialogue Domain
  Adaptation.
\newblock \emph{ArXiv} abs/2003.01680.

\bibitem[{Shi and Yu(2018)}]{Shi2018SentimentAE}
Shi, W.; and Yu, Z. 2018.
\newblock Sentiment Adaptive End-to-End Dialog Systems.
\newblock In \emph{ACL}.

\bibitem[{Song et~al.(2019)Song, Liu, Bi, Yan, and Zhang}]{Song2019LearningTC}
Song, Y.; Liu, Z.; Bi, W.; Yan, R.; and Zhang, M. 2019.
\newblock Learning to Customize Model Structures for Few-shot Dialogue
  Generation Tasks.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.~N.;
  Kaiser, {\L}.; and Polosukhin, I. 2017.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems},
  5998--6008.

\bibitem[{Wen et~al.(2015)Wen, Gasic, Kim, Mrksic, hao Su, Vandyke, and
  Young}]{Wen2015StochasticLG}
Wen, T.-H.; Gasic, M.; Kim, D.; Mrksic, N.; hao Su, P.; Vandyke, D.; and Young,
  S.~J. 2015.
\newblock Stochastic Language Generation in Dialogue using Recurrent Neural
  Networks with Convolutional Sentence Reranking.
\newblock In \emph{SIGDIAL Conference}.

\bibitem[{Wen et~al.(2016)Wen, Vandyke, Mrksic, Gasic, Rojas-Barahona, Su,
  Ultes, and Young}]{wen2016network}
Wen, T.-H.; Vandyke, D.; Mrksic, N.; Gasic, M.; Rojas-Barahona, L.~M.; Su,
  P.-H.; Ultes, S.; and Young, S. 2016.
\newblock A network-based end-to-end trainable task-oriented dialogue system.
\newblock \emph{arXiv preprint arXiv:1604.04562} .

\bibitem[{Williams, Asadi, and Zweig(2017)}]{Williams2017HybridCN}
Williams, J.~D.; Asadi, K.; and Zweig, G. 2017.
\newblock Hybrid Code Networks: practical and efficient end-to-end dialog
  control with supervised and reinforcement learning.
\newblock \emph{ArXiv} abs/1702.03274.

\bibitem[{Wu et~al.(2018)Wu, Tian, Xia, Fan, Qin, Lai, and
  Liu}]{Wu2018LearningTT}
Wu, L.; Tian, F.; Xia, Y.; Fan, Y.; Qin, T.; Lai, J.-H.; and Liu, T.-Y. 2018.
\newblock Learning to Teach with Dynamic Loss Functions.
\newblock In \emph{NeurIPS}.

\bibitem[{Young et~al.(2010)Young, Gasic, Keizer, Mairesse, Schatzmann,
  Thomson, and Yu}]{Young2010TheHI}
Young, S.~J.; Gasic, M.; Keizer, S.; Mairesse, F.; Schatzmann, J.; Thomson, B.;
  and Yu, K. 2010.
\newblock The Hidden Information State model: A practical framework for
  POMDP-based spoken dialogue management.
\newblock \emph{Computer Speech $\&$ Language} 24: 150--174.

\bibitem[{Zhang, Ou, and Yu(2019)}]{zhang2019task}
Zhang, Y.; Ou, Z.; and Yu, Z. 2019.
\newblock Task-Oriented Dialog Systems that Consider Multiple Appropriate
  Responses under the Same Context.
\newblock \emph{arXiv preprint arXiv:1911.10484} .

\bibitem[{Zhao and Eskenazi(2018)}]{zhao2018zero}
Zhao, T.; and Eskenazi, M. 2018.
\newblock Zero-shot dialog generation with cross-domain latent actions.
\newblock \emph{arXiv preprint arXiv:1805.04803} .

\bibitem[{Zhu(2015)}]{Zhu2015MachineTA}
Zhu, X. 2015.
\newblock Machine Teaching: An Inverse Problem to Machine Learning and an
  Approach Toward Optimal Education.
\newblock In \emph{AAAI}.

\end{thebibliography}
