% Encoding: UTF-8

@TechReport{Wang2019,
  author      = {Wang, Ge},
  institution = {Stanford Institute for Human-Centered AI},
  title       = {Humans in the Loop: The Design of Interactive AI Systems},
  year        = {2019},
  address     = {Stanford, CA 94305},
  month       = oct,
  url         = {https://hai.stanford.edu/news/humans-loop-design-interactive-ai-systems},
}

@Article{Offert2019,
  author  = {Offert, Fabian},
  journal = {The Gradient},
  title   = {The Past, Present, and Future of AI Art},
  year    = {2019},
  url     = {https://thegradient.pub/the-past-present-and-future-of-ai-art/},
}

@Article{Mazzone2019,
  author    = {Marian Mazzone and Ahmed Elgammal},
  journal   = {Arts},
  title     = {Art, Creativity, and the Potential of Artificial Intelligence},
  year      = {2019},
  month     = {feb},
  number    = {1},
  pages     = {26},
  volume    = {8},
  doi       = {10.3390/arts8010026},
  publisher = {{MDPI} {AG}},
}

@InCollection{Manovich2019,
  author    = {Lev Manovich},
  booktitle = {Artificial Intelligence and Dialog of Culture},
  publisher = {Hermitage Museum},
  title     = {Defining AI arts: Three proposals},
  year      = {2019},
  address   = {Saint Petersburg, Russia},
}

@InProceedings{dInverno2015,
  author    = {Mark d'Inverno and Jon McCormack},
  booktitle = {24th International Joint Conference on Artificial Intelligence (IJCAI)},
  title     = {Heroic versus Collaborative AI for the Arts},
  year      = {2015},
  address   = {Buenos Aires, Argentina},
  month     = jul,
}

@Article{Wilson1983,
  author  = {Stephen Wilson},
  journal = {Leonardo},
  title   = {Computer Art: Artificial Intelligence and the Arts},
  year    = {1983},
  issn    = {1530-9282},
  month   = jan,
  number  = {1},
  pages   = {15--20},
  volume  = {16},
}

@Article{Jennings2010,
  author    = {Kyle E. Jennings},
  journal   = {Minds and Machines},
  title     = {Developing Creativity: Artificial Barriers in Artificial Intelligence},
  year      = {2010},
  month     = {oct},
  number    = {4},
  pages     = {489--501},
  volume    = {20},
  doi       = {10.1007/s11023-010-9206-y},
  publisher = {Springer Science and Business Media {LLC}},
}

@Book{AIandCreativity,
  author    = {Dartnall, Terry},
  publisher = {Springer Netherlands},
  title     = {Artificial Intelligence and Creativity : an Interdisciplinary Approach},
  year      = {1994},
  address   = {Dordrecht},
  isbn      = {9401707936},
}

@InCollection{Boden1996,
  author    = {Margaret A. Boden},
  booktitle = {Artificial Intelligence: Handbook of Perception and Cognition},
  publisher = {Elsevier},
  title     = {Creativity},
  year      = {1996},
  chapter   = {9},
  pages     = {267--291},
  doi       = {10.1016/b978-012161964-0/50011-x},
}

@Book{FromFingersToDigits,
  author    = {Margaret A. Boden and Ernest A. Edmonds},
  publisher = {MIT Press Ltd},
  title     = {From Fingers to Digits: An Artificial Aesthetic (Leonardo)},
  year      = {2019},
  isbn      = {0262039621},
  month     = jul,
  ean       = {9780262039628},
  pagetotal = {392},
  url       = {https://www.ebook.de/de/product/34447074/margaret_a_research_professor_of_cognitive_science_university_of_sussex_boden_ernest_a_professor_of_computational_art_de_montfort_university_edmonds_from_fingers_to_digits.html},
}

@Article{Heerden2021,
  author    = {Imke Van Heerden and Anil Bas},
  journal   = {Journal of Artificial Intelligence Research},
  title     = {Viewpoint: {AI} as Author {\textendash} Bridging the Gap Between Machine Learning and Literary Theory},
  year      = {2021},
  month     = {jun},
  pages     = {175--189},
  volume    = {71},
  doi       = {10.1613/jair.1.12593},
  publisher = {{AI} Access Foundation},
}

@Misc{GANEdmondBelamy,
  author       = {Hugo Caselles-Dupré and Pierre Fautrel and Gauthier Vernier},
  howpublished = {Auctioned at Christie’s (NY, NY)},
  month        = aug,
  title        = {Portrait of Edmond Belamy},
  year         = {2018},
  groups       = {Obvious Art},
}

@Misc{AICAN2019,
  author       = {AICAN and Ahmed Elgammal},
  howpublished = {Exhibition},
  month        = feb,
  note         = {http://www.hgcontemporary.com/exhibitions/faceless-portraits-transcending-time},
  title        = {Faceless Portraits Transcending Time},
  year         = {2019},
  journal      = {HG Contemporary (NY, NY)},
}

@InProceedings{Goodfellow2014,
  author    = {Ian J. Goodfellow and Jean Pouget-Abadie and et al.},
  booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems},
  title     = {Generative Adversarial Nets},
  year      = {2014},
  month     = dec,
  pages     = {2672–2680},
  volume    = {2},
}

@Book{Goodwin2018,
  author    = {Goodwin, Ross},
  publisher = {Jean Boite Editions},
  title     = {1 the road},
  year      = {2018},
  address   = {Paris},
  isbn      = {2365680275},
  month     = aug,
}

@Book{BetaWriter2019,
  author    = {{Beta Writer}},
  publisher = {Springer International Publishing},
  title     = {Lithium-Ion Batteries},
  year      = {2019},
  month     = apr,
  doi       = {10.1007/978-3-030-16800-1},
}

@InProceedings{Kingma2014,
  author    = {D. P. Kingma and M. Welling},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  title     = {Auto-encoding variational bayes},
  year      = {2014},
}

@InProceedings{Fiebrink2009,
  author    = {Fiebrink, R. and Trueman, D. and Cook, P. R.},
  booktitle = {International Conference on New Interfaces for Musical Expression (NIME)},
  title     = {A meta-instrument for interactive, on-the-fly machine learning},
  year      = {2009},
}

@Misc{Sunspring,
  author       = {Oscar Sharp},
  howpublished = {http://youtu.be/LY7x2Ihqjmc},
  month        = jun,
  note         = {Premiered in Ars Technica},
  title        = {Sunspring},
  year         = {2016},
}

@Misc{AISong,
  author       = {{VPRO}, {NPO 3FM}, {NPO Innovation}},
  howpublished = {International Competition},
  month        = may,
  note         = {https://www.aisongcontest.com/},
  title        = {{AI Song Contest}},
  year         = {2020},
}

@TechReport{Mirza2014,
  author        = {Mehdi Mirza and Simon Osindero},
  institution   = {ArXiv},
  title         = {Conditional Generative Adversarial Nets},
  year          = {2014},
  month         = nov,
  note          = {1411.1784},
  abstract      = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  archiveprefix = {arXiv},
  eprint        = {1411.1784},
  file          = {:http\://arxiv.org/pdf/1411.1784v1:PDF},
  howpublished  = {ArXiv},
  keywords      = {cs.LG, cs.AI, cs.CV, stat.ML},
  primaryclass  = {cs.LG},
}

@Misc{SunspringScript,
  author       = {Benjamin},
  howpublished = {Film Script},
  month        = jun,
  note         = {https://www.docdroid.net/lCZ2fPA/sunspring-final-pdf},
  title        = {Sunspring},
  year         = {2016},
  url          = {https://www.docdroid.net/lCZ2fPA/sunspring-final-pdf},
}

@InProceedings{Calixto2019,
  author        = {Iacer Calixto and Miguel Rios and Wilker Aziz},
  booktitle     = {Proceedings of the 57th Annual Meeting of the ACL},
  title         = {Latent Variable Model for Multi-modal Translation},
  year          = {2019},
  month         = nov,
  abstract      = {In this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation (MMT) through a latent variable model. This latent variable can be seen as a multi-modal stochastic embedding of an image and its description in a foreign language. It is used in a target-language decoder and also to predict image features. Importantly, our model formulation utilises visual and textual inputs during training but does not require that images be available at test time. We show that our latent variable MMT formulation improves considerably over strong baselines, including a multi-task learning approach (Elliott and K\'ad\'ar, 2017) and a conditional variational auto-encoder approach (Toyama et al., 2016). Finally, we show improvements due to (i) predicting image features in addition to only conditioning on them, (ii) imposing a constraint on the minimum amount of information encoded in the latent variable, and (iii) by training on additional target-language image descriptions (i.e. synthetic data).},
  archiveprefix = {arXiv},
  eprint        = {1811.00357},
  file          = {:http\://arxiv.org/pdf/1811.00357v2:PDF},
  keywords      = {cs.CL, I.2.7},
  primaryclass  = {cs.CL},
}

@InProceedings{Specia2016,
  author    = {Lucia Specia and Stella Frank and Khalil Sima'an and Desmond Elliott},
  booktitle = {Proceedings of the First Conference on Machine Translation},
  title     = {A Shared Task on Multimodal Machine Translation and Crosslingual Image Description},
  year      = {2016},
  month     = aug,
  pages     = {543--553},
  doi       = {10.18653/v1/w16-2346},
}

@TechReport{Ramesh2021,
  author        = {Aditya Ramesh and Mikhail Pavlov and et al.},
  institution   = {ArXiv},
  title         = {Zero-Shot Text-to-Image Generation},
  year          = {2021},
  month         = feb,
  note          = {2102.12092},
  abstract      = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  archiveprefix = {arXiv},
  eprint        = {2102.12092},
  file          = {:http\://arxiv.org/pdf/2102.12092v2:PDF},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@TechReport{Radford2021,
  author        = {Alec Radford and Jong Wook Kim and et al.},
  institution   = {ArXiv},
  title         = {Learning Transferable Visual Models From Natural Language Supervision},
  year          = {2021},
  month         = feb,
  note          = {2103.00020},
  abstract      = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  eprint        = {2103.00020},
  file          = {:http\://arxiv.org/pdf/2103.00020v1:PDF},
  howpublished  = {ArXiv},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@Book{HITLML,
  author    = {Munro, Robert},
  publisher = {Manning Publications},
  title     = {Human-in-the-Loop Machine Learning},
  year      = {2021},
  isbn      = {1617296740},
  month     = aug,
  ean       = {9781617296741},
  pagetotal = {325},
}

@Article{LeWitt1967,
  author  = {LeWitt, S},
  journal = {Art Forum},
  title   = {Paragraphs on conceptual art},
  year    = {1967},
  pages   = {79--84},
  volume  = {5},
}

@InProceedings{Elgammal2017,
  author    = {Elgammal, A. and Liu, B. and Elhoseiny, M. and Mazzone, M.},
  booktitle = {8th International Conference on Computational Creativity},
  title     = {CAN: Creative adversarial networks generating “Art” by learning about styles and deviating from style norms.},
  year      = {2017},
}

@Article{Nake1998,
  author  = {Frieder Nake},
  journal = {Leonardo},
  title   = {Art in the Time of the Artificial},
  year    = {1998},
  month   = jun,
  note    = {The MIT Press},
  number  = {31},
  pages   = {163--164},
  volume  = {3},
}

@TechReport{Ackerman2016,
  author        = {Margareta Ackerman and David Loker},
  institution   = {arXiv},
  title         = {Algorithmic Songwriting with ALYSIA},
  year          = {2016},
  month         = dec,
  note          = {1612.01058},
  abstract      = {This paper introduces ALYSIA: Automated LYrical SongwrIting Application. ALYSIA is based on a machine learning model using Random Forests, and we discuss its success at pitch and rhythm prediction. Next, we show how ALYSIA was used to create original pop songs that were subsequently recorded and produced. Finally, we discuss our vision for the future of Automated Songwriting for both co-creative and autonomous systems.},
  archiveprefix = {arXiv},
  eprint        = {1612.01058},
  file          = {:http\://arxiv.org/pdf/1612.01058v1:PDF},
  keywords      = {cs.AI, cs.LG, cs.MM, cs.SD},
  primaryclass  = {cs.AI},
}

@InProceedings{Cohen1979,
  author    = {Harold Cohen},
  booktitle = {Proceedings of the 6th International Joint Conference on Artificial Intelligence},
  title     = {What is an Image?},
  year      = {1979},
}

@Book{Manning1999,
  author    = {Manning, Christopher},
  publisher = {MIT Press},
  title     = {Foundations of statistical natural language processing},
  year      = {1999},
  address   = {Cambridge, Mass},
  isbn      = {0262133601},
}

@Book{Romero2007,
  editor    = {Juan Romero and Penousal Machado},
  publisher = {Springer-Verlag GmbH},
  title     = {The Art of Artificial Evolution},
  year      = {2007},
  isbn      = {3540728767},
  month     = nov,
  ean       = {9783540728764},
  url       = {https://www.ebook.de/de/product/5750601/the_art_of_artificial_evolution.html},
}

@Article{Gatys2016,
  author    = {Leon Gatys and Alexander Ecker and Matthias Bethge},
  journal   = {Journal of Vision},
  title     = {A Neural Algorithm of Artistic Style},
  year      = {2016},
  month     = {sep},
  number    = {12},
  pages     = {326},
  volume    = {16},
  doi       = {10.1167/16.12.326},
  publisher = {Association for Research in Vision and Ophthalmology ({ARVO})},
}

@InProceedings{Brown2020,
  author    = {Brown, Tom and Mann, Benjamin and et al.},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Language Models are Few-Shot Learners},
  year      = {2020},
  pages     = {1877--1901},
}

@InProceedings{Pearlman2021,
  author    = {Ellen Pearlman},
  booktitle = {{ACM} International Conference on Interactive Media Experiences},
  title     = {Building a `Sicko' {AI}},
  year      = {2021},
  month     = {jun},
  publisher = {{ACM}},
  doi       = {10.1145/3452918.3467814},
}

@Misc{DrawingOperationsDuet,
  author = {Chung, Sougwen},
  note   = {Premiered in Art Science Museum in Singapore},
  title  = {DrawingOperations Duet},
  year   = {2018},
}

@Misc{TimeWaves,
  author = {Chung, Neo Christopher and Dyer, George Carter},
  month  = jul,
  note   = {Premiered in Arthropocene Festival at Klub {\L}{\c a}cznik (Wroc{\l}aw, Poland)},
  title  = {Time Waves x Deep Waves},
  year   = {2021},
}

@Article{Jansen2021,
  author    = {Chipp Jansen and Elizabeth Sklar},
  journal   = {Frontiers in Robotics and {AI}},
  title     = {Exploring Co-creative Drawing Workflows},
  year      = {2021},
  month     = {may},
  volume    = {8},
  doi       = {10.3389/frobt.2021.577770},
  publisher = {Frontiers Media {SA}},
}

@Article{Hochreiter1997,
  author  = {Hochreiter, S. and Schmidhuber, J.},
  journal = {Neural Computation},
  title   = {Long Short-Term Memory},
  year    = {1997},
  number  = {8},
  pages   = {1735--1780},
  volume  = {9},
}

@Comment{jabref-meta: databaseType:bibtex;}
