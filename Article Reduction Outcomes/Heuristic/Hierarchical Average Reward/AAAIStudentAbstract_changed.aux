\relax 
\citation{oc}
\citation{oc}
\citation{weightsharingAAAI}
\citation{weightsharingAAAI}
\citation{weightsharingAAAI}
\citation{weightsharingAAAI}
\citation{hoc}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig1}{{1}{1}{ Mean and standard deviations of the learning curves for the average reward and discounted reward OCPG agents, in the grid-world delivery experiment.}{}{}}
\newlabel{HARPG}{{1}{1}{}{}{}}
\citation{bhatnagar2009natural}
\citation{bhatnagar2009natural}
\citation{weightsharingAAAI}
\citation{weightsharingAAAI}
\citation{weightsharingAAAI}
\citation{weightsharingAAAI}
\bibstyle{aaai}
\bibdata{bibliography}
\newlabel{fig2}{{2}{2}{An empirical demonstration illustrating the convergence of the parameters of $Q(s,o)$, $\pi (a|s,o)$, and $\pi (o|s)$. We have randomly selected one parameter from each function approximator and plotted its value against the number of steps.}{}{}}
\newlabel{conv}{{2}{2}{}{}{}}
\newlabel{fig3}{{3}{2}{ \textbf  {(a)} A \textit  {trap} that employs delayed rewards to fool DR-RL agents into learning incorrect credit assignments. \textbf  {(b)} A grid-world navigation experiment where the reward at the drop off point depends upon which pickup location was previously visited (50 for $P_1$ and 100 for $P_2$). The trap at the blue-green junction misguides agents towards the sub-optimal pickup location, $P_1$. }{}{}}
\gdef \@abspage@last{3}
