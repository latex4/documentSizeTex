
@inproceedings{NEURIPS2019_3e9f0fc9,
 author = {Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 title = {Defending Against Neural Fake News},
 url = {https://proceedings.neurips.cc/paper/2019/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{Loshchilov_Hutter_2019_Decoupled,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Decoupled Weight Decay Regularization},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019},
  year = 2019,
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LoshchilovH19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Liu2019ATM,
  title={A Two-Stage Model Based on BERT for Short Fake News Detection},
  author={Chao Liu and Xinghua Wu and Min Yu and Gang Li and Jianguo Jiang and Wei-qing Huang and Xiang Lu},
  booktitle={KSEM},
  year={2019}
}

%%% article on covid19 & misinformation

@article{OConnor2020GoingVD,
  title={Going Viral: Doctors Must Tackle Fake News in the Covid-19 Pandemic},
  author={Oâ€™Connor, Cathal and Murphy, Michelle},
  journal={Bmj},
  volume={369},
  number={10.1136},
  year={2020},
  publisher={British Medical Journal Publishing Group}
}


@inproceedings{ding2020bert,
  title={BERT-based Mental Model, a Better Fake News Detector},
  author={Ding, Jia and Hu, Yongjun and Chang, Huiyou},
  booktitle={Proceedings of the 2020 6th international conference on computing and artificial intelligence},
  pages={396--400},
  year={2020}
}

@inproceedings{thorne-vlachos-2018-automated,
    title = "Automated Fact Checking: Task Formulations, Methods and Future Directions",
    author = "Thorne, James  and
      Vlachos, Andreas",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "ACL",
    url = "https://aclanthology.org/C18-1283",
    pages = "3346--3359",
    abstract = "The recently increased focus on misinformation has stimulated research in fact checking, the task of assessing the truthfulness of a claim. Research in automating this task has been conducted in a variety of disciplines including natural language processing, machine learning, knowledge representation, databases, and journalism. While there has been substantial progress, relevant papers and articles have been published in research communities that are often unaware of each other and use inconsistent terminology, thus impeding understanding and further progress. In this paper we survey automated fact checking research stemming from natural language processing and related disciplines, unifying the task formulations and methodologies across papers and authors. Furthermore, we highlight the use of evidence as an important distinguishing factor among them cutting across task formulations and methods. We conclude with proposing avenues for future NLP research on automated fact checking.",
}

@inproceedings{alhindi-etal-2018-evidence,
    title = "Where is Your Evidence: Improving Fact-checking by Justification Modeling",
    author = "Alhindi, Tariq  and
      Petridis, Savvas  and
      Muresan, Smaranda",
    booktitle = "Proceedings of the First Workshop on Fact Extraction and {VER}ification ({FEVER})",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "ACL",
    url = "https://aclanthology.org/W18-5513",
    doi = "10.18653/v1/W18-5513",
    pages = "85--90",
    abstract = "Fact-checking is a journalistic practice that compares a claim made publicly against trusted sources of facts. Wang (2017) introduced a large dataset of validated claims from the POLITIFACT.com website (LIAR dataset), enabling the development of machine learning approaches for fact-checking. However, approaches based on this dataset have focused primarily on modeling the claim and speaker-related metadata, without considering the evidence used by humans in labeling the claims. We extend the LIAR dataset by automatically extracting the justification from the fact-checking article used by humans to label a given claim. We show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half true, mostly true, true).",
}


@inproceedings{Chernyavskiy2020RecursiveNT,
  title={Recursive Neural Text Classification Using Discourse Tree Structure for Argumentation Mining and Sentiment Analysis Tasks},
  author={Chernyavskiy, Alexander and Ilvovsky, Dmitry},
  booktitle={ISMIS},
  pages={90--101},
  year={2020},
  organization={Springer}
}

@inproceedings{silva2021concept,
  title={How Concept Drift can Impair the Classification of Fake News},
  author={Silva, Renato M and Almeida, Tiago A},
  booktitle={Anais do IX Symposium on Knowledge Discovery, Mining and Learning},
  pages={121--128},
  year={2021},
  organization={SBC}
}

@book{chakraborty2021combating,
  title={Combating Online Hostile Posts in Regional Languages during Emergency Situation: First International Workshop, CONSTRAINT 2021, Collocated with AAAI 2021, Virtual Event, February 8, 2021, Revised Selected Papers},
  author={Chakraborty, Tanmoy},
  year={2021},
  publisher={Springer Nature}
}


@article{schuster-etal-2020-limitations,
    title = "The Limitations of Stylometry for Detecting Machine-Generated Fake News",
    author = "Schuster, Tal  and
      Schuster, Roei  and
      Shah, Darsh J.  and
      Barzilay, Regina",
    journal = "Computational Linguistics",
    volume = "46",
    number = "2",
    month = jun,
    year = "2020",
    url = "https://aclanthology.org/2020.cl-2.8",
    doi = "10.1162/coli_a_00380",
    pages = "499--510",
    abstract = "Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation. In light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. These approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. However, in this work, we show that stylometry is limited against machine-generated misinformation. Whereas humans speak differently when trying to deceive, LMs generate stylistically consistent text, regardless of underlying motive. Thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate LM applications from those that introduce false information. We create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of LMs, utilized in auto-completion and editing-assistance settings.1 Our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.",
}

@inproceedings{wang-etal-2021-k,
    title = "{K-Adapter}: {I}nfusing {K}nowledge into {P}re-{T}rained {M}odels with {A}dapters",
    author = "Wang, Ruize  and
      Tang, Duyu  and
      Duan, Nan  and
      Wei, Zhongyu  and
      Huang, Xuanjing  and
      Ji, Jianshu  and
      Cao, Guihong  and
      Jiang, Daxin  and
      Zhou, Ming",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "ACL",
    url = "https://aclanthology.org/2021.findings-acl.121",
    doi = "10.18653/v1/2021.findings-acl.121",
    pages = "1405--1418",
}

@article{kepler,
  author={Xiaozhi Wang and Tianyu Gao and Zhaocheng Zhu and Zhengyan Zhang and Zhiyuan Liu and Juanzi Li and Jian Tang},
  title={KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation},
  year={2021},
  cdate={1609459200000},
  journal={Trans. Assoc. Comput. Linguistics},
  volume={9},
  pages={176-194},
  url={https://transacl.org/ojs/index.php/tacl/article/view/2447}
}

@inproceedings{oshikawa-etal-2020-survey,
    title ={A Survey on Natural Language Processing for Fake News Detection},
    author={Oshikawa, Ray and Qian, Jing and Wang, William Yang},
    booktitle={Proceedings of the 12th Language Resources and Evaluation Conference},
    year={2020},
    pages={6086-6093},
    address={Marseille, France},
    publisher={European Language Resources Association},
    url={https://www.aclweb.org/anthology/2020.lrec-1.747}
}

@inproceedings{peters-etal-2019-knowledge,
  title={Knowledge Enhanced Contextual Word Representations},
  author={Peters, Matthew E.  and Neumann, Mark  and Logan, Robert  and
     Schwartz, Roy  and Joshi, Vidur  and Singh, Sameer  and Smith, Noah A.},
  booktitle={EMNLP/IJCNLP},
  publisher={ACL},
  address={Hong Kong, China},
  year={2019},
  url={https://www.aclweb.org/anthology/D19-1005},
  pages={6086-6093},
}


@inproceedings{wang-2017-liar,
    title = "{``}Liar, Liar Pants on Fire{''}: A New Benchmark Dataset for Fake News Detection",
    author = "Wang, William Yang",
    booktitle = "Proceedings of the 55th Annual Meeting of the ACL (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "ACL",
    url = "https://aclanthology.org/P17-2067",
    doi = "10.18653/v1/P17-2067",
    pages = "422--426",
    abstract = "Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present LIAR: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.",
}

@article{Ferragina2010TAGMEOA,
  title={TAGME: On-the-fly Annotation of Short Text Fragments (by Wikipedia Entities)},
  author={Paolo Ferragina and Ugo Scaiella},
  journal={Proceedings of the 19th ACM international conference on Information and knowledge management},
  year={2010}
}

@inproceedings{Patwa2021FightingAI,
  title={Fighting an Infodemic: COVID-19 Fake News Dataset},
  author={Parth Patwa and Shivam Sharma and Srinivas Pykl and Vineeth Guptha and Gitanjali Kumari and Md. Shad Akhtar and Asif Ekbal and Amitava Das and Tanmoy Chakraborty},
  booktitle={CONSTRAINT@AAAI},
  year={2021}
}

@article{kaliyar2021fakebert,
  title={FakeBERT: Fake News Detection in Social Media with a BERT-based Deep Learning Approach},
  author={Kaliyar, Rohit Kumar and Goswami, Anurag and Narang, Pratik},
  journal={Multimedia tools and applications},
  volume={80},
  number={8},
  pages={11765--11788},
  year={2021},
  publisher={Springer}
}

@inproceedings{NIPS2013_1cecc7a7,
 author={Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
 booktitle={Advances in Neural Information Processing Systems},
 publisher={Curran Associates, Inc.},
 title={Translating Embeddings for Modeling Multi-relational Data},
 url={https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},
 volume={26},
 year={2013}
}

@article{Sharma2019CombatingFN,
  title={Combating Fake News},
  author={Karishma Sharma and Feng Qian and He Jiang and Natali Ruchansky and Ming Zhang and Yan Liu},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  year={2019},
  volume={10},
  pages={1 - 42}
}

@inproceedings{devlin-etal-2019-bert,
    title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author={Devlin, Jacob  and Chang, Ming-Wei  and Lee, Kenton  and
      Toutanova, Kristina},
    booktitle={NAACL-HLT},
    year={2019},
    address={Minneapolis, Minnesota},
    publisher={ACL},
    url={https://www.aclweb.org/anthology/N19-1423},
    pages={4171--4186}
}

@article{Liu2019RoBERTaAR,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Y. Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and M. Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={CoRR},
  volume={abs/1907.11692},
  year={2019},
  url={http://arxiv.org/abs/1907.11692},
  archivePrefix={arXiv},
}

@inproceedings{zhang-etal-2019-ernie,
    title = "{ERNIE}: Enhanced Language Representation with Informative Entities",
    author = "Zhang, Zhengyan  and
      Han, Xu  and
      Liu, Zhiyuan  and
      Jiang, Xin  and
      Sun, Maosong  and
      Liu, Qun",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "ACL",
    url = "https://aclanthology.org/P19-1139",
    doi = "10.18653/v1/P19-1139",
    pages = "1441--1451",
    abstract = "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.",
}

@article{Miller1995WordNetAL,
  title={WordNet: A Lexical Database for English},
  author={G. Miller},
  journal={Commun. ACM},
  year={1995},
  volume={38},
  pages={39-41}
}



@article{gruppi2022nela, title={NELA-GT-2018: A Large Multi-Labelled News Dataset for the Study of Misinformation in News Articles}, volume={13}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/3261}, abstractNote={&lt;p&gt;In this paper, we present a dataset of 713k articles collected between 02/2018-11/2018. These articles are collected directly from 194 news and media outlets including mainstream, hyper-partisan, and conspiracy sources. We incorporate ground truth ratings of the sources from 8 different assessment sites covering multiple dimensions of veracity, including reliability, bias, transparency, adherence to journalistic standards, and consumer trust. The NELA-GT2018 dataset can be found at https://doi.org/10.7910/DVN/ ULHLCB.&lt;/p&gt;}, number={01}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={NÃ¸rregaard, Jeppe and Horne, Benjamin D. and Adali, Sibel}, year={2019}, month={Jul.}, pages={630-638} }

@article{Horne_Adali_2017, 
title={This Just In: Fake News Packs A Lot in Title, Uses Simpler, Repetitive Content in Text Body, More Similar to Satire Than Real News}, volume={11}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/14976}, abstractNote={ &lt;p&gt; The problem of fake news has gained a lot of attention as it is claimed to have had a significant impact on 2016 US Presidential Elections. Fake news is not a new problem and its spread in social networks is well-studied. Often an underlying assumption in fake news discussion is that it is written to look like real news, fooling the reader who does not check for reliability of the sources or the arguments in its content. Through a unique study of three data sets and features that capture the style and the language of articles, we show that this assumption is not true. Fake news in most cases is more similar to satire than to real news, leading us to conclude that persuasion in fake news is achieved through heuristics rather than the strength of arguments. We show overall title structure and the use of proper nouns in titles are very significant in differentiating fake from real. This leads us to conclude that fake news is targeted for audiences who are not likely to read beyond titles and is aimed at creating mental associations between entities and claims. &lt;/p&gt; }, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Horne, Benjamin and Adali, Sibel}, year={2017}, month={May}, pages={759-766} }

@inproceedings{NEURIPS2018_3e9f0fc9,
 author = {Alvarez Melis, David and Jaakkola, Tommi},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 title = {Towards Robust Interpretability with Self-Explaining Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{2016-Election,
Author = {Allcott, Hunt and Gentzkow, Matthew},
Title = {Social Media and Fake News in the 2016 Election},
Journal = {Journal of Economic Perspectives},
Volume = {31},
Number = {2},
Year = {2017},
Month = {May},
Pages = {211-36},
DOI = {10.1257/jep.31.2.211},
URL = {https://www.aeaweb.org/articles?id=10.1257/jep.31.2.211}}

@article{rogers-etal-2020-primer,
    title = "A Primer in {BERT}ology: What We Know about How {BERT} Works",
    author = "Rogers, Anna  and
      Kovaleva, Olga  and
      Rumshisky, Anna",
    journal = "Transactions of the ACL",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.54",
    doi = "10.1162/tacl_a_00349",
    pages = "842--866",
    abstract = "Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.",
}


@inproceedings{Bender2021OnTD,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}