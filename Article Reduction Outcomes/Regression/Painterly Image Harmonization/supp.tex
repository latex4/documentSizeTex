%File: formatting-instructions-latex-2024.tex
%release 2024.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Supplementary for Painterly Image Harmonization by Learning from Painterly Objects}
\author{
    Li Niu\thanks{Corresponding author.},
    Junyan Cao, 
    Yan Hong, 
    Liqing Zhang 
    \\
}
\affiliations{
    %Afiliations
    MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar, \textsuperscript{\rm 2}
    % J. Scott Penberthy, \textsuperscript{\rm 3}
    % George Ferguson,\textsuperscript{\rm 4}
    % Hans Guesgen, \textsuperscript{\rm 5}.
    % Note that the comma should be placed BEFORE the superscript for optimum readability

    % email address must be in roman text type, not monospace or sans serif
    \{ustcnewly, joy\_c1, hy2628982280, lqzhang\}@sjtu.edu.cn
%
% See more examples next
}
%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

In this document, we provide additional materials to support our main paper. In Section~\ref{sec:training_set_preparation}, we describe more details for our training data preparation. In Section~\ref{sec:ablation}, we conduct ablation studies and provide visualization results for the ablated versions. In Section~\ref{sec:out_of_COCO}, we demonstrate the generalization ability of our method.  In Section~\ref{sec:cmp_with_baseline}, we show more visualization results compared with baseline methods.  In Section~\ref{sec:failure_case}, we discuss the failure case of our method.

\section{More Details of Training Data} \label{sec:training_set_preparation}

Following previous works~\cite{peng2019element,cao2022painterly}, we use COCO \cite{lin2014microsoft} and WikiArt \cite{nichol2016painter}.
COCO \cite{lin2014microsoft} contains instance segmentation annotations for 80 object categories, while WikiArt \cite{nichol2016painter} contains digital artistic paintings from different styles.
We create composite images based on the training sets of these two datasets, with the photographic objects from COCO and the painterly backgrounds from WikiArt.

As introduced in Section 3.1 in the main paper, we use off-the-shelf  object detection model \cite{wu2019detectron2} pretrained on COCO \cite{lin2014microsoft} to detect $34,570$ objects from the artistic paintings in the training set of WikiArt. 
For each painterly object, we aim to retrieve the photographic objects with similar appearance and semantics automatically. 

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{figures/object_retrieval.jpg}
\caption{The network structure for object retrieval. We use VGG-19 \cite{VGG19} encoder and projection module $P$ to extract object feature $\hat{\bm{f}}^{a,o}$ (\emph{resp.}, $\hat{\bm{f}}^{b,o}$) from  painterly (\emph{resp.}, photographic) object in $\bm{I}^a$ (\emph{resp.}, $\bm{I}^b$). }
\label{fig:object_retrieval}
\end{figure}



\begin{figure*}[t]
\centering
\includegraphics[width=0.99\linewidth]{figures/reference_objects_supp.jpg}
\caption{Example annotated pairs of photographic objects and reference objects, in which the objects are outlined in yellow. Paired objects in each column have similar color and semantics.}
\label{fig:reference_object_supp}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\linewidth]{figures/ablation_studies.jpg}
\caption{From left to right, we show the composite image, composite foreground mask, the harmonized results of row 1-6 in Table \ref{tab:ablation}.}
\label{fig:ablation_studies}
\end{figure*}


To achieve this goal, we design a simple object retrieval network as illustrated in Figure~\ref{fig:object_retrieval}, which is similar to our main network. Specifically, we use pretrained VGG-19 network~\cite{VGG19} followed by projection module $P$ (a residual block \cite{he2016deep}) to produce feature maps for both artistic paintings and photographic images, in which $P$ aims to project two domains into a common domain. Given an artistic painting $\bm{I}^a$ and a photographic image $\bm{I}^b$, we extract their feature maps and perform average pooling within the foreground region, producing the painterly object feature $\hat{\bm{f}}^{a,o}$ and the photographic object feature $\hat{\bm{f}}^{b,o}$. To pull close the object features from two domains, we employ adversarial loss $\mathcal{L}_{adv}$ \cite{goodfellow2020generative} to make the painterly object features indistinguishable from photographic object features. Moreover, to preserve the discriminative information of objects, we also apply classification loss $\mathcal{L}_{cls}$ to object features. For photographic objects in COCO dataset \cite{lin2014microsoft}, we have the ground-truth category labels provided by \cite{lin2014microsoft}. For painterly objects, we use the category labels predicted by the object detection model \cite{wu2019detectron2}. For photographic object features and painterly object features, we use the same classifier so that the object features from the same category yet different domains can be grouped together. 
Thus, the total loss to train the object retrieval network can be written as $\mathcal{L}_{ret}=\mathcal{L}_{adv}+\mathcal{L}_{cls}$.

After training the object retrieval network, for each painterly object, we retrieve $100$ nearest photographic objects based on $L_2$ distance between their object features. Nevertheless, the retrieved results are very noisy and far from usable. Therefore, we only treat the retrieved objects as candidate objects, and ask human annotators to filter out dissimilar candidate objects. After filtering, we have $33,294$ painterly objects associated with similar photographic objects, and
each painterly object has an average of $9.83$ similar photographic objects. Given a pair of painterly object and its similar photographic object, we refer to the painterly object as the reference object of this photographic object.
In Figure \ref{fig:reference_object_supp}, we show several example photographic objects and their reference objects. It can be seen that the objects cover a wide range of categories including person, animal, vehicle, furniture, and so on. The photographic objects have similar color and semantics with their reference objects.


\begin{table}[t] 
\centering
\begin{tabular}{c|c|c}
\hline
Row & Method  & B-T score \\
\hline
1 &  w/o ObAdaIN &  -0.475\\
2 & w/o $\hat{\bm{f}}^{co}$/$\hat{\bm{f}}^{po}$  & -0.289\\
3 & w/o $\mathcal{L}_{obj}$  & 0.0278\\
4 & w/o $\mathcal{L}_{map}^p$   & 0.162 \\
5 & w/o $\mathcal{L}_{map}^c$  & 0.211 \\
6 & Full  & 0.363 \\
\hline
\end{tabular}
\caption{Results of different ablated versions of our ArtoPIH. }
\label{tab:ablation}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{figures/failure_cases.jpg}
\caption{Two example failure cases of our ArtoPIH.}
\label{fig:failure_cases}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.92\linewidth]{figures/out_of_COCO_aug.jpg}
\caption{From left to right, we show the composite image, composite foreground mask, the harmonized result using background style vector, and our harmonized result.}
\label{fig:out_of_COCO}
\end{figure}

\begin{figure*}[h]
\centering
\includegraphics[width=0.92\linewidth]{figures/baseline_supp2.jpg}
\caption{In the upper part, we compare with style transfer baselines SANet~\cite{park2019arbitrary}, AdaAttN~\cite{liu2021adaattn}, StyTr2~\cite{deng2022stytr2}, QuantArt~\cite{quantart}, INST~\cite{inst}. 
In the lower part, we compare with painterly image harmonization baselines SDEdit~\cite{sdedit}, CDC~\cite{cdc}, E2STN~\cite{peng2019element}, DPH~\cite{luan2018deep}, PHDNet~\cite{cao2022painterly}.
 }
\label{fig:baseline_supp}
\end{figure*}




\section{Ablation Studies} \label{sec:ablation}

We first build a basic model and then gradually add our designed modules. First, we replace our ObAdaIN module with vanilla AdaIN, by applying background style to the composite object, leading to the basic model in row 1. 
Then, we learn the mapping from background style to object style without using object feature $\hat{\bm{f}}^{co}$/$\hat{\bm{f}}^{po}$, leading to row 2. Next, we add object features in the mapping, leading to our full method in row 6. 
Next, we ablate each loss term $\mathcal{L}_{obj}$, $\mathcal{L}_{map}^p$, and $\mathcal{L}_{map}^c$ in row 3, 4, and 5, respectively. 

We report the user study results (B-T score) for different ablated versions in Table \ref{tab:ablation}. We observe that row 1 achieves the worst performance by directly using background style. Row 2 is only slightly better than row 1 because the object style is not conditioned on the specific object information.  By comparing row 3-5 with row 6, we see that each ablated loss term causes performance drop, which proves that three loss terms contribute to the final performance altogether. 



Next, we show the visualization results of different ablated versions in Figure \ref{fig:ablation_studies}. It can be seen that the row 1 and row 2 could only produce very poor results with notable artifacts,  because they directly transfer background style or hallucinate unsuitable target styles. Row 3-5 are better than row 1-2, but worse than row 6, which proves that each loss term can help improve the performance. 





\section{Results beyond COCO Dataset} \label{sec:out_of_COCO}
To demonstrate the generalization ability of our ArtoPIH, we collect  photographic objects from Open Images \cite{Kuznetsova2020TheOI} dataset, which are out of $80$ COCO \cite{lin2014microsoft} object categories. Then, we create composite images using the collected photographic objects and artistic paintings from WikiArt \cite{nichol2016painter} test set. We show the harmonized results of our method in Figure~\ref{fig:out_of_COCO}. For comparison, we also show the harmonized results obtained using background style vector (BG), in the same way as Figure 5 in the main paper. Note that some exhibited categories may have similar ones in COCO dataset. For example, ``beer" in row 6 is similar to ``cup" in COCO, and ``whiteboard" in row 4 is similar to ``book" in COCO. Some other categories (\emph{e.g.}, ``rooster" in row 1) are farther from their similar ones in COCO. For all these categories, our method can produce more faithful and visually pleasant images than ``BG", which verifies that our ArtoPIH can generalize well to the foreground objects out of COCO dataset. 

\section{More Visual Comparison with Baselines} \label{sec:cmp_with_baseline}

In Section 4.2 in the main paper, we compare with two groups of baselines: artistic style transfer baselines and painterly image harmonization baselines.  

We show the comparison with two groups of baselines in Figure \ref{fig:baseline_supp}. Compared with the first group of baselines, we observe that our method can usually produce more visually pleasing objects which seem to appear naturally on the background. The harmonized results of baselines are prone to  suffer from unreasonable color, noticeable artifacts, or under-stylization. Compared with the second group of baselines, our method can produce well-harmonized foreground object while preserving the original object information. In contrast, the harmonized results from baselines are likely to have unsatisfactory stylization effect, or destroy the original object information. 


\section{Failure Cases} \label{sec:failure_case}

Although our ArtoPIH can usually achieve satisfactory results, there also exist some failure cases. For example, as shown in Figure~\ref{fig:failure_cases}, when the background has unified color style, the harmonized foreground is likely to have color mismatch with the background, because the predicted style vector may deviate from the background style vector in terms of color information. One possible solution is first detecting whether the background has unified color style. If so, we can directly apply background style vector to the foreground to be on the safe side. 










\section*{Acknowledgments}
The work was supported by the National Natural Science Foundation of China (Grant No. 62076162), the Shanghai Municipal Science and Technology Major/Key Project, China (Grant No. 2021SHZDZX0102, Grant No. 20511100300).  


\bibliography{supp.bbl}

\end{document}
