\def\year{2020}\relax
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{graphicx}  % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS

\usepackage[fleqn]{amsmath}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{bm}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
\usepackage{xcolor}


\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

\setlength\titlebox{2.5in} % If your paper contains an overfull \vbox too high warning at the beginning of the document, use this
\title{Hierarchical Average Reward Policy Gradient Algorithms}
\author{Akshay Dharmavaram\\
Birla Institute of Technology and Science, Pilani\\
f20150039@goa.bits-pilani.ac.in\\
 +91-9604079793
\And Matthew Riemer\\
IBM Research\\
mdriemer@us.ibm.com
\And
Shalabh Bhatnagar\\
Indian Institute of Science, Bangalore\\
shalabh@iisc.ac.in
}
 \begin{document}

\maketitle

\begin{abstract}
Option-critic learning is a general-purpose reinforcement learning (RL) framework that aims to address the issue of long term credit assignment by leveraging temporal abstractions. However, when dealing with extended timescales, discounting future rewards can lead to incorrect credit assignments. In this work, we address this issue by extending the hierarchical option-critic policy gradient theorem for the average reward criterion. Our proposed framework aims to maximize the \textit{long-term} reward obtained in the steady-state of the Markov chain defined by the agent's policy. Furthermore, we use an ordinary differential equation based approach for our convergence analysis and prove that the parameters of the intra-option policies, termination functions, and value functions, converge to their corresponding optimal values, with probability \textit{one}. Finally, we illustrate the competitive advantage of learning options, in the average reward setting, on a grid-world environment with sparse rewards.
\end{abstract}

\section{Introduction}
\noindent Humans routinely employ high-level temporal abstractions for everyday decision making. \citet{oc} investigate the use of learning temporally extended abstractions in order to augment the exploration and credit assignment capabilities of the actor-critic framework. However, employing a discount factor to bound the cumulative rewards can inadvertently lead to incorrect credit assignment. We addresses this issue by extending the framework proposed by \citet{weightsharingAAAI} for the average reward (AR) criterion.

Figure \ref{fig3}(a) is a motivating example that illustrates how simple \textit{traps} can beguile the discounted rewards (DR) framework into learning a sub-optimal credit assignment. It illustrates two different Markov chains, resulting from two disparate policies ($\pi_{R}$ and $\pi_{B}$). $\pi_{R}$ always chooses red and $\pi_{B}$ always chooses blue. When a DR-RL agent is at $S_{0}$, it has a predilection for the sub-optimal policy $\pi_{B}$, because $\forall \gamma<1$:
\begin{footnotesize}
\begin{align*}
    v_{\pi_{R}}(S_{11})=\frac{\gamma(2-\gamma)}{(1-\gamma^{4})} < \frac{1}{(1-\gamma^{4})}= v_{\pi_{B}}(S_{21})
\end{align*}
\end{footnotesize}


\section{Policy-Gradient with Function Approximation}

First, we illustrate how to extend the framework proposed by \citet{weightsharingAAAI} for the AR criterion. Apart from addressing the AR criterion, our framework also presents a simplified and intuitive approach to dealing with hierarchical option-critic algorithms \cite{hoc} by introducing the concept of $o^{0}$ and $o^{N}$.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{AVvsDis.png} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{ Mean and standard deviations of the learning curves for the average reward and discounted reward OCPG agents, in the grid-world delivery experiment.}
\label{fig1}
\end{figure}
\begin{theorem}[Hierarchical Average Reward Option-Critic Policy Gradient (OCPG) Theorem]
\label{HARPG}
Given an $N$ level hierarchical set of  Markov  options  with  stochastic  option  policies at each level $\pi^\ell$ and termination functions at each level $\beta^\ell$ differentiable in their parameters $\bm{\theta}$, the gradient of the expected reward per step with respect to $\bm{\theta}$ is:

\begin{footnotesize}
\begin{align*}
    \sum_{s,o^{0:N-1},s'}\!\!\!\!\mu_\Omega(s,o^{0:N-1},s')
    \bigg(\sum_{a} \frac{\partial \pi(a|s,o^{0:N-1})}{\partial \bm{\theta}} \\
    Q_U(s,o^{0:N-1},a) +  \sum_{o'^{0:N-1}} \sum_{\ell=1}^{N-1} \bigg[ \!\prod_{k=N-1}^{\ell}\!\!\!\!\beta^k(s',o^{0:k}) \\ \frac{\partial \pi^\ell(o'^\ell|s',o'^{0:\ell-1})}{\partial \bm{\theta}} Q_\Omega(s',o'^{0:\ell}) P_{\pi,\beta}(o'^{0:\ell-1}|s',o^{0:\ell-1}) \\
    - \frac{\partial \beta^\ell(s',o^{0:\ell})}{\partial \bm{\theta}} A_\Omega(s',o^{0:\ell})\!\!\!\!\prod_{k=N-1}^{\ell+1}\!\!\!\!\beta^k(s',o^{0:k}) \bigg] \bigg),
\end{align*}
\end{footnotesize}
where $\mu_\Omega$ is the stationary distribution of the Markov chain defined by the hierarchical policy, and $P_{\pi,\beta}$ is the probability while at the next state, and terminating the options for the last state, that the agent arrives at a particular new set of option selections.
\end{theorem}

\begin{proof}
The proof for this theorem is in the Appendix.
\end{proof}

\section{Two-Timescale Convergence}
Next, we prove that the aforementioned parameters, $\theta$, asymptotically converge to their optimal values, when employing a linear approximation $\forall$ $Q_\Omega$.
We analyze our framework using the ordinary differential equation (ODE) approach, delineated by \citet{bhatnagar2009natural}, and study its asymptotic properties using the fixed points of the derived ODE.


\begin{theorem}[Convergence Proof]
\label{conv}
For the parameter iterations of the global set of shared parameters defined in Algorithm 1, we have ($\hat{J}_{t}, \upsilon_{t}, \theta_{t} $) $\to$ $\{(J(\theta^{*})_{t}, \upsilon^{*}, \theta^{*} )|\theta^{*} \in \mathcal{Z}\}$ as t $\to \infty$ with probability one, where $\mathcal{Z}$ corresponds to the set of local maxima of a performance function whose gradient is $E[\delta^{\pi}_{t}\psi(s_{t},a_{t})|\theta]$
\end{theorem}


\begin{proof}
The proof for this theorem is in the Appendix.
\end{proof}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{MultiPlot.png} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{An empirical demonstration illustrating the convergence of the parameters of $Q(s,o)$, $\pi(a|s,o)$, and $\pi(o|s)$. We have randomly selected one parameter from each function approximator and plotted its value against the number of steps.}
\label{fig2}
\end{figure}


\section{Empirical Results}
Finally, we look at the susceptibility of our framework to traps, and compare it to the DR setting proposed by \citet{weightsharingAAAI}. Figure \ref{fig3}(b) depicts a grid world environment characterized by sparse rewards. An agent must navigate to either one of the pickup locations, $P_{1}$ or $P_{2}$, in order to retrieve a parcel; and must subsequently deliver the parcel to the drop off location. The agent gets a reward of +100 for every parcel from $P_{2}$, and +50 for every parcel from $P_{1}$. The optimal policy for an agent would naturally involve picking up the parcels from $P_{2}$.

We introduce a trap\footnote[1]{The reward of +20 was primarily chosen for illustrating the potential pitfalls when employ a $\gamma \leq 0.9$. Similar traps can be created for any $\gamma \leq 1$.} at the green-blue junction to entice the DR-RL agents into picking up the parcels from $P_{1}$. Once the agent reaches the blue zone, it obtains a reward of +20 as opposed to a reward of +10 at the red-green junction. In Figure \ref{fig1}, we plot the rewards obtained per cycle for both the AR-RL agent and a DR-RL agent, and show that the hierarchical AR policy gradient performs better than its DR counterpart proposed by \citet{weightsharingAAAI}. Finally, we illustrate the asymptotic convergence of the actor and critic parameters in Figure \ref{fig2}.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{BadCreditAndFRms.jpg} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{ \textbf{(a)} A \textit{trap} that employs delayed rewards to fool DR-RL agents into learning incorrect credit assignments. \textbf{(b)} A grid-world navigation experiment where the reward at the drop off point depends upon which pickup location was previously visited (50 for $P_1$ and 100 for $P_2$). The trap at the blue-green junction misguides agents towards the sub-optimal pickup location, $P_1$. }
\label{fig3}
\end{figure}

\section{Conclusion and Future Work}
In this work, we propose a novel method for maximizing the long term steady-state reward, by learning intra-option policies, termination functions, and value functions end-to-end. These algorithms can be used in infinite-horizon control problems that exhibit an inherent cyclic structure, like inventory-management, queuing and traffic light control. A detailed empirical analysis for a cyclical infinite-horizon application would be necessary to demonstrate the viability of our approach in complex environments. Additionally, while the proofs provided here leverage a linear approximation for each of the $Q_\Omega(s,o^{0:\ell})$, it would also be interesting to investigate the convergence properties of a non-linear critic.




\bibliographystyle{aaai}
Odit facilis voluptate illum soluta laboriosam dolor facere placeat, dignissimos ratione veritatis recusandae in aspernatur laborum distinctio vero consequuntur qui temporibus, laudantium repellat quos porro delectus eos error voluptas voluptatum architecto, sit repellat rerum accusamus.Ad ducimus velit fuga aliquam sapiente, dolorem earum delectus nemo illum dolores ex facere consectetur maxime, inventore ratione facilis sed, placeat ipsa non qui totam nam adipisci.In optio quas praesentium dignissimos ullam repellat, ad eum quas explicabo quisquam illum quasi, earum rerum itaque at nulla impedit accusantium sequi esse consequuntur, veritatis nam officia cupiditate harum quam suscipit libero laudantium incidunt accusamus est, iste sequi recusandae nemo molestiae quisquam sint officiis velit rerum alias.Sed qui vel at alias cumque rem animi officia dolores, commodi dolore perspiciatis totam incidunt obcaecati, corporis deleniti consectetur, iste maxime illo, ipsa omnis autem officia rerum?Veniam culpa vitae laudantium, itaque officiis temporibus dicta laboriosam similique nam eligendi dignissimos vitae neque et, nihil delectus incidunt
\bibliography{bibliography}


\end{document}