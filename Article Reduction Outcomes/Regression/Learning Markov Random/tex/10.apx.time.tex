
\section{Running Time Analysis of Algorithm~\ref{alg:lll-sampler}} \label{appendix:time-analysis}
We dedicate this section to showing the running time of Algorithm~\ref{alg:lll-sampler} on a general weighted case.
%
The expected running time of Algorithm~\ref{alg:lll-sampler} is determined by the number of rounds of re-sampling.  
%
Algorithm~\ref{alg:lll-sampler} re-sample all the related random variables simultaneously in every single round.
%
However, it is hard to get an estimation of the exact total running time over the \textit{random variables}. Instead, we can only have a loose upper bound of the expected running time over the \textit{sequence of sampling record} (the sequence of violated constraints).


The overall structure of the proof is similar to the proof in~\citet[Theorem 13]{DBLP:journals/jacm/GuoJL19}. We show the difference in our proof at the end of this section.

\subsection{Definitions and Notations}
We define the following terms to simplify our notations.
\begin{definition}  \label{def:p_q_st}
Let $S_t$ be a subset of vertices in a dependency graph.
1) Define $p_{S_t}$ as the probability of  constraints in $S_t$ being violated:
\begin{equation} \label{eq:def-pst}
p_{S_t}=\p\left(\bigwedge_{c_i \in S_t}\neg{c}_i \right)
\end{equation}
{where we use $\neg{c}_i $ to indicate the constraint $c_i$ is violated.}
2) Define $q_{S_t}$ as the probability that only the constraints in $S_t$ are violated and nothing else. 
\begin{equation} \label{eq:definition-qst}
q_{S_t}=\p\left(\bigwedge_{c_i \in S_t}\neg{c}_i \wedge \bigwedge_{c_j \in \mathcal{C}\backslash S_t}c_j\right)
\end{equation}
{where $\bigwedge_{c_i \in S_t}\neg{c}_i $ corresponds to only the constraints in $S_t$ are violated and $\bigwedge_{c_j \in \mathcal{C}\backslash S_t}c_j$ corresponds to all the rest constraints are satisfied.}
So $q_{\{c_i\}}$ is the probability that only constraint $c_i$ is broken and all the rest still  hold. Similarly, $q_{\emptyset}$ denotes the probability that all the constraints are satisfied.
\end{definition}



\begin{lemma} \label{lem:recursion_start}
Given Definition~\ref{def:p_q_st}, we can further expand $q_{S_t}$ under Condition~\ref{cond:extreme}:
\begin{equation*}
q_{S_t}= p_{S_t}\p \left( \wedge_{c_j \in \mathcal{C}\backslash \Gamma(S_t)}c_j\right)
\end{equation*}
\begin{proof}
We can split $q_{S_t}$ into the probability of two independent events:
\begin{align*} 
    q_{S_t}&=\p\left(\bigwedge_{c_i \in S_t}\neg{c}_i \wedge \bigwedge_{c_j \in \mathcal{C}\backslash S_t}c_j\right) &\text{ By definition of $q_{S_t}$ in Equation~\eqref{eq:definition-qst}}\\
    &=\p\left(\bigwedge_{c_i \in S_t}\neg{c}_i \wedge \bigwedge_{c_j \in \mathcal{C}\backslash \Gamma(S_t)}c_j\right) \\
    &=\p\left(\bigwedge_{c_i \in S_t}\neg{c}_i\right) \p \left( \bigwedge_{c_n \in \mathcal{C}\backslash \Gamma(S_t)}c_j\right)\\
    &= p_{S_t}\p \left( \wedge_{c_j \in \mathcal{C}\backslash \Gamma(S_t)}c_j\right). &\text{ By definition of $p_{S_t}$ in Equation~\eqref{eq:def-pst}}
\end{align*}
The second equality holds because under  Condition~\ref{cond:extreme}, adjacent vertices have zero probability. In other words, when we observe that constraints in $S_t$ are violated, constraints in $\Gamma(S_t)\backslash S_t$ cannot be violated. The third equality holds because the random variables in $\var(S_t)$ are \textit{independent to} 
those variables in $\var(\mathcal{C}\backslash\Gamma(S_t))$. {So we can apply $P(AB)=P(A)P(B)$ when the events $A,B$  are independent to each other.}
\end{proof}
\end{lemma}

\begin{remark}[Equivalence of Record] \label{rem:equal}
At round $t$ of Algorithm~\ref{alg:lll-sampler},  it finds all the constraints $S_t$ that are broken ($\bigwedge_{c_i \in S_t}\neg{c}_i $), which implies the rest of the constraints $\mathcal{C}\backslash \Gamma(S_t)$ are satisfied ($\bigwedge_{c_j \in \mathcal{C}\backslash S_t}c_j$). Thus the probability of observing $S_t$ in the record is equivalent to the following:
\begin{align} \label{eq:st-expand}
\p(S_t)=\p\left(\bigwedge_{c_i \in S_t}\neg{c}_i \wedge \bigwedge_{c_j \in \mathcal{C}\backslash S_t}c_j\right)
\end{align}
\end{remark}

\begin{lemma} \label{eq:pair-prob}
Given a possible sampling record $S_1$ $\ldots$ $S_{t-1}$,$S_t$ by Algorithm~\ref{alg:lll-sampler}, the following equality holds for the pair $(S_{t-1},S_t)$:
\begin{equation*}
\sum_{S_t}q_{S_t}=\p(\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i)
\end{equation*}
\begin{proof}
By Definition~\ref{def:record} of the sampling record, we have $S_t \subset \Gamma(S_{t-1})$. The  relationship of its complement would be:
\begin{equation*}
\mathcal{C}\backslash\Gamma(S_{t-1})\subset \mathcal{C}\backslash S_t.
\end{equation*}
Using the above result, we have:
\begin{equation}\label{eq:subset}
\p\left(\bigwedge_{c_j \in \mathcal{C}\backslash S_t}c_j \wedge \bigwedge_{c_k \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_k \right)=
\p\left(\bigwedge_{c_j \in \mathcal{C}\backslash S_t}c_j \right)
\end{equation}

Based on Remark~\ref{rem:equal} and Baye's theorem, we have:
\begin{equation} \label{eq:bayes}
\begin{aligned}
\p(S_t |\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i)&=\p\left(\bigwedge_{c_i \in S_t}\neg{c}_i \wedge \bigwedge_{c_j \in \mathcal{C}\backslash S_t}c_j \Big| \wedge_{c_k \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_k \right) & \text{ By Equation~\eqref{eq:st-expand}}\\
&=\frac{\p\left(\bigwedge_{c_i \in S_t}\neg{c}_i \wedge \bigwedge_{c_j \in \mathcal{C}\backslash S_t}c_j \wedge \bigwedge_{c_k \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_k \right)}{\p \left(\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i\right) } &\text{By Bayes's formula}\\
&=\frac{\p\left(\bigwedge_{c_i \in S_t}\neg{c}_i \wedge \bigwedge_{c_k \in \mathcal{C}\backslash S_{t}}c_k \right)}{\p \left(\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i\right) } &\text{ By Equation~\eqref{eq:subset}}\\
&= \frac{q_{S_t} }{\p \left(\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i\right) }. &\text{ By definition of $p_{S_t}$ in Equation~\eqref{eq:definition-qst}}
\end{aligned}
\end{equation}

Since LHS of Equation~\eqref{eq:bayes} sums over all possible $S_t$ is one: $\sum_{S_t}\p(S_t |\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i)=1$. Thus, summing over $S_t$ for the RHS of Equation~\eqref{eq:bayes}, we have:
\begin{equation} \label{eq:summation-st} 
\begin{aligned}
1=\sum_{S_t}\frac{q_{S_t} }{\p \left(\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i\right) }=\frac{\sum_{S_t}q_{S_t} }{\p \left(\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i\right) }
\end{aligned}
\end{equation}
In the second equality, the reason that we can move the summation operator to  the numerator is that the denominator is a constant \textit{w.r.t.} all possible $S_t$. To be specific, given $S_t\subseteq\Gamma(S_{t-1})$, we have  $S_t$ is independent to $\mathcal{C}\backslash \Gamma(S_{t-1})$.
Based on Equation~\eqref{eq:summation-st}, we finally obtain:
\begin{equation*}
\sum_{S_t}q_{S_t}=\p(\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i).
\end{equation*}
The proof  is finished. 
\end{proof}
\end{lemma}


\begin{lemma} \label{lem:sample-record}
The probability of observing the sampling record  $S_1,\ldots,S_T$ by Algorithm~\ref{alg:lll-sampler} under Condition~\ref{cond:extreme} is:
\begin{equation}\label{pS_final}
    \p(S_1,\ldots ,S_T)=q_{S_T} \prod_{t=1}^{T-1} p_{S_t} 
\end{equation}
\begin{proof}
Given sampling record  $S_1,\ldots,S_{t-1}$, the conditional probability of observing the next record {$S_t,S'_t$} can be expanded  based on Lemma~\ref{lem:ratio-prob},
\begin{equation*}
\frac{\p(S_t | S_1, \ldots, S_{t-1})}{\p(S'_t | S_1, \ldots, S_{t-1})}=\frac{\p(S_t |\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i)}{\p(S'_t |\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i)}
\end{equation*}
Based on Equation~\eqref{eq:bayes}, we {can simplify the RHS of the above ratio equality and}] obtain:
\begin{equation*}
\frac{\p(S_t | S_1, \ldots, S_{t-1})}{\p(S'_t | S_1, \ldots, S_{t-1})}=\frac{q_{S_t}}{q_{S'_t}}
\end{equation*}
Because of $\sum_{S_t}\p(S_t | S_1, \ldots, S_{t-1})=1$ and Equation~\eqref{eq:summation-st}, we can get:
\begin{equation}\label{recursion_base}
  \p(S_t | S_1, \ldots, S_{t-1})=  \frac{q_{S_t}}{\p(\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i)}
\end{equation}
We finally compute the probability of observing the sampling record $S_1\,\ldots, S_T$ by:
\begin{equation*}
\begin{aligned} 
    \p(S_1\,\ldots ,S_T)=&   \p(S_1)\prod_{t=2}^T\p(S_t | S_1,\ldots,S_{t-1})  &\text{ By Chain rule}\\
    =&q_{S_1}\prod_{t=2}^T \frac{q_{S_t}}{\p(\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i)}  &\text{ By Equation~\eqref{recursion_base}}\\
    =&q_{S_T} \prod_{t=2}^T \frac{q_{S_{t-1}}}{\p(\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i)}  &\text{ Left shift the numerator from $S_t$ to $S_{t-1}$}\\
    =&q_{S_T} \prod_{t=1}^{T-1} p_{S_t} & \text{Plugin  Lemma~\ref{lem:recursion_start}}
\end{aligned}    
\end{equation*}
The proof is finished.
\end{proof}
\end{lemma}




\subsection{An Upper Bound on Expected Running Time}
Suppose the expected number of samplings of constraints $c_i$ is $\e(T_i)$, then the total running time will be:
\begin{equation*}
\e(T) \le \sum_{i=1}^n \e (T_i)
\end{equation*}

Since each random variable has equal status, then the question comes down to the computation of individual $T_i$'s expectation. Let $S_1,\ldots,S_T$ be any record of the algorithm that successfully terminates, and $T_i(S_1,\ldots,S_T)$ be the total number of sampling related to constraint $c_i$ throughout this record.  Based on Lemma~\ref{lem:sample-record}, we have:
\begin{align*} \label{ti_formula}
    \e(T_i) &= \sum_{S_1,\ldots,S_T} \p(S_1,\ldots,S_T)T_i(S_1,\ldots,S_T) 
\end{align*}
{By far, we have shown the original proof of our work. We leave the difference between our proof with the existing one in Appendix~\ref{apx:difference}.}

The rest of the computation can be done in the same way as the proof in \citet{DBLP:journals/jacm/GuoJL19}. Thus we cite the necessary intermediate steps in the existing work and finish the proof logic for the coherence of the whole running time analysis.
\begin{lemma*}[\citet{DBLP:journals/jacm/GuoJL19} Lemma 12] \label{thm:runtime} Let $q_{\emptyset}$ be a non-zero probability of all the constraints are satisfied. Let $q_{\{c_j\}}$ denote the probability that only constraint $c_j$ is broken and the rest all hold. If $q_\emptyset>0$, then
$\e(T_i) ={q_{\{c_j\}}}/{q_\emptyset}$.
\end{lemma*}

After incorporating our fix, we can conclude the upper bound on the expected running time in Theorem~\ref{thm:time}.
\begin{theorem}[\citet{DBLP:journals/jacm/GuoJL19} Theorem 13] \label{thm:time} Under Condition~\ref{cond:extreme},
 the total number of re-samplings throughout the algorithm is then $\frac{1}{q_{\emptyset}}\sum_{j=1}^L q_{\{c_j\}}$.
\end{theorem}

\subsection{Difference to the Existing Proof}  \label{apx:difference}
 The main difference in the above proof to the existing proof in~\cite[Theorem 13]{DBLP:journals/jacm/GuoJL19} is that: based on Equation~\eqref{recursion_base} and~\eqref{eq:bayes}, we show
\begin{equation*}
\p(S_t | S_1, \ldots, S_{t-1})=\p(S_t |\wedge_{c_i \in \mathcal{C}\backslash \Gamma(S_{t-1})}c_i)
\end{equation*}
In \citet{DBLP:journals/jacm/GuoJL19}'s Equation~(9), the first step cannot holds without the above equality. The original paper uses this result directly without providing enough justification.

