\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi:\discretionary{}{}{}#1}\else
  \providecommand{\doi}{doi:\discretionary{}{}{}\begingroup
  \urlstyle{rm}\Url}\fi

\bibitem[{Alistarh, Allen-Zhu, and Li(2018)}]{alistarh2018byzantine}
Alistarh, D.; Allen-Zhu, Z.; and Li, J. 2018.
\newblock Byzantine stochastic gradient descent.
\newblock In \emph{NeurIPS}.

\bibitem[{Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic}]{alistarh2017qsgd}
Alistarh, D.; Grubic, D.; Li, J.; Tomioka, R.; and Vojnovic, M. 2017.
\newblock QSGD: Communication-efficient SGD via gradient quantization and
  encoding.
\newblock In \emph{NeurIPS}.

\bibitem[{Anguita et~al.(2013)Anguita, Ghio, Oneto, Parra, and
  Reyes-Ortiz}]{anguita2013public}
Anguita, D.; Ghio, A.; Oneto, L.; Parra, X.; and Reyes-Ortiz, J.~L. 2013.
\newblock A public domain dataset for human activity recognition using
  smartphones.
\newblock In \emph{ESANN}.

\bibitem[{Bagdasaryan et~al.(2020)Bagdasaryan, Veit, Hua, Estrin, and
  Shmatikov}]{Bagdasaryan18}
Bagdasaryan, E.; Veit, A.; Hua, Y.; Estrin, D.; and Shmatikov, V. 2020.
\newblock How to backdoor federated learning.
\newblock In \emph{AISTATS}.

\bibitem[{Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and
  Anandkumar}]{bernstein2018signsgd}
Bernstein, J.; Wang, Y.-X.; Azizzadenesheli, K.; and Anandkumar, A. 2018.
\newblock signSGD: Compressed Optimisation for Non-Convex Problems.
\newblock In \emph{ICML}.

\bibitem[{Bhagoji et~al.(2019)Bhagoji, Chakraborty, Mittal, and
  Calo}]{Bhagoji19}
Bhagoji, A.; Chakraborty, S.; Mittal, P.; and Calo, S. 2019.
\newblock Analyzing Federated Learning through an Adversarial Lens.
\newblock In \emph{ICML}.

\bibitem[{Blanchard et~al.(2017)Blanchard, Mhamdi, Guerraoui, and
  Stainer}]{Blanchard17}
Blanchard, P.; Mhamdi, E. M.~E.; Guerraoui, R.; and Stainer, J. 2017.
\newblock Machine Learning with Adversaries: Byzantine Tolerant Gradient
  Descent.
\newblock In \emph{NeurIPS}.

\bibitem[{Bonawitz et~al.(2017)Bonawitz, Ivanov, Kreuter, Marcedone, McMahan,
  Patel, Ramage, Segal, and Seth}]{bonawitz2017practical}
Bonawitz, K.; Ivanov, V.; Kreuter, B.; Marcedone, A.; McMahan, H.~B.; Patel,
  S.; Ramage, D.; Segal, A.; and Seth, K. 2017.
\newblock Practical secure aggregation for privacy-preserving machine learning.
\newblock In \emph{CCS}.

\bibitem[{Chen et~al.(2018)Chen, Wang, Charles, and
  Papailiopoulos}]{chen2018draco}
Chen, L.; Wang, H.; Charles, Z.; and Papailiopoulos, D. 2018.
\newblock DRACO: Byzantine-resilient Distributed Training via Redundant
  Gradients.
\newblock In \emph{ICML}.

\bibitem[{Chen, Su, and Xu(2017)}]{ChenPOMACS17}
Chen, Y.; Su, L.; and Xu, J. 2017.
\newblock Distributed Statistical Machine Learning in Adversarial Settings:
  Byzantine Gradient Descent.
\newblock In \emph{POMACS}.

\bibitem[{Clopper and Pearson(1934)}]{clopper1934use}
Clopper, C.~J.; and Pearson, E.~S. 1934.
\newblock The use of confidence or fiducial limits illustrated in the case of
  the binomial.
\newblock \emph{Biometrika} .

\bibitem[{Fang et~al.(2020)Fang, Cao, Jia, and Gong}]{fang2019local}
Fang, M.; Cao, X.; Jia, J.; and Gong, N.~Z. 2020.
\newblock Local model poisoning attacks to Byzantine-robust federated learning.
\newblock In \emph{USENIX Security}.

\bibitem[{Geyer, Klein, and Nabi(2017)}]{geyer2017differentially}
Geyer, R.~C.; Klein, T.; and Nabi, M. 2017.
\newblock Differentially private federated learning: A client level
  perspective.
\newblock \emph{arXiv preprint arXiv:1712.07557} .

\bibitem[{Hamer, Mohri, and Suresh(2020)}]{hamer2020fedboost}
Hamer, J.; Mohri, M.; and Suresh, A.~T. 2020.
\newblock FedBoost: Communication-Efficient Algorithms for Federated Learning.
\newblock In \emph{ICML}.

\bibitem[{Hitaj, Ateniese, and Perez-Cruz(2017)}]{hitaj2017deep}
Hitaj, B.; Ateniese, G.; and Perez-Cruz, F. 2017.
\newblock Deep models under the GAN: information leakage from collaborative
  deep learning.
\newblock In \emph{CCS}.

\bibitem[{Jia, Cao, and Gong(2020)}]{jia2020intrinsic}
Jia, J.; Cao, X.; and Gong, N.~Z. 2020.
\newblock Intrinsic certified robustness of bagging against data poisoning
  attacks.
\newblock \emph{arXiv preprint arXiv:2008.04495} .

\bibitem[{Kairouz et~al.(2019)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings et~al.}]{kairouz2019advances}
Kairouz, P.; McMahan, H.~B.; Avent, B.; Bellet, A.; Bennis, M.; Bhagoji, A.~N.;
  Bonawitz, K.; Charles, Z.; Cormode, G.; Cummings, R.; et~al. 2019.
\newblock Advances and open problems in federated learning.
\newblock \emph{arXiv preprint arXiv:1912.04977} .

\bibitem[{Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Yu,
  Richt{\'a}rik, Suresh, and Bacon}]{konevcny2016federated}
Kone{\v{c}}n{\`y}, J.; McMahan, H.~B.; Yu, F.~X.; Richt{\'a}rik, P.; Suresh,
  A.~T.; and Bacon, D. 2016.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock In \emph{NeurIPS Workshop on Private Multi-Party Machine Learning}.

\bibitem[{LeCun, Cortes, and Burges(1998)}]{lecun2010mnist}
LeCun, Y.; Cortes, C.; and Burges, C. 1998.
\newblock MNIST handwritten digit database.
\newblock \emph{Available: http://yann. lecun. com/exdb/mnist} .

\bibitem[{Lee et~al.(2017)Lee, Lam, Pedarsani, Papailiopoulos, and
  Ramchandran}]{lee2017speeding}
Lee, K.; Lam, M.; Pedarsani, R.; Papailiopoulos, D.; and Ramchandran, K. 2017.
\newblock Speeding up distributed machine learning using codes.
\newblock \emph{IEEE Transactions on Information Theory} .

\bibitem[{Li, Wen, and He(2020)}]{li2020practical}
Li, Q.; Wen, Z.; and He, B. 2020.
\newblock Practical Federated Gradient Boosting Decision Trees.
\newblock In \emph{AAAI}.

\bibitem[{Li et~al.(2020{\natexlab{a}})Li, Sanjabi, Beirami, and
  Smith}]{Li2020Fair}
Li, T.; Sanjabi, M.; Beirami, A.; and Smith, V. 2020{\natexlab{a}}.
\newblock Fair Resource Allocation in Federated Learning.
\newblock In \emph{ICLR}.

\bibitem[{Li et~al.(2020{\natexlab{b}})Li, Huang, Yang, Wang, and
  Zhang}]{li2019convergence}
Li, X.; Huang, K.; Yang, W.; Wang, S.; and Zhang, Z. 2020{\natexlab{b}}.
\newblock On the convergence of fedavg on non-iid data.
\newblock In \emph{ICLR}.

\bibitem[{Li et~al.(2020{\natexlab{c}})Li, Kovalev, Qian, and
  Richt{\'a}rik}]{li2020acceleration}
Li, Z.; Kovalev, D.; Qian, X.; and Richt{\'a}rik, P. 2020{\natexlab{c}}.
\newblock Acceleration for Compressed Gradient Descent in Distributed and
  Federated Optimization.
\newblock In \emph{ICML}.

\bibitem[{Liu et~al.(2020)Liu, Wu, Ge, Fan, and Zou}]{liu2020federated}
Liu, F.; Wu, X.; Ge, S.; Fan, W.; and Zou, Y. 2020.
\newblock Federated Learning for Vision-and-Language Grounding Problems.
\newblock In \emph{AAAI}.

\bibitem[{Malinovsky et~al.(2020)Malinovsky, Kovalev, Gasanov, Condat, and
  Richtarik}]{malinovsky2020local}
Malinovsky, G.; Kovalev, D.; Gasanov, E.; Condat, L.; and Richtarik, P. 2020.
\newblock From Local SGD to Local Fixed Point Methods for Federated Learning.
\newblock In \emph{ICML}.

\bibitem[{McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson
  et~al.}]{mcmahan2016communication}
McMahan, H.~B.; Moore, E.; Ramage, D.; Hampson, S.; et~al. 2017.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{AISTATS}.

\bibitem[{Melis et~al.(2019)Melis, Song, De~Cristofaro, and
  Shmatikov}]{melis2019exploiting}
Melis, L.; Song, C.; De~Cristofaro, E.; and Shmatikov, V. 2019.
\newblock Exploiting unintended feature leakage in collaborative learning.
\newblock In \emph{IEEE S\&P}.

\bibitem[{Mhamdi, Guerraoui, and Rouault(2018)}]{Mhamdi18}
Mhamdi, E. M.~E.; Guerraoui, R.; and Rouault, S. 2018.
\newblock The Hidden Vulnerability of Distributed Learning in Byzantium.
\newblock In \emph{ICML}.

\bibitem[{Mohri, Sivek, and Suresh(2019)}]{mohri2019agnostic}
Mohri, M.; Sivek, G.; and Suresh, A.~T. 2019.
\newblock Agnostic Federated Learning.
\newblock In \emph{ICML}.

\bibitem[{Peng et~al.(2020)Peng, Huang, Zhu, and Saenko}]{peng2019federated}
Peng, X.; Huang, Z.; Zhu, Y.; and Saenko, K. 2020.
\newblock Federated Adversarial Domain Adaptation.
\newblock In \emph{ICLR}.

\bibitem[{Rothchild et~al.(2020)Rothchild, Panda, Ullah, Ivkin, Stoica,
  Braverman, Gonzalez, and Arora}]{rothchildfetchsgd}
Rothchild, D.; Panda, A.; Ullah, E.; Ivkin, N.; Stoica, I.; Braverman, V.;
  Gonzalez, J.; and Arora, R. 2020.
\newblock FetchSGD: Communication-Efficient Federated Learning with Sketching.
\newblock In \emph{ICML}.

\bibitem[{Sahu et~al.(2018)Sahu, Li, Sanjabi, Zaheer, Talwalkar, and
  Smith}]{sahu2018convergence}
Sahu, A.~K.; Li, T.; Sanjabi, M.; Zaheer, M.; Talwalkar, A.; and Smith, V.
  2018.
\newblock On the convergence of federated optimization in heterogeneous
  networks.
\newblock \emph{arXiv preprint arXiv:1812.06127} .

\bibitem[{Smith et~al.(2017)Smith, Chiang, Sanjabi, and
  Talwalkar}]{smith2017federated}
Smith, V.; Chiang, C.-K.; Sanjabi, M.; and Talwalkar, A.~S. 2017.
\newblock Federated multi-task learning.
\newblock In \emph{NeurIPS}.

\bibitem[{Vogels, Karimireddy, and Jaggi(2019)}]{vogels2019powersgd}
Vogels, T.; Karimireddy, S.~P.; and Jaggi, M. 2019.
\newblock PowerSGD: Practical low-rank gradient compression for distributed
  optimization.
\newblock In \emph{NeurIPS}.

\bibitem[{Wang et~al.(2020)Wang, Yurochkin, Sun, Papailiopoulos, and
  Khazaeni}]{wang2020federated}
Wang, H.; Yurochkin, M.; Sun, Y.; Papailiopoulos, D.; and Khazaeni, Y. 2020.
\newblock Federated Learning with Matched Averaging.
\newblock In \emph{ICLR}.

\bibitem[{Wang, Tong, and Shi(2020)}]{wang2020federatedlatent}
Wang, Y.; Tong, Y.; and Shi, D. 2020.
\newblock Federated Latent Dirichlet Allocation: A Local Differential Privacy
  Based Framework.
\newblock In \emph{AAAI}.

\bibitem[{Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and
  Li}]{wen2017terngrad}
Wen, W.; Xu, C.; Yan, F.; Wu, C.; Wang, Y.; Chen, Y.; and Li, H. 2017.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In \emph{NeurIPS}.

\bibitem[{Xie et~al.(2020)Xie, Huang, Chen, and Li}]{xie2019dba}
Xie, C.; Huang, K.; Chen, P.-Y.; and Li, B. 2020.
\newblock DBA: Distributed Backdoor Attacks against Federated Learning.
\newblock In \emph{ICLR}.

\bibitem[{Xie, Koyejo, and Gupta(2019)}]{Xie19}
Xie, C.; Koyejo, S.; and Gupta, I. 2019.
\newblock Fall of empires: Breaking byzantine-tolerant SGD by inner product
  manipulation.
\newblock In \emph{UAI}.

\bibitem[{Yin et~al.(2019)Yin, Chen, Kannan, and Bartlett}]{yin2019defending}
Yin, D.; Chen, Y.; Kannan, R.; and Bartlett, P. 2019.
\newblock Defending Against Saddle Point Attack in Byzantine-Robust Distributed
  Learning.
\newblock In \emph{ICML}.

\bibitem[{Yin et~al.(2018)Yin, Chen, Ramchandran, and Bartlett}]{Yin18}
Yin, D.; Chen, Y.; Ramchandran, K.; and Bartlett, P. 2018.
\newblock Byzantine-Robust Distributed Learning: Towards Optimal Statistical
  Rates.
\newblock In \emph{ICML}.

\bibitem[{Yurochkin et~al.(2019)Yurochkin, Agarwal, Ghosh, Greenewald, Hoang,
  and Khazaeni}]{yurochkin2019bayesian}
Yurochkin, M.; Agarwal, M.; Ghosh, S.; Greenewald, K.; Hoang, N.; and Khazaeni,
  Y. 2019.
\newblock Bayesian Nonparametric Federated Learning of Neural Networks.
\newblock In \emph{ICML}.

\bibitem[{Zhu, Liu, and Han(2019)}]{zhu2019deep}
Zhu, L.; Liu, Z.; and Han, S. 2019.
\newblock Deep leakage from gradients.
\newblock In \emph{NeurIPS}.

\end{thebibliography}
