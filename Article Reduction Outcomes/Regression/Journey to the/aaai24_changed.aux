\relax 
\bibstyle{aaai24}
\citation{mbert,gpt2,mgpt,openai2023gpt4,touvron2023llama}
\citation{knowledge-base,knowledge_know_fact}
\citation{kandpal2023large}
\citation{zhen2022survey,zhao2023survey}
\citation{localization,kl}
\citation{dai2022kn}
\citation{ancona2019gradient}
\citation{cao2023life}
\citation{mbert}
\citation{mgpt}
\citation{meng2022locating}
\citation{causal-inspired}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig-intro1}{{1a}{1}{}{}{}}
\newlabel{sub@fig-intro1}{{(a)}{a}}
\newlabel{fig:intro2}{{1b}{1}{}{}{}}
\newlabel{sub@fig:intro2}{{(b)}{b}}
\newlabel{fig-intro}{{1}{1}{Explanation of Language-Independent Knowledge Neurons (LIKN) and Degenerate Knowledge Neurons (DKN). KN denotes knowledge neurons.}{}{}}
\citation{rigorousIG}
\citation{degenerate_biological,mason2015degeneracy}
\citation{fill-in-the-blank}
\citation{fill-in-the-blank}
\citation{fill-in-the-blank}
\citation{ig}
\newlabel{eq:1}{{1}{2}{}{}{}}
\citation{transparent}
\citation{enguehard2023sequential}
\citation{enguehard2023sequential}
\citation{m-language-edit,wang-etal-2020-negative}
\newlabel{fig:overall}{{2}{3}{Overall Algorithm Flow, describing (1) our architecture-adapted multilingual integrated gradients (AMIG) method for locating knowledge neurons (KN), (2) the process of detecting language-independent knowledge neurons (LIKN), and (3) the process of detecting degenerate knowledge neurons (DKN).}{}{}}
\newlabel{eqution:attribute}{{2}{3}{}{}{}}
\citation{degenerate_biological,mason2015degeneracy}
\citation{mbert}
\citation{mgpt}
\citation{mlama}
\citation{fill-in-the-blank,lama2}
\newlabel{equation-1}{{9}{4}{}{}{}}
\newlabel{equation-2}{{10}{4}{}{}{}}
\newlabel{alg:degenerate}{{1}{4}{Identification of Degenerate Knowledge Neurons ($\mathcal  {D}$)}{}{}}
\citation{dai2022kn}
\citation{dai2022kn}
\newlabel{fig3}{{3}{5}{The distributions of knowledge neurons in m-BERT and m-GPT models under two languages (English-KN and Chinese-KN) and language-independent knowledge neurons (LIKN).}{}{}}
\newlabel{table1}{{1}{5}{Results of the localization of knowledge neurons. B-KN is the baseline method, the symbol ``\textuparrow " indicates the increase in success rate compared to B-KN for our method, which can be expressed as: $\frac  {{\text  {AMIG}-\text  {B-KN}}}{{\text  {B-KN}}}$, and bold indicates the method with a higher $SR$.}{}{}}
\citation{hallucination_chatgpt1,hallucination_chatgpt2}
\citation{fact-checking-survey}
\newlabel{table:cross-lingual-edit}{{2}{6}{Results of cross-lingual knowledge editing. LIKN represents editing language-independent knowledge neurons, Mono-KN denotes editing knowledge neurons in one language's dataset corresponding to another, and Seq-KN denotes sequentially editing knowledge neurons in two languages. The symbol `\textuparrow ` shows a success rate increase in LIKN over Mono-KN, represented as $\frac  {\text  {LIKN} - \text  {Mono-KN}}{\text  {Mono-KN}}$, and `\textdownarrow ` indicates a decrease in LIKN compared to Seq-KN, represented as $\frac  {\text  {LIKN} - \text  {Seq-KN}}{\text  {LIKN}}$.}{}{}}
\newlabel{fig:d-mbert}{{4a}{6}{}{}{}}
\newlabel{sub@fig:d-mbert}{{(a)}{a}}
\newlabel{fig:d-mgpt}{{4b}{6}{}{}{}}
\newlabel{sub@fig:d-mgpt}{{(b)}{b}}
\newlabel{fig:d}{{4}{6}{The distributions of degenerate knowledge neurons (DKN) in multilingual PLMs under two languages.}{}{}}
\newlabel{fig:mono-D-neurons_distribution}{{5}{6}{The distributions of degenerate knowledge neurons (DKN) in monolingual PLMs under two languages.}{}{}}
\citation{hallucination_chatgpt3,lakshmanan2022large,metz2022new}
\citation{zhou2023comprehensive}
\citation{dai2022kn}
\citation{dai2022kn}
\citation{key_value}
\citation{meng2022locating}
\citation{meng2022locating}
\citation{meng2022locating}
\citation{meng2022memit}
\citation{mend}
\citation{ig}
\citation{ig}
\citation{DIG}
\citation{enguehard2023sequential}
\citation{transparent}
\citation{transparent}
\newlabel{table:combined-experiments}{{3}{7}{Fact-checking experiment results comparing methods with (with\_DKN) and without (wo\_DKN) degenerate knowledge neurons. The symbol ``\textuparrow " shows F1-score improvement in with\_DKN over wo\_DKN as $\frac  {\text  {with\_DKN} - \text  {wo\_DKN}}{\text  {wo\_DKN}}$, with bold indicating the higher score.}{}{}}
\bibdata{aaai24.bib}
\gdef \@abspage@last{8}
