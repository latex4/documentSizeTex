\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Abbeel and Ng(2004)}]{abbeel2004apprenticeship}
Abbeel, P.; and Ng, A.~Y. 2004.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, 1.

\bibitem[{Arenz and Neumann(2020)}]{arenz2020non}
Arenz, O.; and Neumann, G. 2020.
\newblock Non-Adversarial Imitation Learning and its Connections to Adversarial
  Methods.
\newblock \emph{arXiv preprint arXiv:2008.03525}.

\bibitem[{Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba}]{brockman2016openai}
Brockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.; Schulman, J.; Tang,
  J.; and Zaremba, W. 2016.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}.

\bibitem[{Dadashi et~al.(2020)Dadashi, Hussenot, Geist, and
  Pietquin}]{dadashi2020primal}
Dadashi, R.; Hussenot, L.; Geist, M.; and Pietquin, O. 2020.
\newblock Primal wasserstein imitation learning.
\newblock \emph{arXiv preprint arXiv:2006.04678}.

\bibitem[{Fujimoto, van Hoof, and Meger(2018)}]{fujimoto2018addressing}
Fujimoto, S.; van Hoof, H.; and Meger, D. 2018.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, 1587--1596.
  PMLR.

\bibitem[{Ghasemipour, Zemel, and Gu(2020)}]{ghasemipour2020divergence}
Ghasemipour, S. K.~S.; Zemel, R.; and Gu, S. 2020.
\newblock A divergence minimization perspective on imitation learning methods.
\newblock In \emph{Conference on Robot Learning}, 1259--1277. PMLR.

\bibitem[{Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio}]{goodfellow2014generative}
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair,
  S.; Courville, A.; and Bengio, Y. 2014.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27:
  2672--2680.

\bibitem[{Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville}]{gulrajani2017improved}
Gulrajani, I.; Ahmed, F.; Arjovsky, M.; Dumoulin, V.; and Courville, A. 2017.
\newblock Improved training of wasserstein gans.
\newblock \emph{arXiv preprint arXiv:1704.00028}.

\bibitem[{Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine}]{haarnoja2018soft}
Haarnoja, T.; Zhou, A.; Abbeel, P.; and Levine, S. 2018.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, 1861--1870.
  PMLR.

\bibitem[{Ho and Ermon(2016)}]{ho2016generative}
Ho, J.; and Ermon, S. 2016.
\newblock Generative adversarial imitation learning.
\newblock In \emph{Advances in neural information processing systems},
  4565--4573.

\bibitem[{Ke et~al.(2019)Ke, Barnes, Sun, Lee, Choudhury, and
  Srinivasa}]{ke2019imitation}
Ke, L.; Barnes, M.; Sun, W.; Lee, G.; Choudhury, S.; and Srinivasa, S. 2019.
\newblock Imitation Learning as $ f $-Divergence Minimization.
\newblock \emph{arXiv preprint arXiv:1905.12888}.

\bibitem[{Kingma and Ba(2014)}]{kingma2014adam}
Kingma, D.~P.; and Ba, J. 2014.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}.

\bibitem[{Kostrikov et~al.(2018)Kostrikov, Agrawal, Dwibedi, Levine, and
  Tompson}]{kostrikov2018discriminator}
Kostrikov, I.; Agrawal, K.~K.; Dwibedi, D.; Levine, S.; and Tompson, J. 2018.
\newblock Discriminator-actor-critic: Addressing sample inefficiency and reward
  bias in adversarial imitation learning.
\newblock \emph{arXiv preprint arXiv:1809.02925}.

\bibitem[{Kostrikov, Nachum, and Tompson(2019)}]{kostrikov2019imitation}
Kostrikov, I.; Nachum, O.; and Tompson, J. 2019.
\newblock Imitation learning via off-policy distribution matching.
\newblock \emph{arXiv preprint arXiv:1912.05032}.

\bibitem[{Levin and Peres(2017)}]{levin2017markov}
Levin, D.~A.; and Peres, Y. 2017.
\newblock \emph{Markov chains and mixing times}, volume 107.
\newblock American Mathematical Soc.

\bibitem[{Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra}]{lillicrap2015continuous}
Lillicrap, T.~P.; Hunt, J.~J.; Pritzel, A.; Heess, N.; Erez, T.; Tassa, Y.;
  Silver, D.; and Wierstra, D. 2015.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}.

\bibitem[{Liu et~al.(2018)Liu, Li, Tang, and Zhou}]{liu2018breaking}
Liu, Q.; Li, L.; Tang, Z.; and Zhou, D. 2018.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock \emph{arXiv preprint arXiv:1810.12429}.

\bibitem[{Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski et~al.}]{mnih2015human}
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A.~A.; Veness, J.; Bellemare,
  M.~G.; Graves, A.; Riedmiller, M.; Fidjeland, A.~K.; Ostrovski, G.; et~al.
  2015.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518(7540): 529--533.

\bibitem[{Nguyen, Wainwright, and Jordan(2010)}]{nguyen2010estimating}
Nguyen, X.; Wainwright, M.~J.; and Jordan, M.~I. 2010.
\newblock Estimating divergence functionals and the likelihood ratio by convex
  risk minimization.
\newblock \emph{IEEE Transactions on Information Theory}, 56(11): 5847--5861.

\bibitem[{Puterman(2014)}]{puterman2014markov}
Puterman, M.~L. 2014.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons.

\bibitem[{Reddy, Dragan, and Levine(2019)}]{reddy2019sqil}
Reddy, S.; Dragan, A.~D.; and Levine, S. 2019.
\newblock SQIL: Imitation learning via reinforcement learning with sparse
  rewards.
\newblock \emph{arXiv preprint arXiv:1905.11108}.

\bibitem[{Robbins and Monro(1951)}]{robbins1951stochastic}
Robbins, H.; and Monro, S. 1951.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, 400--407.

\bibitem[{Sasaki, Yohira, and Kawaguchi(2018)}]{sasaki2018sample}
Sasaki, F.; Yohira, T.; and Kawaguchi, A. 2018.
\newblock Sample efficient imitation learning for continuous control.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Schulman et~al.(2015)Schulman, Moritz, Levine, Jordan, and
  Abbeel}]{schulman2015high}
Schulman, J.; Moritz, P.; Levine, S.; Jordan, M.; and Abbeel, P. 2015.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{arXiv preprint arXiv:1506.02438}.

\bibitem[{Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov}]{schulman2017proximal}
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}.

\bibitem[{Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller}]{silver2014deterministic}
Silver, D.; Lever, G.; Heess, N.; Degris, T.; Wierstra, D.; and Riedmiller, M.
  2014.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{International conference on machine learning}, 387--395.
  PMLR.

\bibitem[{Sun et~al.(2021)Sun, Mahajan, Hofmann, and
  Whiteson}]{sun2021softdice}
Sun, M.; Mahajan, A.; Hofmann, K.; and Whiteson, S. 2021.
\newblock SoftDICE for Imitation Learning: Rethinking Off-policy Distribution
  Matching.
\newblock \emph{arXiv preprint arXiv:2106.03155}.

\bibitem[{Sutton and Barto(2018)}]{sutton2018reinforcement}
Sutton, R.~S.; and Barto, A.~G. 2018.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[{Syed and Schapire(2007)}]{syed2007game}
Syed, U.; and Schapire, R.~E. 2007.
\newblock A game-theoretic approach to apprenticeship learning.
\newblock \emph{Advances in neural information processing systems}, 20:
  1449--1456.

\bibitem[{Wang et~al.(2019)Wang, Ciliberto, Amadori, and
  Demiris}]{wang2019random}
Wang, R.; Ciliberto, C.; Amadori, P.; and Demiris, Y. 2019.
\newblock Random expert distillation: Imitation learning via expert policy
  support estimation.
\newblock \emph{arXiv preprint arXiv:1905.06750}.

\bibitem[{Yu(2015)}]{yu2015convergence}
Yu, H. 2015.
\newblock On convergence of emphatic temporal-difference learning.
\newblock In \emph{Conference on Learning Theory}, 1724--1751. PMLR.

\end{thebibliography}
