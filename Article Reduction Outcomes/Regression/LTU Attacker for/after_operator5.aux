\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{nasr2019comprehensive}
\citation{shokri2017membership}
\citation{li2013membership}
\citation{shokri2017membership}
\citation{long2017towards}
\citation{song2021systematic}
\citation{jayaraman2020revisiting}
\@writefile{toc}{\contentsline {section}{Introduction}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Related work}{1}{section*.2}\protected@file@percent }
\citation{yeom2018privacy}
\citation{truex2019demystifying}
\citation{hayes2018logan}
\citation{hilprecht2019reconstruction}
\citation{chen2020gan}
\citation{dwork2006calibrating}
\citation{abadi2016deep}
\citation{xie2018differentially}
\citation{ganin2016domain}
\citation{nasr2018machine}
\citation{huang2021damia}
\citation{nasr2019comprehensive}
\citation{sablayrolles2019white}
\citation{liu2020mace}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces {\bf  Methodology Flow Chart. (a) Defender:} Source data are divided into {\color  {red} Defender data}, to train the model under attack ({\color  {red} Defender model}) and {\color  {blue} Reserved data} to evaluate such model. The {\color  {red} Defender model trainer} creates a model optimizing a utility objective, while being as resilient as possible to attacks. {\bf  (b) LTU Attacker:} The evaluation apparatus includes an {\color  {orange} LTU Attacker} and an {\color  {teal} Evaluator}: The evaluation apparatus performs a hold-out evaluation leaving two unlabeled examples (LTU) by repeatedly providing the {\color  {orange} LTU Attacker} with ALL of the {\color  {red} Defender} and {\color  {blue} Reserved} data samples, together with their {\em  membership origin}, hiding only the membership label of 2 samples. The {\color  {orange} LTU Attacker} must turn in the membership label (Defender data or Reserved data) of these 2 samples (Attack predictions). {\bf  (c) Evaluator:} The {\color  {teal} Evaluator} computes two scores: {\color  {orange} LTU Attacker} prediction error ({\color  {teal} Privacy metric}), and {\color  {red} Defender model} classification performance ({\color  {teal} Utility metric}).}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:teaser}{{1}{2}{{\bf Methodology Flow Chart. (a) Defender:} Source data are divided into {\color {red} Defender data}, to train the model under attack ({\color {red} Defender model}) and {\color {blue} Reserved data} to evaluate such model. The {\color {red} Defender model trainer} creates a model optimizing a utility objective, while being as resilient as possible to attacks. {\bf (b) LTU Attacker:} The evaluation apparatus includes an {\color {orange} LTU Attacker} and an {\color {teal} Evaluator}: The evaluation apparatus performs a hold-out evaluation leaving two unlabeled examples (LTU) by repeatedly providing the {\color {orange} LTU Attacker} with ALL of the {\color {red} Defender} and {\color {blue} Reserved} data samples, together with their {\em membership origin}, hiding only the membership label of 2 samples. The {\color {orange} LTU Attacker} must turn in the membership label (Defender data or Reserved data) of these 2 samples (Attack predictions). {\bf (c) Evaluator:} The {\color {teal} Evaluator} computes two scores: {\color {orange} LTU Attacker} prediction error ({\color {teal} Privacy metric}), and {\color {red} Defender model} classification performance ({\color {teal} Utility metric})}{figure.1}{}}
\citation{guyon-1998}
\citation{yeom2018privacy}
\citation{Dwork2017}
\@writefile{toc}{\contentsline {section}{Problem statement and methodology}{3}{section*.3}\protected@file@percent }
\newlabel{equation:privacy}{{1}{3}{Problem statement and methodology}{equation.0.1}{}}
\newlabel{equation:utility}{{2}{3}{Problem statement and methodology}{equation.0.2}{}}
\@writefile{toc}{\contentsline {section}{Theoretical analysis of na\"ive attackers}{3}{section*.4}\protected@file@percent }
\citation{yeom2018privacy}
\citation{yeom2018privacy}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  {\bf  Taxonomy of LTU Attacker\nobreakspace  {}.} {\bf  Top:} Any LTU Attacker\nobreakspace  {}has available the Defender trainer $\mathcal  {T}_D$, the trained Defender model $\mathcal  {M}_D$, and attack data $\mathcal  {D}_A$ including (almost) all the Defender data $\mathcal  {D}_D$ and Reserved data $\mathcal  {D}_R$ $\mathcal  {D}_A=\mathcal  {D}_D-\{\texttt  {membership}(d)\} \cup \mathcal  {D}_R-\{\texttt  {membership}(r)\}$. But it may use only part of this available knowledge to conduct attacks. $r$ and $d$ are two labeled examples belonging $\mathcal  {D}_R$ and $\mathcal  {D}_D$ respectively, and $u_1$ and $u_2$ are two unlabeled examples, one from $\mathcal  {D}_R$ and one from $\mathcal  {D}_D$ (ordered randomly). {\bf  Left:} Attacker $\mathcal  {M}_A$ targets only the trained Defender model $\mathcal  {M}_D$. {\bf  Right:} $\mathcal  {M}_A$ targets both $\mathcal  {M}_D$ and its trainer $\mathcal  {T}_D$.}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:taxonomy}{{2}{4}{{\bf Taxonomy of \oracle .} {\bf Top:} Any \oracle has available the Defender trainer $\mathcal {T}_D$, the trained Defender model $\mathcal {M}_D$, and attack data $\mathcal {D}_A$ including (almost) all the Defender data $\mathcal {D}_D$ and Reserved data $\mathcal {D}_R$ $\mathcal {D}_A=\mathcal {D}_D\sminus \{\texttt {membership}(d)\} \cup \mathcal {D}_R\sminus \{\texttt {membership}(r)\}$. But it may use only part of this available knowledge to conduct attacks. $r$ and $d$ are two labeled examples belonging $\mathcal {D}_R$ and $\mathcal {D}_D$ respectively, and $u_1$ and $u_2$ are two unlabeled examples, one from $\mathcal {D}_R$ and one from $\mathcal {D}_D$ (ordered randomly). {\bf Left:} Attacker $\mathcal {M}_A$ targets only the trained Defender model $\mathcal {M}_D$. {\bf Right:} $\mathcal {M}_A$ targets both $\mathcal {M}_D$ and its trainer $\mathcal {T}_D$}{figure.2}{}}
\newlabel{thm:pairwise}{{1}{4}{}{theorem.1}{}}
\newlabel{thm:blf}{{2}{4}{}{theorem.2}{}}
\citation{krizhevsky2009learning}
\citation{qmnist-2019}
\citation{rahman2018membership}
\citation{hilprecht2019reconstruction}
\citation{shokri2017membership}
\citation{qmnist-2019}
\citation{lecun1998gradient}
\citation{simonyan2014very}
\citation{deng2009imagenet}
\citation{tan2021efficientnetv2}
\newlabel{thm:deterministic}{{3}{5}{}{theorem.3}{}}
\@writefile{toc}{\contentsline {section}{Data and experimental setting}{5}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Results}{5}{section*.6}\protected@file@percent }
\newlabel{sec:results}{{}{5}{Results}{section*.6}{}}
\@writefile{toc}{\contentsline {subsection}{Black-box attacker}{5}{section*.7}\protected@file@percent }
\citation{deepdaJindong}
\citation{he2016deep}
\citation{deng2009imagenet}
\citation{sun2021omniprint}
\citation{zhuDeepSubdomainAdaptation2021}
\citation{deepdaJindong}
\citation{zhuDeepSubdomainAdaptation2021}
\@writefile{toc}{\contentsline {subsection}{White-box attacker}{6}{section*.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  \relax \fontsize  {9}{10}\selectfont  {\bf  Utility and Privacy on QMNIST and CIFAR-10 of different scikit-learn models} with three levels of randomness: Original sample order + Fixed random seed (no randomness); Random sample order + Fixed random seed; Random sample order + Random seed. The Defender data and Reserved data have both 1600 examples. All numbers shown in the table have {\em  at least} two significant digits (standard error lower than 0.004). For model implementations, we use scikit-learn (version 0.24.2) with default values. Results with Utility or Privacy $>0.90$ are highlighted and those meeting both criteria are underlined. Shaded in gray: fully deterministic models with Privacy$\equiv 0$.}}{6}{table.1}\protected@file@percent }
\newlabel{tab:bbox_resu}{{1}{6}{\footnotesize {\bf Utility and Privacy on QMNIST and CIFAR-10 of different scikit-learn models} with three levels of randomness: Original sample order + Fixed random seed (no randomness); Random sample order + Fixed random seed; Random sample order + Random seed. The Defender data and Reserved data have both 1600 examples. All numbers shown in the table have {\em at least} two significant digits (standard error lower than 0.004). For model implementations, we use scikit-learn (version 0.24.2) with default values. Results with Utility or Privacy $>0.90$ are highlighted and those meeting both criteria are underlined. Shaded in gray: fully deterministic models with Privacy$\equiv 0$}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  \relax \fontsize  {9}{10}\selectfont  \bf  Utility and Privacy of DNN ResNet50 Defender models trained on QMNIST.}}{6}{table.2}\protected@file@percent }
\newlabel{tab:DA}{{2}{6}{\footnotesize \bf Utility and Privacy of DNN ResNet50 Defender models trained on QMNIST}{table.2}{}}
\citation{nasr2019comprehensive}
\citation{nasr2019comprehensive}
\citation{song2021systematic}
\bibstyle{aaai}
\@writefile{toc}{\contentsline {section}{Discussion and further work}{7}{section*.9}\protected@file@percent }
\newlabel{sec:discussion}{{}{7}{Discussion and further work}{section*.9}{}}
\@writefile{toc}{\contentsline {section}{Conclusion}{7}{section*.10}\protected@file@percent }
\bibdata{bibliography}
\gdef \@abspage@last{7}
