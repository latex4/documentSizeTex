\relax 
\bibstyle{aaai22}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{griffiths2007mere}
\citation{gopnik2001causal}
\citation{spirtes2000causation,pearl2000models,janzing2012information}
\citation{griffiths2009theory,ortega2015subjectivity}
\citation{genewein2020algorithms,shafer1996art}
\citation{gopnik2001causal}
\@writefile{toc}{\contentsline {section}{Introduction}{1}{section*.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Either $X$ is the cause of $Y$ or $Y$ is the cause of $X$, and we distinguish between the Agents hypothesis $H^a$ about the causal orientation and the actual orientation $H^0$. Our overall goal is to quantify how much information can be gained about $H^0$ on average from a single intervention.}}{1}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig1svg}{{1}{1}{Either $X$ is the cause of $Y$ or $Y$ is the cause of $X$, and we distinguish between the Agents hypothesis $H^a$ about the causal orientation and the actual orientation $H^0$. Our overall goal is to quantify how much information can be gained about $H^0$ on average from a single intervention}{figure.caption.2}{}}
\newlabel{fig1svg@cref}{{[figure][1][]1}{[1][1][]1}}
\citation{wieczorek2019information}
\citation{compton2021entropic}
\citation{janzing2012information}
\citation{agrawal2019abcd,tong2001active}
\citation{ortega2015subjectivity}
\citation{genewein2020algorithms}
\citation{ortega2015subjectivity}
\citation{compton2021entropic}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A probability tree for the problem involving $X$ and $Y$. The variable $H = h$ or $H = \neg h$ determines the causal orientation of $X$ and $Y$. Thus, given $H$, the effect of interventions is computed from the correct branch. }}{2}{figure.caption.5}\protected@file@percent }
\newlabel{fig2svg}{{2}{2}{A probability tree for the problem involving $X$ and $Y$. The variable $H = h$ or $H = \neg h$ determines the causal orientation of $X$ and $Y$. Thus, given $H$, the effect of interventions is computed from the correct branch}{figure.caption.5}{}}
\newlabel{fig2svg@cref}{{[figure][2][]2}{[1][2][]2}}
\@writefile{toc}{\contentsline {paragraph}{Related work}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Methods}{2}{section*.4}\protected@file@percent }
\newlabel{eq:1}{{1}{2}{Methods}{equation.0.1}{}}
\newlabel{eq:1@cref}{{[equation][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsection}{Bayesian learning}{2}{section*.6}\protected@file@percent }
\newlabel{eq:6}{{4}{2}{Bayesian learning}{equation.0.4}{}}
\newlabel{eq:6@cref}{{[subequation][4][]4}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsection}{Information gain}{2}{section*.8}\protected@file@percent }
\newlabel{eq:6}{{5}{2}{Information gain}{equation.0.5}{}}
\newlabel{eq:6@cref}{{[equation][5][]5}{[1][2][]2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The plots show both the expected information gain (triangles) and the realized gain (circles) for Example 1 (left), Example 2 (middle) and Example 3 (right). The expected gain, \cref  {eq:9}, signifies the agent's estimate of the information gain from a given intervention $\hat  x$ or $\hat  y$, i.e. the agent will decide based on this value (by symmetry, all possible interventions are represented in the figures). The realized gain (computed using \cref  {eq:12}), is the gain the agent will actually obtain. Each simulation is run 1000 times, using a varying number of observations $N$, and the shaded region indicates one standard deviation. }}{3}{figure.caption.7}\protected@file@percent }
\newlabel{fig3}{{3}{3}{The plots show both the expected information gain (triangles) and the realized gain (circles) for Example 1 (left), Example 2 (middle) and Example 3 (right). The expected gain, \cref {eq:9}, signifies the agent's estimate of the information gain from a given intervention $\hat x$ or $\hat y$, i.e. the agent will decide based on this value (by symmetry, all possible interventions are represented in the figures). The realized gain (computed using \cref {eq:12}), is the gain the agent will actually obtain. Each simulation is run 1000 times, using a varying number of observations $N$, and the shaded region indicates one standard deviation}{figure.caption.7}{}}
\newlabel{fig3@cref}{{[figure][3][]3}{[1][2][]3}}
\newlabel{eq:12}{{6}{3}{Information gain}{equation.0.6}{}}
\newlabel{eq:12@cref}{{[equation][6][]6}{[1][2][]3}}
\newlabel{eq:9}{{7}{3}{Information gain}{equation.0.7}{}}
\newlabel{eq:9@cref}{{[equation][7][]7}{[1][3][]3}}
\@writefile{toc}{\contentsline {subparagraph}{Example 1: Two correlated variables}{3}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example 2: Exploration}{3}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example 3: A single good intervention}{3}{section*.12}\protected@file@percent }
\citation{genewein2020algorithms}
\bibdata{library2}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The problem described in Example 3, shown in \cref  {fig3} (right), but where we consider the chance of selecting the single best intervention ($\hat  x=1$) as a function of prior $\alpha $ and for different number of observations. The prior is necessary to obtain stable estimates of the expected information gain, however, it will impact the estimate of the expected information gain slightly differently and therefore a large prior may change the ordering. The method selects the best intervention with a much higher probability than chance even for very low counts. }}{4}{figure.caption.10}\protected@file@percent }
\newlabel{fig4}{{4}{4}{The problem described in Example 3, shown in \cref {fig3} (right), but where we consider the chance of selecting the single best intervention ($\hat x=1$) as a function of prior $\alpha $ and for different number of observations. The prior is necessary to obtain stable estimates of the expected information gain, however, it will impact the estimate of the expected information gain slightly differently and therefore a large prior may change the ordering. The method selects the best intervention with a much higher probability than chance even for very low counts}{figure.caption.10}{}}
\newlabel{fig4@cref}{{[figure][4][]4}{[1][3][]4}}
\@writefile{toc}{\contentsline {paragraph}{Example 4: Active learning}{4}{section*.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Evaluations of method for an actual intervention-selection problem. The truth is considered fixed as $H^0 = h^0_{X \rightarrow Y}$. From this, $10^5$ random joint distributions $P(X,Y)$ are generated (see text), and the information gains towards $h^0_{X \rightarrow Y}$ are computed when interventions are either selected randomly, or when using the maximum anticipated gain \cref  {eq:9}. As shown, the information gain is in both cases positive, but about twice as large when interventions are selected using our method. }}{4}{figure.caption.14}\protected@file@percent }
\newlabel{fig5}{{5}{4}{Evaluations of method for an actual intervention-selection problem. The truth is considered fixed as $H^0 = h^0_{X \rightarrow Y}$. From this, $10^5$ random joint distributions $P(X,Y)$ are generated (see text), and the information gains towards $h^0_{X \rightarrow Y}$ are computed when interventions are either selected randomly, or when using the maximum anticipated gain \cref {eq:9}. As shown, the information gain is in both cases positive, but about twice as large when interventions are selected using our method}{figure.caption.14}{}}
\newlabel{fig5@cref}{{[figure][5][]5}{[1][4][]4}}
\@writefile{toc}{\contentsline {section}{Discussion and conclusion}{4}{section*.15}\protected@file@percent }
\gdef \@abspage@last{5}
