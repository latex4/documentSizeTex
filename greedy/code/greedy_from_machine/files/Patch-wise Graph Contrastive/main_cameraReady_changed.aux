\relax 
\bibstyle{aaai24}
\citation{pix2pix}
\citation{cyclegan}
\citation{gcgan,distancegan}
\citation{cut}
\citation{negcut}
\citation{sesim}
\citation{HnegSRC}
\citation{hkd}
\citation{deepSpectral,tokenCut}
\citation{visionGNN}
\citation{hkd}
\newlabel{sec:intro}{{}{1}{}{}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:concept}{{1}{1}{ The semantic connectivity of input is extracted by the encoder, and shared to construct the graph network. We maximize the mutual information between the nodes. }{}{}}
\citation{cut,negcut}
\citation{sesim}
\citation{mcl}
\citation{qsAttn}
\citation{moNCE}
\citation{HnegSRC}
\citation{splicing,text2live}
\citation{HnegSRC}
\citation{splicing,text2live}
\citation{gcn,tagcn}
\citation{lagraph,sep,structPool}
\citation{superGlue}
\citation{semantic2graph}
\citation{hkd,gkd}
\citation{deepSpectral,tokenCut}
\citation{visionGNN}
\newlabel{sec:related_works}{{}{2}{}{}{}}
\newlabel{sec:graph}{{}{2}{}{}{}}
\citation{tagcn}
\citation{cpc}
\citation{graphUnet}
\newlabel{fig:method}{{2}{3}{(a) Overall framework of the proposed method. We impose patch-wise regularization by the GNN constructed by the encoder $E$. We extract the node feature $Z, V$ and maximize $I(Z; V)$. Pooled graphs are utilized to focus on task-relevant nodes. (b) The motivation of the proposed approach to use patch-wise connection of input image as the prior knowledge. }{}{}}
\newlabel{fig:graph}{{3}{3}{The construction of graphs $g_o, g_i$ with shared adjacency matrix $A$. Each graph extracts $l$-hop features $Z, V$ from the given node $F_i, F_o$.}{}{}}
\newlabel{eq:infoNCE}{{4}{3}{}{}{}}
\newlabel{sec:pooling}{{}{3}{}{}{}}
\citation{graphUnet}
\citation{graphUnet}
\citation{cbam,bam}
\citation{HnegSRC,cut,negcut,sesim}
\citation{lsgan}
\citation{cut}
\citation{cut}
\citation{vgg}
\citation{cyclegan}
\citation{munit}
\citation{distancegan}
\citation{gcgan}
\citation{cut}
\citation{negcut}
\citation{sesim}
\citation{HnegSRC}
\newlabel{fig:pooling}{{4}{4}{The top-$K$ graph pooling\nobreakspace  {}\citep  {graphUnet}. The pooling vector $p$ provides the focused view of the graph for the given task. The final node feature is also weighted by $p$.}{}{}}
\newlabel{eq:overall}{{10}{4}{}{}{}}
\newlabel{fig:pool-attn}{{5}{4}{Top-$K$ graph pooling allocates higher weights to the informative nodes, similarly to the attention mechanism. (a) Top-$K$ graph pooling. (b) Attention method.}{}{}}
\citation{cyclegan}
\citation{munit}
\citation{distancegan}
\citation{gcgan}
\citation{cut}
\citation{negcut}
\citation{sesim}
\citation{HnegSRC}
\citation{cut}
\citation{cut}
\citation{cut}
\citation{strotss}
\citation{wct2}
\citation{cut}
\citation{sesim}
\citation{HnegSRC}
\newlabel{fig:result1}{{6}{5}{Qualitative comparison with related methods. Our result shows enhanced input-output correspondence, compared to the previous methods.}{}{}}
\newlabel{fig:spatial_specific}{{7}{5}{Closer views of the output images. Our method enhances the spatial-specific information given in the input.}{}{}}
\citation{deepSpectral}
\newlabel{table:main}{{1}{6}{Quantitative results. Our model outperforms the baselines in both of FID and KID$\times $100 metrics. }{}{}}
\newlabel{fig:single}{{8}{6}{Qualitative comparison on single image translation.}{}{}}
\newlabel{fig:analy}{{9}{7}{Analysis of the proposed method: (a) Input and the output images. (b) Visualization of $\sigma (S_{in}), \sigma (S_{out})$. The vector $p$ allocates higher weights for the object parts which are task-relevant. Similar appearance refers the correspondence between input and output. (c) Eigenvectors of the Laplacian matrix of $A$, which are coherent to the semantics of the image.}{}{}}
\newlabel{fig:adj_FC}{{10}{7}{The adjacency matrix $A$ is constructed from $F_i$ which is the output of learnable $h$. Here, $h$ is updated by the gradient from the $F_o$ similar to CUT\nobreakspace  {}\citep  {cut}. }{}{}}
\newlabel{table:ablation}{{2}{7}{Quantitative results of ablation studies. Our setting shows the best performance in both of FID and KID$\times $100.}{}{}}
\bibdata{aaai24}
\gdef \@abspage@last{8}
