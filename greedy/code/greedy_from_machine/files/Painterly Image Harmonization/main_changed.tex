%File: formatting-instructions-latex-2024.tex
%release 2024.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
%  -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
%  -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Painterly Image Harmonization by Learning from Painterly Objects}
\author{
    Li Niu\thanks{Corresponding author.},
    Junyan Cao,
    Yan Hong,
    Liqing Zhang
    \\
}
\affiliations{
    %Afiliations
    MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar, \textsuperscript{\rm 2}
    % J. Scott Penberthy, \textsuperscript{\rm 3}
    % George Ferguson,\textsuperscript{\rm 4}
    % Hans Guesgen, \textsuperscript{\rm 5}.
    % Note that the comma should be placed BEFORE the superscript for optimum readability

    % email address must be in roman text type, not monospace or sans serif
    \{ustcnewly, joy\_c1, hy2628982280, lqzhang\}@sjtu.edu.cn
%
% See more examples next
}
%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Given a composite image with photographic object and painterly background, painterly image harmonization targets at stylizing the composite object to be compatible with the background. Despite the competitive performance of existing painterly harmonization works, they did not fully leverage the painterly objects in artistic paintings. In this work, we explore learning from painterly objects for painterly image harmonization. In particular, we learn a mapping from background style and object information to object style based on painterly objects in artistic paintings. With the learnt mapping, we can hallucinate the target style of composite object, which is used to harmonize encoder feature maps to produce the harmonized image. Extensive experiments on the benchmark dataset demonstrate the effectiveness of our proposed method. Dataset and code are available at \url{https://github.com/bcmi/ArtoPIH-Painterly-Image-Harmonization.}
\end{abstract}

\section{Introduction}


Compositing multiple regions from different images is a fundamental photo editing technique. However, when pasting the foreground object extracted from one image on another background image, there may exist style inconsistency between foreground and background. To address such style inconsistency, myriads of image harmonization methods \cite{tsai2017deep,cong2020dovenet,ling2021region} have been developed to match the illumination information between foreground and background.
Nevertheless, they are less effective when the foreground object is extracted from a photographic image and the background is an artistic painting, because complex styles (color, texture, pattern, strokes) need to be matched between foreground and background.
Image harmonization with photographic object and painterly background is called painterly image harmonization \cite{luan2018deep}, which has only received limited attention.

The existing painterly image harmonization methods can be roughly divided into optimization-based methods \cite{luan2018deep,zhang2020deep} and feed-forward methods \cite{peng2019element,cao2022painterly,yan2022style}. Optimization-based methods update the composite object iteratively to minimize the designed losses, which is too slow for real-time application. In contrast, feed-forward methods pass the composite image through the network only once to produce a harmonized image, which is much more efficient than optimization-based methods. To name a few,
\cite{peng2019element} introduced AdaIN \cite{huang2017arbitrary} to adjust the style of composite object.  \cite{cao2022painterly} explored harmonizing the composite image in both spatial domain and frequency domain. Some diffusion model based methods~\cite{sdedit,cdc} for cross-domain image composition can also be applied to this task. However, the harmonized objects may still look unnatural on the painterly background.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{figures/illustration.jpg}
\caption{(a) The background image with two marked query patches (red and blue). We show the similarity maps of two query patches based on VGG-19 \cite{VGG19} features. (b) The composite image with two inserted foreground objects. (c) The ideal harmonized image.}
\label{fig:illustration}
\end{figure}

One factor that hinders painterly image harmonization is the absence of ground-truth harmonized objects, but we can learn what an object in an artistic painting should be like based on the existent painterly objects in the artistic paintings. Therefore, this work explores learning from painterly objects, which is substantially overlooked by previous works \cite{luan2018deep,peng2019element,cao2022painterly,yan2022style}. The key insight of our work is hallucinating the target style of composite object. Previous works \cite{peng2019element,cao2022painterly} migrate the global style of painterly background to the composite object using AdaIN \cite{huang2017arbitrary}, during which the style is represented by the feature statistics (mean, variance) in pretrained VGG-19 \cite{VGG19} encoder. Nevertheless, the styles of different regions/objects in an artistic painting could vary considerably. As shown in Figure \ref{fig:illustration}(a), we divide the whole image into $16$ patches and extract the style vectors (mean/variance of VGG-19 encoder features) of all patches. We calculate their pairwise similarity based on style vectors. Given each query patch, we display a similarity map containing the similarities between this query patch and all patches.
The patches with high similarity values have similar visual styles with the query patch.
According to the similarity map, we can see that the style vector has prominent locality property, that is, the style vectors of various regions in the same image could be dramatically distinct.
Therefore, when compositing different objects (\emph{e.g.}, house, boat) at different locations on the background, they should be harmonized to different target styles (see Figure \ref{fig:illustration}(b) and (c)).

\emph{To achieve the goal of target style hallucination, we learn a mapping module based on painterly objects, which maps background style and object feature to object style.} The object features are supposed to contain the useful conditional information (\emph{e.g.}, color, semantics, local context) for adapting background style to object style. With the learnt mapping module, we attempt to hallucinate the target style of composite object based on its object feature and the background style. Intuitively, the mapping module answers the following question: ``\emph{given a painterly background, if its painter draws a specific object at this location, what should it look like?}"

However, one critical issue is the domain gap of object features between photographic objects and painterly objects, so that the mapping module trained on painterly objects cannot work well on composite objects. We have tried unsupervised manner like adversarial learning \cite{goodfellow2020generative} to bridge the domain gap, but the performance is far from satisfactory. Therefore, we choose to annotate pairs of similar photographic objects and painterly objects. Given an artistic painting with a painterly object, we retrieve and annotate photographic objects which are similar to this painterly object, leading to abundant triplets (artistic painting, painterly object, photographic object). In each triplet, we refer to the painterly object (\emph{resp.}, artistic painting) as the reference object (\emph{resp.}, reference image) of the photographic object. We put the photographic object within the bounding box of reference object in the reference image, resulting in a composite image (see Figure~\ref{fig:pair_construction}).
In this way, we can acquire abundant pairs of composite images and reference images, in which the composite object feature should be close to reference object feature and the hallucinated style of composite object should be close to reference object style. With the above assumptions, we design a novel network with mapping module, and train it using pairs of composite images and reference images.

Our major contributions can be summarized as follows. 1) We explore learning from painterly objects in the painterly image harmonization task, which has never been studied before. 2) We design a novel network with mapping module, which can hallucinate the target style of composite object based on the background style and object feature. 3) We will release our annotated reference images/objects, which would greatly benefit the future research of painterly image harmonization.
4) Extensive experiments on COCO \cite{lin2014microsoft} and WikiArt \cite{nichol2016painter} demonstrate the effectiveness of our proposed method.



\section{Related Work}

\subsection{Image Harmonization} \label{sec:image_harmonization}

Image harmonization aims to harmonize a composite image by adjusting foreground illumination to match background illumination.
In recent years, abundant deep image harmonization methods \cite{tsai2017deep,Jiang_2021_ICCV,xing2022composite,peng2022frih,zhu2022image,valanarasu2022interactive,LEMaRT} have been developed. For example, \cite{xiaodong2019improving,Hao2020bmcv,sofiiuk2021foreground} proposed diverse attention modules to treat the foreground and background separately, or establish the relation between foreground and background. \cite{cong2020dovenet,cong2021bargainnet,ling2021region,hang2022scs} directed image harmonization to domain translation or style transfer by treating different illumination conditions as different domains or styles.  \cite{guo2021image,guo2021intrinsic,guo2022transformer}  decomposed an image into reflectance map and illumination map.  More recently, \cite{cong2022high,ke2022harmonizer,liang2021spatial,xue2022dccf,PCTNet,WangCVPR2023} used deep network to predict color transformation, striking a good balance between efficiency and effectiveness.

However, most image harmonization methods only adjust illumination and require ground-truth supervision, which is unsuitable for painterly image harmonization.

%-------------------------------------------------------------------------
\subsection{Painterly Image Harmonization}

Different from Section \ref{sec:image_harmonization}, in painterly image harmonization, the foreground is a photographic object while the background is artistic painting. The goal of painterly image harmonization is adjusting the foreground style to match background style and preserving the foreground content. The existing methods \cite{luan2018deep,zhang2020deep,peng2019element} can be roughly categorized into optimization-based methods \cite{luan2018deep,zhang2020deep} and feed-forward methods \cite{peng2019element}.  The optimization-based methods \cite{luan2018deep,zhang2020deep} iteratively optimize over the composite foreground to minimize the designed loss functions. The feed-forward methods \cite{peng2019element,yan2022style,cao2022painterly} send the composite image through the harmonization network once and generate the harmonized image. %Without iterative optimization during inference, the feed-forward methods are far more efficient than the optimization-based methods.
Some diffusion model based methods~\cite{sdedit,cdc} for cross-domain image composition can also harmonize the composite image.

Our proposed method belongs to feed-forward methods. Different from previous works \cite{peng2019element,yan2022style,cao2022painterly}, we explore learning from painterly objects to hallucinate the target styles of composite objects.


\subsection{Artistic Style Transfer}
Artistic style transfer~\cite{kolkin2019style,jing2020dynamic,chen2021dualast,chen2017stylebank,sanakoyeu2018style,wang2020collaborative,li2018learning,chen2016fast,sheng2018avatar,gu2018arbitrary,zhang2019multimodal,chen2022toward,huo2021manifold} targets at recomposing a content image in the style of a style image. Amounts of works~\cite{huang2017arbitrary,li2017universal} focus on global style transfer.
%For example, AdaIN~\cite{huang2017arbitrary} applied mean and standard deviation to shift and re-scale the normalized content feature.
%WCT~\cite{li2017universal} adopted two transformation steps including whitening and coloring to achieve style transfer. A linear transformation according to content and style features was proposed in \cite{li2019learning}.
Recently, some works \cite{park2019arbitrary,liu2021adaattn,deng2022stytr2} turn to fine-grained local style transfer by establishing local correspondences between content image and style image.
For example, non-local attention mechanism was adopted in SANet~\cite{park2019arbitrary} to attentively transfer style features to content features. AdaAttN~\cite{liu2021adaattn} proposed  novel attentive normalization  by combining AdaIN~\cite{huang2017arbitrary} and SANet~\cite{park2019arbitrary}.  StyTr$^2$~\cite{deng2022stytr2} applied transformer architecture to capture patch-wise features to solve content leak issue.

The motivation of local style transfer \cite{elad2017style,li2016combining,park2019arbitrary,huo2021manifold} is seeking for relevant regions in the style image for a given content image and transferring the corresponding local style. \emph{Nevertheless, the existence of relevant regions is questionable, and the methods may not find the relevant regions accurately (see Section~\ref{sec:cmp_with_baseline})}.


\section{Our Method}

\begin{figure*}[t]
\centering
\includegraphics[width=0.85\linewidth]{figures/network_new.jpg}
\caption{The illustration of our network structure. Given a pair of composite image $\bm{I}^c$ and painterly image $\bm{I}^p$, we pass them through pretrained VGG-19~\cite{VGG19} encoder and projection module $P$ to get object features $\hat{\bm{f}}^{c,o}$ and $\hat{\bm{f}}^{p,o}$ respectively. We insert our designed ObAdaIN modules into skip connections and bottleneck, which harmonize the encoder feature maps of $\bm{I}^c$. The harmonized encoder feature maps are delivered to the decoder to produce the harmonized image $\tilde{\bm{I}}^h$. In the ObAdaIN module in the $l$-th layer, the mapping module $M_l$ learns a mapping from background style $\bm{s}_l^{p,b}$ (\emph{resp.}, $\bm{s}_l^{c,b}$) and object feature $\hat{\bm{f}}^{p,o}$ (\emph{resp.}, $\hat{\bm{f}}^{c,o}$) to object style $\tilde{\bm{s}}_l^{p,o}$  (\emph{resp.}, $\tilde{\bm{s}}_l^{c,o}$) for painterly (\emph{resp.}, composite) object. }
\label{fig:network}
\end{figure*}

We suppose that a composite image $\bm{I}^c$ is obtained by pasting a photographic object on the painterly background.  Painterly image harmonization aims to transfer the style from painterly background to composite object while preserving the content of composite object, resulting in a harmonized image $\tilde{\bm{I}}^h$. To learn from painterly objects in artistic paintings, we first build a training set with pairs of composite images $\bm{I}^c$ and reference images $\bm{I}^p$.
Then, we design a network which could be trained on paired training images. Our network is based on U-Net  \cite{ronneberger2015u} structure with skip connections, with our designed ObAdaIN module inserted into the bottleneck and skip connections. The ObAdaIN module aims to hallucinate the target style of composite object.
We name our constructed training data as Arto (\textbf{Art}istic \textbf{O}bject) dataset and our method as ArtoPIH (\textbf{P}ainterly \textbf{I}mage \textbf{H}armonization).
Next, we will introduce our Arto dataset in Section~\ref{sec:train_data_preparation} and our ArtoPIH in Section~\ref{sec:network_structure}.

\subsection{Training Data Preparation}\label{sec:train_data_preparation}


\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/reference_object_main.jpg}
\caption{The illustration of constructing training image pairs $\{\bm{I}^p, \bm{I}^c\}$. Given a photographic object (outlined in yellow), we have a reference object  (outlined in yellow) with similar color and semantics in the reference image $\bm{I}^p$. We put the photographic object within the bounding box of reference object, resulting in a composite image $\bm{I}^c$. }
\label{fig:pair_construction}
\end{figure}


Based on $57,025$ artistic paintings in the training set of WikiArt \cite{nichol2016painter}, we use off-the-shelf object detection model \cite{wu2019detectron2} pretrained on COCO \cite{lin2014microsoft} dataset to detect objects in artistic paintings. Despite the domain gap between artistic paintings and photographic images, we detect $34,570$ painterly objects.
For each painterly object, we first retrieve $100$ photographic objects with similar appearance and semantics automatically (see details in the Supplementary).

However, the retrieved photographic objects are very noisy, so we ask annotators to manually remove those dissimilar photographic objects. After filtering, we have $33,294$ painterly objects associated with similar photographic objects.
%Among them, ``person" is the dominant category with $18,185$ painterly objects.
Each painterly object has an average of $9.83$ similar photographic objects, leading to a total of $327,181$  triplets (artistic painting $\bm{I}^p$, painterly object, photographic object). In each triplet, we refer to the painterly object (\emph{resp.}, artistic painting) as the reference object (\emph{resp.}, reference image) of the photographic object. As illustrated in Figure~\ref{fig:pair_construction}, we put the photographic object within the bounding box of reference object in the reference image, producing a composite image $\bm{I}^c$ with foreground mask $\bm{M}^c$. Therefore, we have in total $327,181$ training image pairs $\{\bm{I}^c, \bm{I}^p\}$.

\subsection{Our Network Structure} \label{sec:network_structure}

We employ the encoder and decoder structures in \cite{huang2017arbitrary} as our backbone, in which the encoder is pretrained VGG-19~\cite{VGG19} and the decoder is symmetric structure of encoder. We only use the layers up to \emph{ReLU-4\_1} of VGG-19 as our encoder, which are fixed to extract multi-scale encoder features. The encoder is divided into four encoder layers, which are up to \emph{ReLU-1\_1}, \emph{ReLU-2\_1}, \emph{ReLU-3\_1}, and \emph{ReLU-4\_1} respectively. We add skip connections for the first three encoder layers.

We feed composite image $\bm{I}^c$ into the encoder to extract the feature map $\bm{F}_l^c$ of $l$-th layer for \emph{l} $\in \left\{1, 2, 3, 4\right\}$. Similarly, we also feed its reference image $\bm{I}^p$ with reference object into the encoder to extract the feature map $\bm{F}^p_l$ of $l$-th layer for \emph{l} $\in \left\{1, 2, 3, 4\right\}$.
Previous works on painterly image harmonization \cite{peng2019element,cao2022painterly} usually apply vanilla Adaptive Instance Normalization (AdaIN)~\cite{huang2017arbitrary} to the encoder feature maps, by aligning the feature statistics (mean, standard deviation) of foreground feature map with those of background feature map. As the feature statistics represent the style information, the composite object would be rendered with the background style.

However, the background style may not be the ideal target style for the composite object, considering the diversified local styles within the same image and the unique property (\emph{e.g.}, color, semantics, local context) of composite object. In this work, we attempt to hallucinate the ideal target style of composite object by considering background style and object information. To achieve this goal, we design a novel Object-aware AdaIN (ObAdaIN) module to replace the vanilla AdaIN operation.

\textbf{Domain-Invariant Object Feature:} Our ObAdaIN module requires the object feature of foreground object, which contains the necessary conditional information for adapting background style to object style.

Given a training image pair $\{\bm{I}^c, \bm{I}^p\}$, the composite object in $\bm{I}^c$ is photographic object but the reference object in $\bm{I}^p$ is painterly object, so their object features $\hat{\bm{f}}^{c,o}$ and $\hat{\bm{f}}^{p,o}$ belong to two different domains. As shown in Figure~\ref{fig:network}, we first project feature maps $\bm{F}^p_4$ and $\bm{F}^c_4$ to a common domain through a projection module $P$, yielding $\hat{\bm{F}}^p$ and $\hat{\bm{F}}^c$ respectively. Then, we perform average pooling within the foreground region on  $\hat{\bm{F}}^p$ (\emph{resp.}, $\hat{\bm{F}}^c$) to get the object feature $\hat{\bm{f}}^{p,o}$ (\emph{resp.}, $\hat{\bm{f}}^{c,o}$).

Since the composite object and its reference object have similar appearance and semantics, we push close $\hat{\bm{f}}^{p,o}$ and $\hat{\bm{f}}^{c,o}$ using the following loss:
\begin{eqnarray}\label{eqn:loss_obj}
\mathcal{L}_{obj} = \|\hat{\bm{f}}^{p,o}-\hat{\bm{f}}^{c,o}\|^2.
\end{eqnarray}

By pulling close the object features of similar objects from two domains,  we enforce the extracted object features to be domain-invariant.

\textbf{ObAdaIN Module: }With extracted domain-invariant object features, our ObAdaIN module learns a mapping from background style and object feature to object style. The architecture of ObAdaIN is shown in Figure~\ref{fig:network}.

By taking the reference image $\bm{I}^p$ and the $l$-th encoder layer as an example, we have the feature map $\bm{F}^p_l$. We extract the background style vector $\bm{s}^{p,b}_l=[\bm{\mu}^{p,b}_l, \bm{\sigma}^{p,b}_l]$, in which $\bm{\mu}^{p,b}_l$ (\emph{resp.}, $\bm{\sigma}^{p,b}_l$) is channel-wise mean (\emph{resp.}, standard deviation) of background feature map.

We concatenate the background style vector $\bm{s}^{p,b}_l$ with object feature $\hat{\bm{f}}^{p,o}$, which are sent to a mapping module $M_l$ to generate the object style vector $\tilde{\bm{s}}^{p,o}_l$.
The above process can be represented by $\tilde{\bm{s}}^{p,o}_l=M_l(\bm{s}^{p,b}_l, \hat{\bm{f}}^{p,o})$.
For the reference object in  $\bm{I}^p$, we can obtain its ground-truth style vector $\bm{s}^{p,o}_l$ by calculating the channel-wise mean and standard deviation of foreground feature map. We use $\bm{s}^{p,o}_l$ to supervise $\tilde{\bm{s}}^{p,o}_l$ as follows,
\begin{eqnarray}
\mathcal{L}_{map}^p = \|\tilde{\bm{s}}^{p,o}_l-\bm{s}^{p,o}_l\|^2.
\end{eqnarray}

Compared with $\bm{I}^p$, its paired composite image $\bm{I}^c$ has similar foreground object, the same background, and the same foreground placement (bounding box). Therefore, we assume that the ideal target style of composite object should be close to reference object style $\bm{s}^{p,o}_l$.
Specifically, given $\bm{I}^c$, we extract its background style vector $\bm{s}^{c,b}_l$ and object feature $\hat{\bm{f}}^{c,o}$ in a similar way to $\bm{I}^p$. Then, we can have $\tilde{\bm{s}}^{c,o}_l=M_l(\bm{s}^{c,b}_l, \hat{\bm{f}}^{c,o})$, which is supervised by $\bm{s}^{p,o}_l$:
\begin{eqnarray}
\mathcal{L}_{map}^c = \|\tilde{\bm{s}}^{c,o}_l-\bm{s}^{p,o}_l\|^2.
\end{eqnarray}

Next, we use the hallucinated style vector $\tilde{\bm{s}}^{c,o}_l=[\tilde{\bm{\mu}}^{c,o}_l, \tilde{\bm{\sigma}}^{c,o}_l]$ to harmonize the composite image  $\bm{I}^c$. By denoting the foreground (\emph{resp.}, background) feature map in $\bm{F}_l^c$ as $\bm{F}^{c,o}_l$ (\emph{resp.}, $\bm{F}^{c,b}_l$), we apply AdaIN to adjust $\bm{F}^{c,o}_l$ as follows,
\begin{eqnarray}\label{eqn:adain}
    \tilde{\bm{F}}^{c,o}_{l} = \tilde{\bm{\sigma}}^{c,o}_l\frac{\bm{F}^{c,o}_l-\mu(\bm{F}^{c,o}_l)}{\sigma(\bm{F}^{c,o}_l)} + \tilde{\bm{\mu}}^{c,o}_l,
\end{eqnarray}
in which $\mu(\cdot)$ (\emph{resp.}, $\sigma(\cdot)$) means calculating the mean (\emph{resp.}, standard deviation) of the specified feature map. The adjusted foreground feature map $\tilde{\bm{F}}^{c,o}_{l}$ is combined with the unchanged background feature map $\bm{F}^{c,b}_l$ to form the harmonized feature map $\tilde{\bm{F}}^{c}_{l}$.

The harmonized feature maps of all encoder layers are sent to the decoder to produce the harmonized image $\bm{I}^h$. Similar to \cite{cao2022painterly}, we also adopt blending layer to combine the output image $\bm{I}^h$ and the composite image $\bm{I}^c$ with predicted soft blending mask, giving rise to the final harmonized image $\tilde{\bm{I}}^h$.

For the harmonized image $\tilde{\bm{I}}^h$, we expect its object style to match the reference object style $\bm{s}^{p,o}_l$, and thus design the style loss accordingly:
\begin{eqnarray}\label{eqn:style_loss}
\mathcal{L}_{sty} =\!\!\!\!\!\!\!\!\!\!\!\!&&\sum_{l=1}^{4}\|\mu\left(\phi_{l}(\tilde{\bm{I}}^h)\circ \bm{M}^c\right)-\bm{\mu}^{p,o}_l \|^2 \nonumber\\
&&\!\!\!\!\!\!\!\!\!\!\!\!+ \sum_{l=1}^{4}\|\sigma\left(\phi_{l}(\tilde{\bm{I}}^h)\circ \bm{M}^c\right)-\bm{\sigma}^{p,o}_l \|^2,
\end{eqnarray}
where each $\phi_{l}(\cdot)$ denotes the $l$-th encoder layer in VGG-19~\cite{VGG19} encoder, $\circ$ means element-wise product.

We use content loss \cite{gatys2016image} to ensure that the foreground content is maintained:
\begin{equation}\label{eqn:content_loss}
    \mathcal{L}_{con} =\left\|\phi_4(\tilde{\bm{I}}^h)-\phi_4(\bm{I}^c)\right\|^2,
\end{equation}
in which $\phi_{4}(\cdot)$ has been defined in Eqn.~\ref{eqn:style_loss}.

The total loss can be written as
\begin{eqnarray} \label{eqn:total_loss}
\mathcal{L}_{total} = \mathcal{L}_{obj} \!+\! \lambda (\mathcal{L}_{map}^p \!+\! \mathcal{L}_{map}^c) \!+\!  \mathcal{L}_{sty} \!+\!  \mathcal{L}_{con}, %\nonumber
\end{eqnarray}
in which the hyper-parameter $\lambda$ is empirically set as $10$.

\section{Experiments}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{figures/baseline_main.jpg}
\caption{In the upper part, we compare with style transfer baselines SANet~\cite{park2019arbitrary}, AdaAttN~\cite{liu2021adaattn}, StyTr2~\cite{deng2022stytr2}, QuantArt~\cite{quantart}, INST~\cite{inst}.
In the lower part, we compare with painterly image harmonization baselines SDEdit~\cite{sdedit}, CDC~\cite{cdc}, E2STN~\cite{peng2019element}, DPH~\cite{luan2018deep}, PHDNet~\cite{cao2022painterly}.
 }
\label{fig:baseline_main}
\end{figure*}



\subsection{Datasets and Implementation Details} \label{sec:imp_detail}

We conduct experiments on COCO \cite{lin2014microsoft} and WikiArt \cite{nichol2016painter}. The details are left to Supplementary.
Our network is implemented using Pytorch 1.10.0 and trained using Adam optimizer with learning rate of $1e{-4}$ on ubuntu 20.04 LTS operation system, with 128GB memory, Intel(R) Xeon(R) Silver 4116 CPU, and one GeForce RTX 3090 GPU. For the encoder and decoder structure, we follow \cite{cao2022painterly}. For $P$ module, we use one residual block \cite{he2016deep}. For $M_l$ module in the $l$-th encoder layer, we stack three ResMLP Layers \cite{TouvronBCCEGIJSVJ23}, in which the intermediate dimension is equal to the dimension of style vector in the $l$-th layer.
%We set the batch size as $4$ and resize the input images as $256 \times 256$ during the training phase.
%As our network is fully convolutional, it can be applied to images of any size in the test phase. Total of 100 epochs are used for convergence.


\subsection{Comparison with Baselines} \label{sec:cmp_with_baseline}

We compare with two groups of baselines: artistic style transfer methods and painterly image harmonization methods.  Since the standard image harmonization methods introduced in Section~\ref{sec:image_harmonization} only adjust the illumination statistics and require ground-truth images as supervision, they are not suitable for painterly image harmonization.

For the first group of baselines, we first use artistic style transfer methods to stylize the entire photographic image according to the painterly background, and then paste the stylized photographic object on the painterly background.
We compare with typical or recent style transfer methods: SANet~\cite{park2019arbitrary}, AdaAttN~\cite{liu2021adaattn},  StyTr2~\cite{deng2022stytr2}, QuantArt~\cite{quantart}, INST~\cite{inst}.
For the second group of baselines, we compare with SDEdit~\cite{sdedit}, CDC~\cite{cdc}, E2STN~\cite{peng2019element}, DPH~\cite{luan2018deep}, PHDNet~\cite{cao2022painterly}.

\textbf{Visualization Results:} The comparison with the first group of baselines is shown in the upper part in Figure~\ref{fig:baseline_main}. We can see that the harmonized objects generated by our method look more natural and compatible with the background. In row 1, one interesting observation is that the vases harmonized by different methods have different colors. One possible reason is that SANet, AdaAttN, and StyTr2 seek for the relevance between composite object and different regions in the painterly background, and transfer the local style of relevant region to the composite object. However, there may not exist relevant regions in the painterly background. Even if relevant regions exist, they may not accurately attend to the relevant regions. In contrast, we hallucinate the target style based on background style and object information. Our harmonized vase maintains the object color and also has background texture, while the baseline results are over-smooth without compatible textures or losing some structure details (\emph{e.g.}, vase neck). In row 2, the harmonized faces from baseline methods all have corrupted structures, while our method preserves the facial structure well. This might be because that our model learns from abundant painterly person in artistic paintings and the hallucinated target style is able to preserve the facial structure.

\begin{table}[t]
\centering
\begin{tabular}{c|c|c|c}
\hline
Method  & B-T score & Time(s)  & FLOPs(G) \\
\hline
SANet  & -0.365 & 0.0097 & 43.32\\
AdaAttN  & -0.535 & 0.0115 & 49.64\\
StyTr2  &  0.149 & 0.0504 & 39.74\\
QuantArt   & 0.336 & 0.1031 & 133.34 \\
Inst  & -0.762 & 2.2996 & 3378.43 \\
\hline
SDEdit  & -0.654 & 2.1321 & 3164.52\\
CDC   &  0.204 & 2.3427 & 3299.81 \\
E2STN & -0.152 & 0.0079 & 29.28\\
DPH   & 0.340 & 55.24 & -\\
PHDNet  & 0.485 & 0.0321 & 158.41\\
\hline
ArtoPIH  & 0.953 & 0.0258 & 40.03\\
\hline
\end{tabular}
\caption{The comparison between different methods. }
\label{tab:results}
\end{table}

The comparison with the second group of baselines is shown in the lower part in Figure~\ref{fig:baseline_main}. In row 1, our harmonized dog has more harmonious color and compatible texture with the background, while the baseline results are corrupted or overdark. In row 2, our harmonized fruits have more suitable color and textures, and thus look more visually pleasant than the baseline results. More visualization results can be found in Supplementary.



\begin{figure*}[t]
\centering
\includegraphics[width=0.95\linewidth]{figures/reference_results.jpg}
\caption{From left to right, we show the reference image, the mask of reference object, the composite image, the mask of composite object, and the harmonized results obtained using different style vectors. ``BG" uses background style vector, ``RO" uses reference object style vector, and ``Ours" uses our hallucinated style vector.}
\label{fig:reference_results}
\end{figure*}

\textbf{User Study:} Following \cite{cao2022painterly}, we also conduct user study to compare different methods. We randomly select 100 content images from COCO and 100 style images from WikiArt to generate 100 composite images. We compare the harmonized results generated by 10 baselines and our ArtoPIH.

Specifically, for each composite image, we obtain $11$ harmonized outputs and use every two images from these $11$ images to construct image pairs. With $100$ composite images, we construct $5500$ image pairs. Then, we invite $100$ participants to watch one image pair each time and choose the better one. Finally, we collect $550,000$ pairwise results and calculate the overall ranking of all methods using Bradley-Terry (B-T) model~\cite{bradley1952rank,lai2016comparative}. The B-T score results are summarized in Table~\ref{tab:results}, in which our method achieves the highest B-T score.


\textbf{Efficiency Analyses:} For efficiency comparison, we report the inference time and FLOPs. We evaluate with the image size $256 \times 256$ and the inference time is averaged over 100 test images on a single RTX 3090 GPU.
The optimization-based method DPH is time-consuming, due to iteratively updating the input composite image. Diffusion-based methods are much slower than the other feed-forward methods, which limits their real-world applicability.
Our method is relatively efficient compared with the competitive baselines \cite{cao2022painterly,luan2018deep}.









\subsection{Hallucinated Object Style}
To intuitively demonstrate the effectiveness of our hallucinated object styles, we sample some photographic objects in the test set and find their reference painterly objects in the test set, similar to the process of constructing training image pairs (see Section \ref{sec:train_data_preparation}). Specifically, given a photographic object and a painterly object in a painterly image, we can obtain a composite image as illustrated in Figure~\ref{fig:pair_construction}, followed by calculating the $L_2$ distance between the composite object feature $\hat{\bm{f}}^{c,o}$ and painterly object feature $\hat{\bm{f}}^{p,o}$. For each photographic object, we retrieve its nearest painterly object as reference object and the corresponding painterly image as reference image. In Figure \ref{fig:reference_results}, we show several examples of photographic objects and the retrieved reference objects. It can be seen that the photographic objects have similar color and semantics with their reference objects, which verifies the effectiveness of our learnt domain-invariant object features. %More examples of retrieved reference objects are provided in the Supplementary.

Based on the composite image composed by the photographic object and its reference image, we further adjust the shape of photographic object to fully cover the reference object in the reference image, to exclude the influence of reference object. Then, based on our trained model, we apply three types of style vectors to the composite object to get the harmonized images.
The first one is the background style vector $\bm{s}_l^{c,b}$. The second one is the style vector $\bm{s}_l^{p,o}$ of reference object. The third one is our hallucinated style vector $\tilde{\bm{s}}_l^{c,o}$. We apply the above three style vectors to the foreground feature map using AdaIN operation. The obtained harmonized images are denoted as ``BG", ``RO", and ``Ours" respectively, as shown in Figure~\ref{fig:reference_results}.
The harmonized results ``BG" and ``RO" differ a lot, which demonstrates the huge gap between background style and object style. Directly applying background style may destroy the original color of composite object or bring in noticeable artifacts.

Since the composite object and reference object have similar object information and background style, the target style $\tilde{\bm{s}}_l^{c,o}$ of composite object is expected to approach the reference object style $\bm{s}_l^{p,o}$. Based on Figure~\ref{fig:reference_results},  the harmonized results ``Ours" are close to ``RO", which proves that our model can hallucinate ideal target styles.

\subsection{More Results in Supplementary}

In the supplementary, we will provide ablation study results, more visual comparison with baselines, and discussion on failure cases. We will also show the results beyond COCO \cite{lin2014microsoft} dataset to demonstrate the generalization ability of our method across different datasets and different object categories.

\section{Conclusion}
In this work, we have explored learning from painterly objects for painterly image harmonization. Based on the annotated pairs of composite images and reference painterly images, we have succeeded in hallucinating the target style of composite object, leading to visually pleasing harmonization results. Extensive experiments on the benchmark dataset have proved the advantage of our proposed ArtoPIH.




\section*{Acknowledgments}
The work was supported by the National Natural Science Foundation of China (Grant No. 62076162), the Shanghai Municipal Science and Technology Major/Key Project, China (Grant No. 2021SHZDZX0102, Grant No. 20511100300).


Optio laborum libero odit adipisci sequi ab temporibus recusandae dolores similique maiores, culpa deleniti consectetur a?Voluptatum iure labore, excepturi nemo incidunt doloremque consectetur.Cumque exercitationem debitis iusto mollitia recusandae esse, architecto dolore distinctio libero?Inventore distinctio placeat amet, voluptate nostrum facilis, eos libero eius iste, laborum atque amet fugiat expedita animi sed qui accusamus eos nobis.Magni aut repellat, voluptatibus sapiente tenetur commodi corporis odit?Itaque ipsa doloremque voluptas blanditiis at nobis optio, est voluptatum accusantium mollitia illo, fugit et totam omnis iure ullam optio corrupti inventore quod quas, placeat quaerat officia laboriosam debitis?Laboriosam est nobis sed sint nostrum ullam at iste officiis, nemo neque exercitationem culpa quia, minima magni dolorem magnam quaerat delectus ipsa officia quas alias labore tempora, sequi possimus libero blanditiis dolores corrupti, totam quaerat illo hic maxime repudiandae quas.Eveniet ipsum ea nobis numquam, quo fugit sequi harum.Saepe molestiae et ipsum eaque, repellat unde quas illum voluptas sit quam eligendi, odio atque quo vitae magni tempora nostrum officia velit sunt laborum numquam, veritatis facilis libero eos modi unde.Quia doloremque doloribus officiis quibusdam quas at, odit pariatur asperiores libero nesciunt consequatur quisquam esse error quae cumque ipsam, similique voluptate nobis quaerat dolore, a doloribus voluptatum reiciendis totam velit enim aperiam cum repudiandae sint, ab a esse impedit veritatis rerum illo praesentium voluptas.Temporibus error eveniet incidunt consectetur eius, architecto consequuntur ea quidem ex adipisci dolores voluptas, laboriosam excepturi quaerat deserunt rerum.\clearpage
\bibliography{main.bbl}

\end{document}