\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Ancona et~al.(2019)}]{ancona2019gradient}
Ancona, M.; et~al. 2019.
\newblock Gradient-based attribution methods.
\newblock In \emph{Explainable AI: Interpreting, explaining and visualizing deep learning}, 169--191.

\bibitem[{Andreas(2022)}]{kl}
Andreas, J. 2022.
\newblock Language Models as Agent Models.
\newblock arXiv:2212.01681.

\bibitem[{Cao et~al.(2023)Cao, Lin, Han, and Sun}]{cao2023life}
Cao, B.; Lin, H.; Han, X.; and Sun, L. 2023.
\newblock The Life Cycle of Knowledge in Big Language Models: A Survey.
\newblock arXiv:2303.07616.

\bibitem[{Dai et~al.(2022)Dai, Dong, Hao, Sui, Chang, and Wei}]{dai2022kn}
Dai, D.; Dong, L.; Hao, Y.; Sui, Z.; Chang, B.; and Wei, F. 2022.
\newblock Knowledge Neurons in Pretrained Transformers.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022}, 8493--8502.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova}]{mbert}
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2018.
\newblock {BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding.
\newblock \emph{CoRR}, abs/1810.04805.

\bibitem[{Edwards(2022)}]{hallucination_chatgpt3}
Edwards, B. 2022.
\newblock OpenAI invites everyone to test ChatGPT, a new AI-powered chatbot—with amusing results.
\newblock Retrieved 29 December 2022.

\bibitem[{Edwards(2023)}]{hallucination_chatgpt1}
Edwards, B. 2023.
\newblock Why ChatGPT and Bing Chat are so good at making things up.
\newblock Retrieved 11 June 2023.

\bibitem[{Enguehard(2023)}]{enguehard2023sequential}
Enguehard, J. 2023.
\newblock Sequential Integrated Gradients: a simple but effective method for explaining language models.
\newblock arXiv:2305.15853.

\bibitem[{Geva et~al.(2021)Geva, Schuster, Berant, and Levy}]{key_value}
Geva, M.; Schuster, R.; Berant, J.; and Levy, O. 2021.
\newblock Transformer Feed-Forward Layers Are Key-Value Memories.
\newblock arXiv:2012.14913.

\bibitem[{Hase et~al.(2023)Hase, Bansal, Kim, and Ghandeharioun}]{localization}
Hase, P.; Bansal, M.; Kim, B.; and Ghandeharioun, A. 2023.
\newblock Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models.
\newblock arXiv:2301.04213.

\bibitem[{Jiang et~al.(2020)Jiang, Xu, Araki, and Neubig}]{knowledge_know_fact}
Jiang, Z.; Xu, F.~F.; Araki, J.; and Neubig, G. 2020.
\newblock How Can We Know What Language Models Know?
\newblock arXiv:1911.12543.

\bibitem[{Kandpal et~al.(2023)Kandpal, Deng, Roberts, Wallace, and Raffel}]{kandpal2023large}
Kandpal, N.; Deng, H.; Roberts, A.; Wallace, E.; and Raffel, C. 2023.
\newblock Large language models struggle to learn long-tail knowledge.
\newblock In \emph{International Conference on Machine Learning}, 15696--15707. PMLR.

\bibitem[{Kassner, Dufter, and Schütze(2021)}]{mlama}
Kassner, N.; Dufter, P.; and Schütze, H. 2021.
\newblock Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models.
\newblock arXiv:2102.00894.

\bibitem[{Lakshmanan(2022)}]{lakshmanan2022large}
Lakshmanan, L. 2022.
\newblock Why large language models like ChatGPT are bullshit artists.
\newblock \emph{becominghuman.ai}.
\newblock Archived from the original on December 17, 2022.

\bibitem[{Li et~al.(2022)Li, Li, Shang, Dong, Sun, Liu, Ji, Jiang, and Liu}]{causal-inspired}
Li, S.; Li, X.; Shang, L.; Dong, Z.; Sun, C.; Liu, B.; Ji, Z.; Jiang, X.; and Liu, Q. 2022.
\newblock How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis.
\newblock arXiv:2203.16747.

\bibitem[{Liu et~al.(2022)Liu, Fan, Xiong, Wang, Hu, Lv, Chen, Wu, and Gao}]{transparent}
Liu, S.; Fan, C.; Xiong, Y.; Wang, M.; Hu, Y.; Lv, T.; Chen, Z.; Wu, R.; and Gao, Y. 2022.
\newblock The Effective coalitions of Shapley value For Integrated Gradients.

\bibitem[{Lundstrom, Huang, and Razaviyayn(2022)}]{rigorousIG}
Lundstrom, D.~D.; Huang, T.; and Razaviyayn, M. 2022.
\newblock A rigorous study of integrated gradients method and extensions to internal neuron attributions.
\newblock In \emph{International Conference on Machine Learning}, 14485--14508. PMLR.

\bibitem[{Mason(2015)}]{mason2015degeneracy}
Mason, P.~H. 2015.
\newblock Degeneracy: Demystifying and destigmatizing a core concept in systems biology.
\newblock \emph{Complexity}, 20(3): 12--21.

\bibitem[{Meng et~al.(2022{\natexlab{a}})Meng, Bau, Andonian, and Belinkov}]{meng2022locating}
Meng, K.; Bau, D.; Andonian, A.; and Belinkov, Y. 2022{\natexlab{a}}.
\newblock Locating and Editing Factual Associations in {GPT}.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Meng et~al.(2022{\natexlab{b}})Meng, Sen~Sharma, Andonian, Belinkov, and Bau}]{meng2022memit}
Meng, K.; Sen~Sharma, A.; Andonian, A.; Belinkov, Y.; and Bau, D. 2022{\natexlab{b}}.
\newblock Mass Editing Memory in a Transformer.
\newblock \emph{arXiv preprint arXiv:2210.07229}.

\bibitem[{Metz(2022)}]{metz2022new}
Metz, C. 2022.
\newblock The new chatbots could change the world. Can you trust them.
\newblock \emph{The New York Times}, 10.

\bibitem[{Mitchell et~al.(2022)Mitchell, Lin, Bosselut, Finn, and Manning}]{mend}
Mitchell, E.; Lin, C.; Bosselut, A.; Finn, C.; and Manning, C.~D. 2022.
\newblock Fast Model Editing at Scale.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{OpenAI(2023)}]{openai2023gpt4}
OpenAI. 2023.
\newblock GPT-4 Technical Report.
\newblock arXiv:2303.08774.

\bibitem[{Petroni et~al.(2020)Petroni, Lewis, Piktus, Rockt{\"a}schel, Wu, Miller, and Riedel}]{lama2}
Petroni, F.; Lewis, P.; Piktus, A.; Rockt{\"a}schel, T.; Wu, Y.; Miller, A.~H.; and Riedel, S. 2020.
\newblock How Context Affects Language Models' Factual Predictions.
\newblock In \emph{Automated Knowledge Base Construction}.

\bibitem[{Petroni et~al.(2019{\natexlab{a}})Petroni, Rockt{\"a}schel, Riedel, Lewis, Bakhtin, Wu, and Miller}]{fill-in-the-blank}
Petroni, F.; Rockt{\"a}schel, T.; Riedel, S.; Lewis, P.; Bakhtin, A.; Wu, Y.; and Miller, A. 2019{\natexlab{a}}.
\newblock Language Models as Knowledge Bases?
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, 2463--2473. Hong Kong, China: Association for Computational Linguistics.

\bibitem[{Petroni et~al.(2019{\natexlab{b}})Petroni, Rocktäschel, Lewis, Bakhtin, Wu, Miller, and Riedel}]{knowledge-base}
Petroni, F.; Rocktäschel, T.; Lewis, P.; Bakhtin, A.; Wu, Y.; Miller, A.~H.; and Riedel, S. 2019{\natexlab{b}}.
\newblock Language Models as Knowledge Bases?
\newblock arXiv:1909.01066.

\bibitem[{Pitt(2022)}]{hallucination_chatgpt2}
Pitt, S. 2022.
\newblock Google vs. ChatGPT: Here's what happened when I swapped services for a day.
\newblock Retrieved 30 December 2022.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever}]{gpt2}
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019.
\newblock Language Models are Unsupervised Multitask Learners.

\bibitem[{Sanyal and Ren(2021)}]{DIG}
Sanyal, S.; and Ren, X. 2021.
\newblock Discretized Integrated Gradients for Explaining Language Models.
\newblock arXiv:2108.13654.

\bibitem[{Shliazhko et~al.(2022)Shliazhko, Fenogenova, Tikhonova, Mikhailov, Kozlova, and Shavrina}]{mgpt}
Shliazhko, O.; Fenogenova, A.; Tikhonova, M.; Mikhailov, V.; Kozlova, A.; and Shavrina, T. 2022.
\newblock mGPT: Few-Shot Learners Go Multilingual.

\bibitem[{Sundararajan, Taly, and Yan(2017)}]{ig}
Sundararajan, M.; Taly, A.; and Yan, Q. 2017.
\newblock Axiomatic Attribution for Deep Networks.
\newblock arXiv:1703.01365.

\bibitem[{Tononi, Sporns, and Edelman(1999)}]{degenerate_biological}
Tononi, G.; Sporns, O.; and Edelman, G.~M. 1999.
\newblock Measures of degeneracy and redundancy in biological networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 96(6): 3257--3262.

\bibitem[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom}]{touvron2023llama}
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D.; Blecher, L.; Ferrer, C.~C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hosseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I.; Korenev, A.; Koura, P.~S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.; Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E.~M.; Subramanian, R.; Tan, X.~E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J.~X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023.
\newblock Llama 2: Open Foundation and Fine-Tuned Chat Models.
\newblock arXiv:2307.09288.

\bibitem[{Vladika and Matthes(2023)}]{fact-checking-survey}
Vladika, J.; and Matthes, F. 2023.
\newblock Scientific Fact-Checking: A Survey of Resources and Approaches.
\newblock arXiv:2305.16859.

\bibitem[{Wang, Lipton, and Tsvetkov(2020)}]{wang-etal-2020-negative}
Wang, Z.; Lipton, Z.~C.; and Tsvetkov, Y. 2020.
\newblock On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 4438--4450. Online: Association for Computational Linguistics.

\bibitem[{Xu et~al.(2023)Xu, Hou, Che, and Zhang}]{m-language-edit}
Xu, Y.; Hou, Y.; Che, W.; and Zhang, M. 2023.
\newblock Language Anisotropic Cross-Lingual Model Editing.
\newblock arXiv:2205.12677.

\bibitem[{Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong et~al.}]{zhao2023survey}
Zhao, W.~X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et~al. 2023.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}.

\bibitem[{Zhen et~al.(2022)Zhen, Shang, Liu, Li, Chen, and Zhang}]{zhen2022survey}
Zhen, C.; Shang, Y.; Liu, X.; Li, Y.; Chen, Y.; and Zhang, D. 2022.
\newblock A survey on knowledge-enhanced pre-trained language models.
\newblock \emph{arXiv preprint arXiv:2212.13428}.

\bibitem[{Zhou et~al.(2023)Zhou, Li, Li, Yu, Liu, Wang, Zhang, Ji, Yan, He et~al.}]{zhou2023comprehensive}
Zhou, C.; Li, Q.; Li, C.; Yu, J.; Liu, Y.; Wang, G.; Zhang, K.; Ji, C.; Yan, Q.; He, L.; et~al. 2023.
\newblock A comprehensive survey on pretrained foundation models: A history from bert to chatgpt.
\newblock \emph{arXiv preprint arXiv:2302.09419}.

\end{thebibliography}
