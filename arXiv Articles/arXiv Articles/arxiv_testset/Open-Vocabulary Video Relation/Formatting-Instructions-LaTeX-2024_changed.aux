\relax 
\bibstyle{aaai24}
\citation{kong2022human}
\citation{9062498}
\citation{chen2019deep}
\citation{chen2019semantic,wang2021visual}
\citation{song2023relation}
\citation{Shang2017VideoVR,shang2019annotating}
\citation{Shang2017VideoVR}
\citation{genome}
\providecommand \oddpage@label [2]{}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:motivation}{{1}{1}{Open-vocabulary Video Relation Extraction enables a contextual-level comprehension of video content, bridging the gap between general action classification and precise language description.}{}{}}
\citation{Shang2017VideoVR}
\citation{9609554}
\citation{Kay2017TheKH,8237884,monfortmoments}
\citation{Hei2015anet,liu2022fineaction}
\citation{chen2019deep}
\citation{qian2023locate}
\citation{song2021spatial}
\citation{Shang2017VideoVR}
\citation{shang2019annotating}
\citation{gao2023compositional}
\citation{genome}
\citation{yatskar2016situation,sadhu2021visual}
\citation{li2022hake,gkioxari2018detecting}
\citation{10.1145/3343031.3351058}
\citation{gao2021video}
\citation{zheng2022vrdformer}
\citation{zhang2019reconstruct,tang2021clip4caption,lin2022swinbert}
\citation{wang2022git,xu2023mplug2,chen2023valor}
\newlabel{fig:compare_anno}{{2}{2}{Two \texttt  {falling} videos depicted in VidVRD and OVRE diverse in (a) salient objects interconnected throughout frames are exhaustively annotated with diverse relations in VidVRD; (b) OVRE builds a relation graph with various actors and relations that closely related to the falling action. }{}{}}
\citation{Kay2017TheKH}
\citation{zhang2021videolt}
\citation{7780940}
\citation{wang2019vatex}
\citation{Shang2017VideoVR}
\citation{shang2019annotating}
\citation{genome}
\citation{sadhu2021visual}
\citation{gao2021simcse}
\newlabel{tab:dataset_comp}{{1}{3}{Representative video understanding datasets.}{}{}}
\newlabel{fig:freq}{{3}{3}{Counts of top 25 relationships in Moments-OVRE.}{}{}}
\citation{9609554}
\newlabel{fig:sanki}{{4}{4}{A weighted bipartite mapping of the top 12 most frequent relations in \texttt  {eating} videos.}{}{}}
\citation{radford2021learning}
\citation{ni2022expanding,tang2021clip4caption,luo2022clip4clip}
\citation{dosovitskiy2020image}
\citation{yan2023videococa}
\citation{radford2019language}
\citation{cabot2021rebel}
\citation{Ekin2019randaugment}
\newlabel{fig:arch}{{5}{5}{Overview of our model architecture, enabling simple relation generation with the powerful vision-language pre-trained model CLIP-ViT as the video encoder and the large language model GPT-2 as the text decoder. }{}{}}
\citation{mokady2021clipcap}
\citation{wang2022git}
\citation{Rasheed2023vificlip}
\citation{zhong2021regionclip}
\citation{han2016seqnms}
\newlabel{tab:main_res}{{2}{6}{Baseline comparison on Moments-OVRE.}{}{}}
\newlabel{tab:abl_ft}{{3}{6}{Study on vision and language model fine-tuning. The {\fontfamily  {pzd}\fontencoding  {U}\fontseries  {m}\fontshape  {n}\selectfont  \char 51} mark means fine-tuning the corresponding module.}{}{}}
\newlabel{tab:abl_feat}{{4}{6}{Comparisons with different visual features (w/o fine-tuning visual encoder).}{}{}}
\newlabel{fig:abl_qnumber}{{6}{6}{Impact of the query numbers on the performance. For each query number, we report CIDEr (blue) and METEOR scores (red) over the Moments-OVRE test set. }{}{}}
\citation{yang2018commonsense}
\citation{li2023knowledge,li2018deep}
\bibdata{aaai24}
\newlabel{fig:visualize}{{7}{7}{Comparisons of triplets generation across diverse OVRE methods. The illustration highlights accurately described triplets in green, triplets with semantic correlation in blue, and irrelevant triplets in red. }{}{}}
\gdef \@abspage@last{8}
