@misc{mgpt,
 doi = {10.48550/ARXIV.2204.07580},
 
 url = {https://arxiv.org/abs/2204.07580},
 
 author = {Shliazhko, Oleh and Fenogenova, Alena and Tikhonova, Maria and Mikhailov, Vladislav and Kozlova, Anastasia and Shavrina, Tatiana},
 
 keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2; I.2.7, 68-06, 68-04, 68T50, 68T01},
 
 title = {mGPT: Few-Shot Learners Go Multilingual},
 
 publisher = {arXiv},
 
 year = {2022},
 
 copyright = {Creative Commons Attribution 4.0 International}
}
@article{mbert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
@inproceedings{dai2022kn,
  author    = {Damai Dai and
               Li Dong and
               Yaru Hao and
               Zhifang Sui and
               Baobao Chang and
               Furu Wei},
  title     = {Knowledge Neurons in Pretrained Transformers},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational
               Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
               May 22-27, 2022},
  pages     = {8493--8502},
  year      = {2022},
}
@article{meng2022locating,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2022}
}
@misc{enguehard2023sequential,
      title={Sequential Integrated Gradients: a simple but effective method for explaining language models}, 
      author={Joseph Enguehard},
      year={2023},
      eprint={2305.15853},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{key_value,
      title={Transformer Feed-Forward Layers Are Key-Value Memories}, 
      author={Mor Geva and Roei Schuster and Jonathan Berant and Omer Levy},
      year={2021},
      eprint={2012.14913},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{mlama,
      title={Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models}, 
      author={Nora Kassner and Philipp Dufter and Hinrich Schütze},
      year={2021},
      eprint={2102.00894},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{fill-in-the-blank,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1250",
    doi = "10.18653/v1/D19-1250",
    pages = "2463--2473",
    abstract = "",
}
@misc{ig,
      title={Axiomatic Attribution for Deep Networks}, 
      author={Mukund Sundararajan and Ankur Taly and Qiqi Yan},
      year={2017},
      eprint={1703.01365},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{wang-etal-2020-negative,
    title = "On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment",
    author = "Wang, Zirui  and
      Lipton, Zachary C.  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.359",
    doi = "10.18653/v1/2020.emnlp-main.359",
    pages = "4438--4450",
    abstract = "",
}
@misc{chatgpt_survey,
      title={A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions}, 
      author={Yuntao Wang and Yanghe Pan and Miao Yan and Zhou Su and Tom H. Luan},
      year={2023},
      eprint={2305.18339},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}
@article{
degenerate_biological,
author = {Giulio Tononi  and Olaf Sporns  and Gerald M. Edelman },
title = {Measures of degeneracy and redundancy in biological networks},
journal = {Proceedings of the National Academy of Sciences},
volume = {96},
number = {6},
pages = {3257-3262},
year = {1999},
doi = {10.1073/pnas.96.6.3257},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.96.6.3257},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.96.6.3257},
abstract = {}}
@article{mason2015degeneracy,
  title={Degeneracy: Demystifying and destigmatizing a core concept in systems biology},
  author={Mason, Paul H},
  journal={Complexity},
  volume={20},
  number={3},
  pages={12--21},
  year={2015},
  publisher={Wiley Online Library}
}

@inproceedings{lama2,
  title={How Context Affects Language Models' Factual Predictions},
  author={Fabio Petroni and Patrick Lewis and Aleksandra Piktus and Tim Rockt{\"a}schel and Yuxiang Wu and Alexander H. Miller and Sebastian Riedel},
  booktitle={Automated Knowledge Base Construction},
  year={2020},
  url={https://openreview.net/forum?id=025X0zPfn}
}

@misc{m-language-edit,
      title={Language Anisotropic Cross-Lingual Model Editing}, 
      author={Yang Xu and Yutai Hou and Wanxiang Che and Min Zhang},
      year={2023},
      eprint={2205.12677},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{fact-checking-survey,
      title={Scientific Fact-Checking: A Survey of Resources and Approaches}, 
      author={Juraj Vladika and Florian Matthes},
      year={2023},
      eprint={2305.16859},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{transparent,
  title={The Effective coalitions of Shapley value For Integrated Gradients},
  author={Liu, ShuYang and Fan, Changjie and Xiong, Yu and Wang, Meng and Hu, Yujing and Lv, Tangjie and Chen, Zixuan and Wu, Runze and Gao, Yang},
  year={2022}
}
@misc{DIG,
      title={Discretized Integrated Gradients for Explaining Language Models}, 
      author={Soumya Sanyal and Xiang Ren},
      year={2021},
      eprint={2108.13654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{meng2022memit,
  title={Mass Editing Memory in a Transformer},
  author={Kevin Meng and Sen Sharma, Arnab and Alex Andonian and Yonatan Belinkov and David Bau},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}
@inproceedings{mend,
    title={Fast Model Editing at Scale},
    author={Eric Mitchell and Charles Lin and Antoine Bosselut and Chelsea Finn and Christopher D Manning},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/pdf?id=0DcZxeWfOPt}
}
@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{knowledge_know_fact,
      title={How Can We Know What Language Models Know?}, 
      author={Zhengbao Jiang and Frank F. Xu and Jun Araki and Graham Neubig},
      year={2020},
      eprint={1911.12543},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{human_brain,
  title={Deep neural networks: a new framework for modeling biological vision and brain information processing},
  author={Kriegeskorte, Nikolaus},
  journal={Annual review of vision science},
  volume={1},
  pages={417--446},
  year={2015},
  publisher={Annual Reviews}
}
@article{cross_lingual_word,
    author = {Agirre, Eneko},
    title = "{Cross-Lingual Word Embeddings}",
    journal = {Computational Linguistics},
    volume = {46},
    number = {1},
    pages = {245-248},
    year = {2020},
    month = {03},
    issn = {0891-2017},
    doi = {10.1162/coli_r_00372},
    url = {https://doi.org/10.1162/coli\_r\_00372},
    eprint = {https://direct.mit.edu/coli/article-pdf/46/1/245/1847767/coli\_r\_00372.pdf},
}
@misc{knowledge-base,
      title={Language Models as Knowledge Bases?}, 
      author={Fabio Petroni and Tim Rocktäschel and Patrick Lewis and Anton Bakhtin and Yuxiang Wu and Alexander H. Miller and Sebastian Riedel},
      year={2019},
      eprint={1909.01066},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{human-brain1,
  title={Where do you know what you know? The representation of semantic knowledge in the human brain},
  author={Patterson, Karalyn and Nestor, Peter J. and Rogers, Timothy T.},
  journal={Nature reviews neuroscience},
  volume={8},
  number={12},
  pages={976--987},
  year={2007},
  publisher={Nature Publishing Group}
}
@article{huamn-brain2,
  title={Two forms of knowledge representations in the human brain},
  author={Wang, Xiaoying and et al.},
  journal={Neuron},
  volume={107},
  number={2},
  pages={383--393},
  year={2020},
  publisher={Cell Press}
}
@misc{Robustness ,
      title={Towards Deep Learning Models Resistant to Adversarial Attacks}, 
      author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
      year={2019},
      eprint={1706.06083},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{hallucination,
	doi = {10.1145/3571730},
  
	url = {https://doi.org/10.1145%2F3571730},
  
	year = 2023,
	month = {mar},
  
	publisher = {Association for Computing Machinery ({ACM})},
  
	volume = {55},
  
	number = {12},
  
	pages = {1--38},
  
	author = {Ziwei Ji and Nayeon Lee and Rita Frieske and Tiezheng Yu and Dan Su and Yan Xu and Etsuko Ishii and Ye Jin Bang and Andrea Madotto and Pascale Fung},
  
	title = {Survey of Hallucination in Natural Language Generation},
  
	journal = {{ACM} Computing Surveys}
}
@online{hallucination_chatgpt1,
  author       = {Benj Edwards},
  title        = {Why ChatGPT and Bing Chat are so good at making things up},
  year         = {2023},
  url          = {https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/},
  journal      = {Ars Technica},
  month        = {4},
  day          = {6},
  note         = {Retrieved 11 June 2023}
}
@online{hallucination_chatgpt2,
  author       = {Sofia Pitt},
  title        = {Google vs. ChatGPT: Here's what happened when I swapped services for a day},
  year         = {2022},
  url          = {https://www.cnbc.com/2022/12/15/google-vs-chatgpt-what-happened-when-i-swapped-services-for-a-day.html},
  journal      = {CNBC},
  month        = {12},
  day          = {15},
  note         = {Retrieved 30 December 2022}
}
@online{hallucination_chatgpt3,
  author       = {Benj Edwards},
  title        = {OpenAI invites everyone to test ChatGPT, a new AI-powered chatbot—with amusing results},
  year         = {2022},
  url          = {https://arstechnica.com/information-technology/2022/12/openai-invites-everyone-to-test-new-ai-powered-chatbot-with-amusing-results/},
  journal      = {Ars Technica},
  month        = {12},
  day          = {1},
  note         = {Retrieved 29 December 2022}
}
@misc{localization,
      title={Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models}, 
      author={Peter Hase and Mohit Bansal and Been Kim and Asma Ghandeharioun},
      year={2023},
      eprint={2301.04213},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{cao2023life,
      title={The Life Cycle of Knowledge in Big Language Models: A Survey}, 
      author={Boxi Cao and Hongyu Lin and Xianpei Han and Le Sun},
      year={2023},
      eprint={2303.07616},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{kandpal2023large,
  title={Large language models struggle to learn long-tail knowledge},
  author={Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={15696--15707},
  year={2023},
  organization={PMLR}
}

@article{zhen2022survey,
  title={A survey on knowledge-enhanced pre-trained language models},
  author={Zhen, Chaoqi and Shang, Yanlei and Liu, Xiangyu and Li, Yifei and Chen, Yong and Zhang, Dell},
  journal={arXiv preprint arXiv:2212.13428},
  year={2022}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@misc{kl,
      title={Language Models as Agent Models}, 
      author={Jacob Andreas},
      year={2022},
      eprint={2212.01681},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{rigorousIG,
  title={A rigorous study of integrated gradients method and extensions to internal neuron attributions},
  author={Lundstrom, Daniel D and Huang, Tianjian and Razaviyayn, Meisam},
  booktitle={International Conference on Machine Learning},
  pages={14485--14508},
  year={2022},
  organization={PMLR}
}

@misc{causal-inspired,
      title={How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis}, 
      author={Shaobo Li and Xiaoguang Li and Lifeng Shang and Zhenhua Dong and Chengjie Sun and Bingquan Liu and Zhenzhou Ji and Xin Jiang and Qun Liu},
      year={2022},
      eprint={2203.16747},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lakshmanan2022large,
  title={Why large language models like ChatGPT are bullshit artists},
  author={Lakshmanan, Lak},
  journal={becominghuman.ai},
  year={2022},
  note={Archived from the original on December 17, 2022},
  url={URL of the original article}
}
@article{metz2022new,
  title={The new chatbots could change the world. Can you trust them.},
  author={Metz, Cade},
  journal={The New York Times},
  volume={10},
  year={2022}
}
@incollection{ancona2019gradient,
  title={Gradient-based attribution methods},
  author={Ancona, Marco and others},
  booktitle={Explainable AI: Interpreting, explaining and visualizing deep learning},
  pages={169--191},
  year={2019}
}
@online{riemannsum_wikipedia,
    title = {Riemann sum},
    author = {{Wikipedia contributors}},
    year = {2023},  
    url = {https://en.wikipedia.org/wiki/Riemann_sum},
    note = {Accessed on: August 16, 2023}
}
@article{zhou2023comprehensive,
  title={A comprehensive survey on pretrained foundation models: A history from bert to chatgpt},
  author={Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and others},
  journal={arXiv preprint arXiv:2302.09419},
  year={2023}
}