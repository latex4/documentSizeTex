\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\def\year{2020}\relax
%File: formatting-instruction.tex
\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{graphicx}  % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}

% /Title ()
% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case.
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands,
% remove them.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{caption} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \natbib} -- This package is specifically forbidden -- use the following workaround:
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
%  -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
%  -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai20.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%
\setlength\titlebox{2.5in} % If your paper contains an overfull \vbox too high warning at the beginning of the document, use this
% command to correct it. You may not alter the value below 2.5 in
\title{A Knowledge Driven Approach to Adaptive Assistance Using Preference Reasoning and Explanation}
%Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\author{Jason R. Wilson\\Franklin \& Marshall College\\Lancaster, Pennsylvania\\jrw@fandm.edu \And Leilani Gilpin\\Massachusetts Institute of Technology\\Cambridge, Massachusetts\\lgilpin@mit.edu \And Irina Rabkina\\Occidental College\\Los Angeles, California\\irabkina@oxy.edu}
% \author{Jason R. Wilson,\textsuperscript{\rm 1} Leilani Gilpin,\textsuperscript{\rm 2} Irina Rabkina\textsuperscript{\rm 3} \\% All authors must be in the same font size and format. Use \Large and \textbf to achieve this result when breaking a line
% \textsuperscript{\rm 1}Franklin and Marshall College\\ %If you have multiple authors and multiple affiliations
% % use superscripts in text and roman font to identify them. For example, Sunil Issar,\textsuperscript{\rm 2} J. Scott Penberthy\textsuperscript{\rm 3} George Ferguson,\textsuperscript{\rm 4} Hans Guesgen\textsuperscript{\rm 5}. Note that the comma should be placed BEFORE the superscript for optimum readability
% TODO: address\
% \\
% jrw@fandm.edu % email address must be in roman text type, not monospace or sans serif
% \\
% \textsuperscript{\rm 2}Massachusetts Institute of Technology\\ %If you have multiple authors and multiple affiliations
% % use superscripts in text and roman font to identify them. For example, Sunil Issar,\textsuperscript{\rm 2} J. Scott Penberthy\textsuperscript{\rm 3} George Ferguson,\textsuperscript{\rm 4} Hans Guesgen\textsuperscript{\rm 5}. Note that the comma should be placed BEFORE the superscript for optimum readability
% TODO: address\
% \\
% lgilpin@mit.edu % email address must be in roman text type, not monospace or sans serif\\
% \\
% \textsuperscript{\rm 3}Occidental College\\ %If you have multiple authors and multiple affiliations
% % use superscripts in text and roman font to identify them. For example, Sunil Issar,\textsuperscript{\rm 2} J. Scott Penberthy\textsuperscript{\rm 3} George Ferguson,\textsuperscript{\rm 4} Hans Guesgen\textsuperscript{\rm 5}. Note that the comma should be placed BEFORE the superscript for optimum readability
% TODO: address\
% \\
% irabkina@oxy.edu % email address must be in roman text type, not monospace or sans serif
% }

\begin{document}

\maketitle

\begin{abstract}
% There is a need for socially assistive robots (SARs) to provide
% transparency in their behavior by explaining their reasoning and to have
% the reasoning and explanation represent the users preferences and
% goals.
There is a need for socially assistive robots (SARs) to provide
transparency in their behavior by explaining their reasoning. Additionally,
the reasoning and explanation should represent the users preferences and
goals.
To work towards satisfying this need for interpretable
reasoning and representations, we propose the robot uses Analogical
Theory of Mind to infer what the user is trying to do and uses the
Hint Engine to find an appropriate assistance based on what the user
is trying to do.  If the user is unsure or confused, the robot
provides the user with an explanation, generated by the Explanation
Synthesizer.  The explanation helps the user understand what the robot
inferred about the users preferences and why the robot decided to
provide the assistance it gave.  A knowledge-driven approach provides
transparency to reasoning about preferences, assistance, and
explanations, thereby facilitating the incorporation
of user feedback and allowing the robot to learn and adapt to the user.
\end{abstract}

\section{Introduction}
Socially assistive robots (SARs) can aid humans in a variety of tasks.
One of the most compelling assistive tasks is in medication
management, where a SAR can instruct, record, and oversee a patient's
medication usage.  However, since this is a medical application, it is
important that a robot is robust, transparent, and open to user
feedback; especially for corrections.

However, SARs, like other social robots, are complex to understand.  Robots are built of many parts, with underlying language tools (e.g., for NLP, NLU, or NLG) that are  not inherently interpretable.  Therefore, SARs cannot
effectively communicate and collaborate with humans on tasks without \emph{explainability}.  This is
troublesome when the robot fails, or when the assistive application is
critical, like healthcare or medical applications.  In this paper, we
make two distinct contributions towards explainable SARs: (1) we
contribute a complex cognitive model for incorporating user feedback,
and (2) we show a proof-of-concept on a real-life medication case
study.  Our approach combines three components:
\begin{enumerate}
\item Preference Reasoning: The robot considers what it knows or can infer about the user's preferences, and how these affect possible actions.
\item Assistance: The robot interacts with the user and aids
them if deemed necessary.
\item Explanation: The robot explains its reasoning and validates its
recommendation and conclusion with a user.
\end{enumerate}
In this paper, we show the capabilities of a SAR with knowledge-driven
adaptive assistance.  We start with a detailed overview of the
medication sorting task.  We present the approach and initial results,
and conclude with future work, discussion, and a reiteration of the
contributions.

\section{Task Description}
\label{sec:task-description}
We consider a SAR that provides social assistance in a medication sorting task.  In this task, a person organizes a set of medications, vitamins, and other supplements.  Each has constraints, provided in the form of a prescription, doctor recommendation, or personal preference.  In the example we use in this paper, there is a vitamin to be taken each morning and a medication that needs to be taken prior to any physical activity.

The role of the robot is to observe the person while they are organizing the pills into a sorting grid, a device that has compartments for each day of the week and multiple time of day (see Figure~\ref{fig:grid}).  Our example uses a sorting grid for four times in the day: morning, noon, evening, and bedtime.

\begin{figure}[hb]
    \centering
    \includegraphics[width=0.8\columnwidth]{grid.jpg}
    \caption{An example of a sorting grid.}
    \label{fig:grid}
\end{figure}

Consider the scenario in which a person is to take a vitamin each morning and then Levodopa before any physical activity.  The robot knows that the person has a physical therapy appointment at 1pm on Wednesday and a dance class at 6pm on Friday.  The person has a preference that Levodopa is taken enough time before the activity for it to take effect.

After the person has placed one vitamin in the morning compartment for each day, they begin to figure out where to place some Levodopa.  The person hesitates, and the robot interjects with a suggestion:

\begin{itemize}[left=30pt]
    \item[Robot:] Try placing a Levodopa pill in the morning on Wednesday.
    \item[User:] Why?
    \item[Robot:] Levodopa needs to be taken before any physical activity, and you have a physical therapy appointment at 1pm on Wednesday.  Since you prefer to take it a few hours before activity, you should take it in the morning.
    \item[User:] Oh, right.  Thank you.
\end{itemize}

Alternatively, consider the case in which the person prefers to take Levodopa closer to when the activity is to occur.  In this case, the robot would explain:

\begin{itemize}[left=30pt]
    \item[Robot:] A Levodopa pill needs to be taken before any physical activity, and you have a physical therapy appointment at 1pm on Wednesday.  Since you prefer to take it immediately before activity, you should take it in the afternoon.
\end{itemize}

While previous work demonstrated the robot adapting its assistance based on how much assistance the person needed \cite{wilson2020challenges}, the current work looks at how the robot can reason about the person's preferences, thus adapting its assistance and explanation.

% This doesn't belong here but holding onto it in case it is useful:
% Explanations can serve to help the user gain knowledge about ..., such as why a medication should be taken on a given day.  In this way, the explanation improves the interaction by aligning goals, communicating knowledge, supporting trust, blah, blah.  Additionally, the explanation will serve as an opportunity for the person to find errors in the robot's reasoning, thus providing an opportunity for the robot's knowledge be updated and ultimately provide better assistance going forward.


% \subsection{Working Example}
% This simple scenario is intended to demonstrate how a robot can provide assistance, infer a users preferences, provide an explanation, and get user feedback on the robots reasoning.  First, the scenario is described at a high-level as a dialogue between the robot and the user.  Then, a more detailed breakdown is provided.
% \bigskip

% \noindent
% Robot: Try placing a green pill in the morning on Wednesday\\
% User:Why?\\
% Robot:A green pill needs to be taken before any physical activity, and you have a physical therapy appointment at 1pm on Wednesday.\\
% User:But I want to take it with lunch on Wednesday\\
% Robot:Why?\\
% User:I like to take it closer to my appointment.\\
% Robot:Ok.  Wednesday lunch is good.  I will make note of this.\\

% \noindent
% Then, later in the task a similar scenario occurs where the person needs to take a pill before an activity on Friday.\\

% \noindent
% Robot: Try placing a green pill in the afternoon on Friday.\\
% User: Why?\\
% Robot: A green pill needs to be taken before any physical activity, and you have a physical therapy appointment at 1pm on Friday.\\
% User: Oh, yes.  Thank you. \\

% TODO: more explanation

% \subsection{Preference}
% In the first part of the scenario, the robot does not yet know of any user preference.  In this case, either it provides some preference based on other people and scenarios or provides not preference information.  If no preference information is provided, other components need to be able to use some default.


% TODO: merge with content under Approach

% % TODO LHG Clean up.
% \subsection{Explanation}
% When the user requests an explanation, the robot explains why it
% provided the assistance it did.  The HTN trace and the preconditions
% of the action (and maybe its parents) can hopefully be used to produce
% an explanation.  In the example scenario, the text is mostly based on
% the action preconditions, though not all the antecedents are relevant
% for the explanation.  For our task, the activity related antecedents are
% particularly relevant.

% In this paper, we show the explanations in symbolic triples.  A simple
% template-based NLG can be used to convert a symbolic explanation to
% natural language.  This is not the focus of our work but something
% simple will eventually need to be in place.

% TODO: merge with content under Approach

% \subsection{Feedback}
% After the robot gives an explanation, the user may provide feedback.  This feedback, which will probably come in the form of natural language, needs to be (somehow) converted into a symbolic representation.

% In the example given, the user is expressing a preference (i.e., I want to.., I like to).  Feedback of this type needs to then be passed on to the component reasoning about preferences.  Perhaps also interesting here is that the explanation given in the first part of the scenario did not include any information about preferences.  This could be helpful in figuring out what the feedback is in reference to and what is missing from the explanation.

% TODO: clean up

\section{Approach}

%\subsection{Architecture Overview}

For the social robot to provide assistance and explanations to the user, we propose an
architecture consisting of Preference Reasoning, Adaptive Assistance, and Explanation
components, as shown in Figure~\ref{fig:arch}.  The Adaptive Assistance component uses updates in the task and social cues conveyed by the user to generate a plan, which is used in determining what action the robot is to take to assist the user.  The plan would be sent to the Explanation component, along with the user preferences from the Preference Reasoning component, to generate an explanation for the robot's method of assistance. Each component is functional individually, but full integration is a work in progress

\begin{figure}[h]
\centering
\includegraphics[width=1.0\columnwidth]{architecture.png}
\caption{User preferences are sent to the Adaptive Assistance and Explanation components.  The Adaptive Assistance component uses the preference in generating a plan for completing the task, and the Explanation component uses the preference and plan to explain the robot's assistance.}
\label{fig:arch}
\end{figure}

\subsection{Preference Reasoning}
% start by discussing preferences, representations, etc. then move into AToM
In the context of pill sorting, user preferences can take two forms: preferences about how to sort specific pills (e.g., take Levodopa directly before an activity vs. several hours before) and preferences about the sorting task as a whole (e.g., sort all of one type of pill for the week vs. each day in order). These preferences take the form:
 \small{
 \begin{verbatim}
(prefers user
    (medicationBeforeActivityBy
        medtype
        distance))
 \end{verbatim}}
 \noindent
 and

 \small{
 \begin{verbatim}(prefers user (sortOrder order))
  \end{verbatim}}
 %removedVspace
 \noindent
 respectively.

 Note that all preferences use the \texttt{prefers} predicate and take the user as the first argument. This representation allows us to generate preferences through both inference and user input. More importantly, it allows us to use the preferences for further reasoning, including in the Adaptive Assistance and Explanation components.

 In the present work, we assume that preferences are given by the user. This may be in the form of correcting the robot (e.g., "No, don't put that pill in the morning. I want to take it in the afternoon") or stated out right (e.g., "Let's start by sorting the green pills."). However, such preferences can also be inferred. We are working on integrating the Analogical Theory of Mind (AToM) ~\cite{rabkina2017towards} model into the architecture to do so.

 AToM is a computational cognitive model of the processes by which people learn to reason about others' preferences, goals, beliefs, desires, etc. (called theory of mind reasoning). It has successfully modeled children's learning in two developmental studies ~\cite{rabkina2017towards,rabkina2018bootstrapping} and has previously been used to recognize the goals and intentions of simulated agents in multiagent interactions ~\cite{rabkina2019analogical,rabkina2020acs}.

 We plan to use AToM because its reasoning is human-like, and therefore easy for people to understand. Furthermore, AToM can learn from just a handful of examples and can incorporate user feedback to improve its reasoning on the fly.

\subsection{Adaptive Assistance}

The Adaptive Assistance component uses a Hint Engine to generate an appropriate assistance to help the user complete the task~\cite{wilson2018general}.  The Hint Engine integrates information from three models (need, assistance, and domain) to determine how and when the robot should assist \cite{wilson2019developing}.  The need model is used to infer how much assistance the person needs based on progress in the task and social cues (e.g., verbal requests, eye gaze patterns).  The assistance model represents the relations between the types of actions the user can take to complete the task, the types of actions the robot can take to assist, and the amount of assistance provided in any robot action.

The domain model represents the task that the user is performing and is used to infer a plan, the actions the user can take to complete the task.  When information from the need model indicates that the user has a sufficient level of need, the Hint Engine generates a plan for completing the task.  The first action in the plan indicates where the robot should focus its assistance.  Based on the action type and how much assistance the user needs, the Hint Engine uses the assistance model to determine the appropriate assistive action for the robot to perform.

To generate a plan, the Hint Engine uses its domain model, which is represented with a hierarchical task network (HTN) \cite{nau1999shop}.  The HTN used for medication sorting is shown in Figure~\ref{fig:htn}.  At the top level, the user is working toward sorting the pills for all of the medications.  To complete the task, the user needs to go through each medication, sorting the pills for each. For a given medication, a user may go through the days of the week, adding and removing pills as they go.  If no mistakes have been made, the user is just missing pills (leading to \texttt{addPill} actions).  If a pill has been placed at the wrong time of the day, then the action pair \texttt{removePill} and \texttt{addPill} are included in the plan to indicate the moving of a pill.

\begin{figure}[h]
\centering
\includegraphics[width=0.99\columnwidth]{medsorting_htn.png}
\caption{Hierarchical task network representing a medication sorting task. It assumes task is done by sorting all pills of one medication before doing the next one. White boxes are tasks, and shaded boxes are operators.  }
\label{fig:htn}
\end{figure}

To determine whether there is a missing pill, an extra pill, or a pill placed at a wrong time, the planner % maybe mention the pyhop planner being used?
checks the preconditions of the method, which includes the constraints defined for the given medication.  For example, each medication defines the maximum number to be taken in a day.  If the number of pills for that medication exceed the maximum, allowed, then the \texttt{extraPill} condition is satisfied.

We extend the precondition checks to also consider user preferences.
For example, a vitamin could be taken at any time, but a user may prefer to take it in the morning.  Similarly, a medication like Levodopa might be taken before physical activity, and the user may have preferences regarding how long before the activity.  In this case, there is a constraint of the form \texttt{(beforeActivity pill row col activity)} that can be inferred with a rule like the following:

\small{
\begin{verbatim}
(beforeActivity pill row col activity) <-
    (activityAt activity rowX col)
    (isa pill med)
    (medicationBeforeActivityBy med
        distance)
    (difference rowX row distance)
\end{verbatim}}

In this example, the HTN represents blocks of time as they relate to the sorting grid (i.e., morning, noon, etc.) and not specific times in the day.


% Old text begins below.  Willie still needs to work on integrating in this text, cleaning it up, etc.


% \subsection{Assistance}
% After each user action (and sometimes inaction or social cue), an assistance is generated based on what the user needs to do next.  If the robot thinks the next step to take is to place a pill on Wednesday morning, then it will give some suggestion to do so.  The internal representation for the action is something like (addPill 3 0), where  Wednesday is the third column and morning is the top row of a sorting grid.

% Since an HTN is used to determine which action to take next, each action has associated to it all of the parent tasks and methods used to infer each action.  For the example scenario, the robot suggests adding a pill because one is missing.  The trace for (addPill 3 0) is something like addPill, missingPill, fixDay, oneDayAtATime, sortPill, sortMeds (pardon my switch from snake to camel case).

% Additionally, when checking the preconditions for addPill, it will need to lookup the constraints for the green pill and verify each of them.  In this case, there is one constraint of the form (beforeActivity pill row col activity).  This could perhaps be inferred from a rule like the following:
% \\

% {\fontfamily{pcr}\selectfont
% (beforeActivity pill row col activity) <-
% 	(activityAt activity rowX col)
% 	(isa pill med)
% 	(medicationBeforeActivityBy med distance)
% 	(difference rowX row distance)}

% TODO: clean up

% TODO: htn figure


% LHG - I can put more explanations here, but it's already rather long.
% JRW - I think a little more detail would be good.  For example, how is the plan get used?  This will help us tie together these 2 components
\subsection{Explanation}
The explanations are generated from an existing
system \cite{gilpin2018monitoring,leilanithesis} that incorporates commonsense
knowledge, rules, and constraint checking towards an explanation of
intended behavior.  The Explanation Synthesizer proceeds in 3 steps:
\begin{enumerate}
\item Parsing and aggregation: The input query is parsed for key \emph{concepts}.  Those
concepts are used as search terms in the commonsense knowledge base.  A list of facts (symbolic triples) is returned.
\item Constraint Checking: Commonsense rules are triggered to generate
new facts and evidence.
\item Synthesizing: Once all the facts are aggregated, an explanation
synthesizer constructs the most plausible chain of reasoning
towards an explanation.
\end{enumerate}

For example, consider the query \texttt{(pill onDate Friday)} which justifies that the user can take the pill on Friday.  The query is parsed and
the key concepts are \texttt{pill} and \texttt{Friday}, which are search terms for
the commonsense knowledge base (KB).  The KB returns facts like
\texttt{(Friday IsA 'business day')}.  The relation \texttt{onDate} is
used as a constraint in the system.

The constraints are a combination of commonsense rules and user
preferences.  These constraints are application dependent.   In the
medication sorting domain, they are rules related to the requirements
for each type of pill, as well as user preferences.  For example, the
user may prefer to take pills in the morning, or users may be
\emph{instructed} to ingest pills with meals or food.  The facts are
forward chained against these rules to generate new facts and
evidence.
% TODO Mention reasons

After this process, there may be more than one plausible explanation
supporting the query.  The explanation synthesizer starts from the
query and constructs a goal tree to satisfy the query
\cite{leilanithesis}.  For this paper, we only examine one
explanation.  Choosing the best explanation may be explored in future
work.

% For example, consider the motivating example where the explanation generator is queried for: "Why take
% the pill on Wednesday", which is represented as \texttt{(pill onDate Wednesday)}.  The explanation generator is also provided
% with a \emph{preference}; the user prefers to take the medication before
% an activity.

% % \begin{figure}[htbp]
% % \small{
% % \begin{verbatim}
% % [(pill IsA obj), 'Default anchor point']
% % [(pill AtLocation cabinet), 'ConceptNet']

% % [(Wednesday IsA 'The fourth day of the week'),
% %     'ConceptNet']
% % [(Wednesday IsA weekday), 'ConceptNet']
% % [(Wednesday IsA 'business day'), 'ConceptNet']
% % [(Wednesday IsA day), 'ConceptNet']

% % [(user prefers (pill before activity)),
% %     'Given preference']
% % [(appt IsA activity), 'Given knowledge']
% % [(appt atTime '1pm'),
% %     'Given knowledge from calendar']
% % [(appt onDay Wednesday),
% %     'Given knowledge from calendar']

% % [(appt atTime afternoon),
% %     'Rule triggered: commonsense']
% % \end{verbatim}}
% % \caption{The facts and rules that are triggered in the explanation generation.}
% % \label{code:explantion}
% % \end{figure}
% \small{
% \begin{verbatim}
% [(pill AtLocation cabinet), 'ConceptNet']
% ...
% [(Wednesday IsA weekday), 'ConceptNet']
% [(Wednesday IsA day), 'ConceptNet']
% ...

% [(user prefers (pill before activity)),
%     'Given preference']
% [(appt IsA activity), 'Given knowledge']
% [(appt atTime '1pm'),
%     'Given knowledge from calendar']
% [(appt onDay Wednesday),
%     'Given knowledge from calendar']
% [(appt atTime afternoon),
%     'Rule triggered: commonsense']
% \end{verbatim}}


% The justification is that \texttt{(pill onDay Wednesday) (pill beforeTime
% afternoon)}.  And the final explanation reads in a series of symbolic
% triples: \texttt{(user prefers (pill before activity)) (appt IsA activity) (appt atTime
% '1pm') (appt onDay Wednesday) ('1pm' IsA afternoon)}.

% Once this explanation is provided to the user, the user may provide
% feedback.  An explanation synthesizer extracts key terms from the
% feedback, interprets the feedback, and determines which component(s)
% need adjustment.  The explanation synthesizer also \emph{validates} its
% conclusion by verifying its conclusion with the user.

\section{Proof of Concept}

% From assistance

To demonstrate our components adapting to and explaining with user preferences, we consider the scenario in which the user has already correctly placed all of their Vitamin D and is now working to sort their Levodopa, which is to be taken before activity.  The user has two activities planned for the week, Wednesday at 1pm and Friday at 8pm.  The user just placed one Levodopa pill in the space for Monday midday.

We assume that the user has a previously stated preference to take Levodopa during time slot prior to an activity.  When the user misplaces the Levodopa, the Adaptive Assistance component recognizes that the user needs assistance and generates a plan to
move the Monday pill to an earlier time slot:

\small{
\begin{verbatim}
(planFor state8
  ((preference beforeActivity 1))
  ((removePill Levodopa 3 1)
   (addPill Levodopa 3 0)
   (addPill Levodopa 5 2)))
\end{verbatim}}

The first action, \texttt{(removePill Levodopa 3 1)}, is used along with an inference of a level of assistance to determine that the robot should provide direct assistance, which has the robot clearly stating what should be done next.  In this case, the pill needs to be removed and the robot would say ``Try removing a Levodopa from Wednesday''.

Additionally, the Adaptive Assistance component considers alternative plans, a counterfactual reasoning over different preferences.  The alternative plans do not affect how the robot assists but would be sent to the Explanation component.  An alternative plan for taking Levodopa at the same time as the activity is shown below.
The plan indicates that the Monday pill is in the correct location and the only action remaining is to place the Friday pill:

\small{
\begin{verbatim}
(alternativePlanFor state8
  ((preference beforeActivity 0))
  ((addPill Levodopa 5 3)))
\end{verbatim}}


%\textbf{IR: we need to smooth this transition (sidebar: is "to smooth" a verb?)}
Finally, the user can inquire about the robot's actions.  For example, the user can ask why the robot said to ``Try removing a Levodopa from Wednesday.''  This question is parsed into an intermediate representation: \texttt{(onDate Levodopa Wednesday)}, which is passed to the Explanation Synthesizer along with the associated \emph{preference}; the user prefers to take the medication before an activity.
The following is a trace of the reasoning of the Explanation Synthesizer

\small{
\begin{verbatim}
[(IsA Levodopa pill), 'Given']
[(AtLocation pill cabinet), 'ConceptNet']
...
[(IsA Wednesday weekday), 'ConceptNet']
[(IsA Wednesday day), 'ConceptNet']
...
[(prefers user (before pill activity)),
    'Given preference']
[(IsA appt activity), 'Given knowledge']
[(atTime appt '1pm'), 'calendar']
[(onDay appt Wednesday), 'calendar']
[(atTime appt afternoon), 'Rule fired']
\end{verbatim}}

The justification is that \texttt{(onDay pill Wednesday) (beforeTime pill
afternoon)}.  And the final explanation reads in a series of symbolic
triples: \texttt{(prefers user (before pill activity)) (IsA user activity) (atTime appt '1pm') (onDay appt Wednesday) (IsA '1pm' afternoon)}.

\section{Future Work}
% clear next steps: combine pieces, really test it out, get feedback working
% but really setting foundation for a whole bunch of future work
% Willie TODO
The work we describe here sets the foundation for a whole line of work in
designing social robots to adapt to users, adhere to user preferences, and
provide explanations.  The most immediate next step is to build upon the
proof of concept we have demonstrated here by integrating the individual
parts and evaluating the system on more complex scenarios.

Once we have a fully integrated system, the critical next step is to
incorporate feedback from the user.
One of the greatest advantages of taking a knowledge-driven approach is
that the entire system is inspectable, which will facilitate integrating
the user feedback.
The user may provide feedback after the robot provides an explanation to
the user.  An explanation synthesizer extracts key terms from the
feedback, interprets the feedback, and determines which component(s)
need adjustment.  The explanation synthesizer also \emph{validates} its
conclusion by verifying with the user.

In this ongoing work, if the explanation synthesizer identifies that the feedback is related
to the user preferences, the Preference Reasoner will construct a new
case that is used by AToM to update the model of the user.  Even a single
piece of feedback from the user can be sufficient for AToM to learn the
user's preferences because analogical learning, which is used by AToM, is
data efficient and capable of learning from only a few examples
\cite{chen2019human,wilson2019analogical}.

% We are exploring other tasks with which a user could receive
% social assistance from a robot.  Keeping within the health domain, medication
% reminding \cite{x} would be a natural transition, and exercise assistance \cite{x}
% would also be useful.  Tasks such as Sudoku \cite{x} and furniture assembly \cite{x}
% are interesting tasks as they often require assistance, may have preferred approaches,
% and explanations can be vital to understanding the assistance.
% not sure if we really need to list any of the above examples of task, but i do have citations for all of them

\section{Related Work}
Consideration of \textit{user preferences} is an important aspect of human-robot interaction, as it allows the robot to modify its behaviors according to its understanding of the user. Hiatt, Harrison, and Trafton (\citeyear{hiatt2011accommodating}) found that people prefer collaborating with robots that adapt their behaviors in this way. However, most approaches to recognizing users' preferences use statistical techniques like reinforcement learning \cite{woodworth2018preference} and Markov Decision Processes \cite{munzer2017preference} to predict preferences. This means that they require large amounts of training data, are not responsive to user feedback, and are not explainable. That is, once trained, such systems predict preferences based on their built-up statistical models; a user cannot state a preference directly or inspect why the robot predicts a particular preference. By incorporating stated user preferences, and moving toward learning preferences by analogy, we attempt to avoid these pitfalls.

There are many forms of \textit{adaptive assistance} in robotics.  One approach is
shared autonomy, in which the system infers human intentions and adapts how
much assistance in provided in controlling a robot \cite{Nikolaidis2017,Jain2019}.
This work is focused on assisting people in physically controlling robots,
whereas we are working towards an autonomous robot that provides social assistance.

Other work has looked at adapting a robot's behavior based on user preferences.
%Instead of actions or intents, a robot can learn user preferences that are used
%to adapt the robot's behavior.
For example, a recursive neural network was used
to learn weights pertaining to user preferences, which influence the plans for
a robot \cite{Bacciu2014}.  While the preferences did affect the plans used by a robot,
the plans are used to improve the
robot's navigation.  Thus, the user preferences do not relate to assistance
provided to the user.

A model of Theory of Mind (ToM) has been proposed to adapt the assistance provided
by a social robot \cite{Gorur2017}.  A stochastic model is used to infer what action a person could
be executing.  Based on this estimate, they generate a plan to determine with which
action the robot should help.
While they use ToM to estimate a user's intent (via a set of possible actions),
they do not represent a user's preference for how the task should be completed.
% they sorta do have a learned preference for whether to help or not.  get back to that later.

% Adaptive planning approaches.
%   Robot planning that takes explicit user preferences, adjust weights, finds plan.
% Adaptive planning approaches that explicitly consider ToM.
%   Try not to overlap with previous subsection.
% Many forms of adaptive assistance relate to physical assistance.
%   This includes things like robots with shared autonomy, where the robot and user adapt to each other while working towards some common goal.

One way to understand complex decision making systems is with
\emph{interpretable} or \emph{explainable} parts.  Explanations can describe
proxy methods \cite{why-trust,grad-cam,visualizing}, representations
\cite{netdissect2017,cavs}, or be inherently explanation-producing
\cite{multimodal}.
% These methods have been reviewed to create a
% taxonomy of explanations \cite{explaining-explanations}.
In the context
of human-robot interaction, explanations can help to communicate and
build trust \cite{wang2016trust}, justify the robot's actions
\cite{stange2020effects} or motions \cite{dragan2013legibility}, or
describe \emph{unreasonable} perceptions \cite{gilpin-hri}.  But most of
these explanations are generated \emph{after-the-fact} and cannot be used
to improve the completion of tasks moving forward.

We propose to use explanations as \emph{feedback} to augment assistive
robots.  This has been explored for agents playing games, especially
\emph{when} to provide explanations \cite{li2020reasoning}.  This approach
builds on Rainbow, a self-adaptive system that can correct itself and
reuse the same baseline framework \cite{rainbow}.  To our knowledge,
this is the first work to propose a knowledge-driven architecture that could use explanations to \emph{improve} robotic reasoning and inference.
\section{Contributions}
% For socially assistive robots (SARs) to be \emph{trusted} HRI tasks, they need to be able to receive and process user feedback.

In this paper, we motivate a knowledge-driven architecture for adaptive assistance. We demonstrate the functionality of the components of this architecture in a task for socially assistive robots (SARs). In future work, we will expand the architecture to incorporate and process feedback and learn user preferences. This paper opens a new area of research in adaptable and interpretable SARs.
% LHG - Tried to reframe that here
%explanatory feedback.
% \textbf{JRW: We need to update this and maybe minimize the feedback or reframe it since we don't actually demo any feedback.}

\bibliographystyle{aaai}
Molestias unde ut totam quaerat eum numquam maxime consequatur aperiam similique fugit, sunt rem tempora architecto dicta minima ipsum magnam adipisci accusamus vero eos, dolorum porro deserunt excepturi asperiores blanditiis, numquam nulla deleniti dicta perspiciatis.Expedita ullam quas quos hic rerum dignissimos maiores, unde numquam sed nihil eligendi asperiores illo, blanditiis cupiditate tempore perferendis suscipit ab tenetur pariatur molestias nisi quaerat consequuntur, laudantium cumque eligendi rem doloremque optio animi quo quae, rem ullam totam nobis sint esse assumenda consectetur ex quasi unde vitae?Aut soluta consequatur aliquid officiis quas dicta eum inventore, porro molestias reprehenderit dolorum perspiciatis fugiat impedit assumenda aspernatur, possimus eum iste nisi amet dolor iusto?\clearpage
\bibliography{pgaef}
\end{document}