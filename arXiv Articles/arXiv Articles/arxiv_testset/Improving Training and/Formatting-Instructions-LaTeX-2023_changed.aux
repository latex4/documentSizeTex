\relax 
\bibstyle{aaai23}
\citation{wen2016discriminative,deng2019arcface,wang2018cosface,schroff2015facenet,liu2017sphereface}
\citation{shi2016deep,wang2020suppressing}
\citation{kalka2018ijb,maze2018iarpa,klare2015pushing}
\citation{shi2019probabilistic}
\citation{chang2020data}
\newlabel{sec:intro}{{1}{1}{}{}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:intro}{{1}{1}{Random Temperature Scaling models the data uncertainty via the scale of temperature (top). We find that the learned scale, $v(\bm  {x})$, is a good metric for out-of-distribution detection (bottom).}{}{}}
\citation{liang2018enhancing}
\citation{shafaei2019less}
\citation{hsu2020generalized}
\citation{techapanurak2019hyperparameter}
\citation{shafaei2019less}
\citation{gal2016dropout,theobald2021bayesian}
\citation{gal2016dropout,blundell2015weight,kendall2017uncertainties}
\citation{kendall2017uncertainties}
\citation{shi2019probabilistic}
\citation{chang2020data}
\citation{moosavi2017universal,nguyen2015deep}
\citation{liang2018enhancing}
\citation{hsu2020generalized}
\citation{techapanurak2019hyperparameter}
\citation{guo2017calibration,neumann18relaxed}
\citation{hinton2015distilling}
\citation{zhang2019adacos}
\citation{hinton2015distilling}
\citation{zhang2019adacos}
\citation{neumann18relaxed}
\newlabel{sec:related}{{2}{2}{}{}{}}
\newlabel{sec:method}{{3}{2}{}{}{}}
\newlabel{ssec:preliminary}{{3.1}{2}{}{}{}}
\newlabel{eq:softmax}{{1}{2}{}{}{}}
\citation{deng2019arcface,wang2018cosface}
\citation{techapanurak2019hyperparameter,neumann18relaxed}
\citation{chang2020data}
\citation{tolstikhin2018wasserstein}
\citation{kingma2014auto}
\newlabel{fig:temp-scaling}{{2}{3}{A probabilistic view of softmax and temperature scaling. The standard softmax function models the uncertainty of prediction by implicit Gumbel random variables. In temperature scaling, the temperature corresponds to the scale of uncertainty noise added to the classification score.}{}{}}
\newlabel{eq:cross_entropy}{{2}{3}{}{}{}}
\newlabel{eq:arcface}{{3}{3}{}{}{}}
\newlabel{ssec:rts}{{3.2}{3}{}{}{}}
\newlabel{alg:rts}{{1}{3}{Random Temperature Scaling (RTS)}{}{}}
\newlabel{eq:prob-softmax}{{4}{3}{}{}{}}
\newlabel{eq:prob-softmax-ts}{{5}{3}{}{}{}}
\citation{yi2014learning}
\citation{deepglint}
\citation{huang2008labeled}
\citation{sengupta2016frontal}
\citation{moschoglou2017agedb}
\citation{zheng2017cross}
\citation{zheng2018cross}
\citation{cao2018vggface2}
\citation{huang2008labeled}
\citation{li2017reliable}
\citation{he2016deep}
\citation{hu2018squeeze}
\citation{hsu2020generalized}
\citation{techapanurak2019hyperparameter,neumann18relaxed}
\citation{chang2020data}
\citation{deng2019arcface}
\citation{meng2021magface}
\citation{kim2022adaface}
\citation{deng2019arcface}
\citation{chang2020data}
\newlabel{fig:temp-dist}{{3}{4}{The distributions of $t$ with different $\delta $ and $v$.}{}{}}
\newlabel{fig:variation_uncertainty_curves}{{4}{4}{Variation of uncertainty score in RTS during the training process. Training data: DeepGlint.}{}{}}
\newlabel{ssec:objective}{{3.3}{4}{}{}{}}
\newlabel{eq:loss_kl}{{6}{4}{}{}{}}
\newlabel{sec:experiments}{{4}{4}{}{}{}}
\newlabel{ssec:details}{{4.1}{4}{}{}{}}
\newlabel{ssec:training phase}{{4.2}{4}{}{}{}}
\newlabel{fig:ood_dist}{{5}{5}{Distributions of magnitudes or uncertainty scores on the OOD dataset. From left to right, each subplot corresponds to the model trained with MagFace, AdaFace, GODIN, Relaxed Softmax, DUL and RTS respectively. LFW and RAFDB are considered as in-distribution data. Cat and dog are considered as out-of-distribution data. All the models are trained on DeepGlint. }{}{}}
\newlabel{fig:image-score}{{6}{5}{Images with different uncertainty scores estimated by RTS. }{}{}}
\newlabel{tab:image_noise}{{1}{5}{Results of recognition with image noise. Training data: CASIA-WebFace}{}{}}
\newlabel{tab:tnr_and_auc}{{2}{5}{Performance of different methods on OOD testset.}{}{}}
\newlabel{fig:noise_levels}{{7}{5}{Uncertainty scores vs. blur levels (Gaussian blur radius are 2, 3 and 5, respectively). See Appendix for distributions of other types of noise. Training data: DeepGlint. }{}{}}
\newlabel{ssec:testing phase}{{4.3}{5}{}{}{}}
\citation{hsu2020generalized}
\newlabel{tab:recognition_deepglint}{{3}{6}{Results of recognition trained on DeepGlint (except PFE). The results are all comparably high enough.}{}{}}
\newlabel{fig:rej_fr_curve}{{8}{6}{Face recognition performance with rejecting low-quality images. The curves show the effectiveness of rejecting low-quality face images in terms of false non-match rate (FNMR) at false match rate (FMR) threshold of 0.001. Training data: DeepGlint.}{}{}}
\newlabel{fig:FR-OOD}{{9}{6}{Balance between FR and OOD performance. Training data: DeepGlint.}{}{}}
\newlabel{ssec:ood}{{4.4}{6}{}{}{}}
\bibdata{aaai23}
\newlabel{tab:kl_weight}{{4}{7}{Results of our models trained with different weights for KL divergence ($\delta $ is 16). Training data: CASIA-WebFace.}{}{}}
\newlabel{tab:degree_of_freedom}{{5}{7}{Results of our models trained with different values for degree of freedom ($\lambda $ is 10). Training data: CASIA-WebFace.}{}{}}
\newlabel{ssec:face recognition}{{4.5}{7}{}{}{}}
\newlabel{ssec:ablation}{{4.6}{7}{}{}{}}
\newlabel{sec:conclusion}{{5}{7}{}{}{}}
\gdef \@abspage@last{8}
