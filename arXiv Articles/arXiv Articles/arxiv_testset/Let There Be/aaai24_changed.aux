\relax 
\bibstyle{aaai24}
\citation{smith1987when,midgley2006new}
\citation{kumar2019lipper,mira2022svts,kim2023lip}
\citation{son2017lip,kim2022distinguishing}
\citation{elias2021parallel,kim2022fluenttts}
\citation{le2015reconstructing,le2017generating}
\citation{ephrat2017vid2speech,kumar2019lipper}
\citation{prajwal2020learning,kim2021lip,yadav2021speech,he2022flow,mira2022svts,kim2023lip}
\citation{baevski2020wav2vec,hsu2021hubert}
\citation{yang21c_interspeech,chang2022distilhubert}
\citation{skerry2018towards,sun2020fully}
\citation{ren2021portaspeech}
\citation{ren2022revisiting}
\citation{le2015reconstructing,le2017generating}
\citation{ephrat2017vid2speech}
\citation{kumar2019lipper}
\citation{prajwal2020learning}
\citation{shen2018natural}
\citation{kim2021lip}
\citation{yadav2021speech}
\citation{he2022flow}
\citation{vaswani2017attention}
\citation{mira2022svts}
\citation{gulati2020conformer}
\citation{kim2023lip}
\citation{prenger2019waveglow,yamamoto2020parallel,kong2020hifi,kim21f_interspeech}
\citation{hunt1996unit}
\citation{black2007statistical}
\citation{shen2018natural,ren2020fastspeech2,lancucki2021fastpitch,lee2021multi,popov2021grad}
\citation{devlin2018bert}
\citation{bachman2019learning,lin2021completer}
\citation{baevski2020wav2vec}
\citation{hsu2021hubert}
\citation{baevski2021unsupervised}
\citation{lee2021voicemixer,choi2021neural}
\citation{polyak2021speech}
\citation{yang21c_interspeech,chang2022distilhubert}
\citation{kim21f_interspeech}
\citation{ji20123d}
\citation{he2016deep}
\citation{gulati2020conformer}
\citation{petridis2018end,ma2022visual}
\citation{guo2022cmt}
\citation{ephrat2017vid2speech}
\citation{kim2023lip}
\citation{hsu2021hubert}
\citation{polyak2021speech,lakhotia2021generative,kreuk2021textless}
\citation{yasuda2019investigation,lancucki2021fastpitch}
\citation{lancucki2021fastpitch}
\citation{lancucki2021fastpitch}
\citation{mauch2014pyin}
\citation{bulut2007analysis}
\citation{choi2021neural}
\citation{ren2020fastspeech2}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:overall}{{1(a)}{3}{}{}{}}
\newlabel{sub@fig:overall}{{(a)}{3}}
\newlabel{fig:enc}{{1(b)}{3}{}{}{}}
\newlabel{sub@fig:enc}{{(b)}{3}}
\newlabel{fig:dec}{{1(c)}{3}{}{}{}}
\newlabel{sub@fig:dec}{{(c)}{3}}
\newlabel{fig:post}{{1(d)}{3}{}{}{}}
\newlabel{sub@fig:post}{{(d)}{3}}
\newlabel{fig:architecture}{{1}{3}{In subfigure (a) and (b), $\boldsymbol  {e}_{spk}$ is a speaker embedding. In subfigure (a) and (c), $\boldsymbol  {h}_v$ denotes the encoded video feature. In (c), Emb.T. refers to an embedding table. In subfigure (d), the paths with dotted lines are operated only in a training stage. $\boldsymbol  {y}_{mel}$ and $\hat  {\boldsymbol  {y}}_{mel}$ refer to the ground truth and predicted mel-spectrogram, respectively. $cond$ means the post-net conditions which contain the input and output of the conformer decoder, and $\boldsymbol  {e}_{spk}$. In our experiment, we set $N=8$.}{}{}}
\citation{ren2022revisiting}
\citation{liu2022diffsinger}
\citation{ren2021portaspeech}
\citation{ren2022revisiting}
\citation{kim21f_interspeech}
\citation{daubechies1988orthonormal}
\citation{griffin1984signal}
\citation{kim2021lip,he2022flow,kim2023lip}
\citation{cooke2006audio}
\citation{prajwal2020learning}
\citation{he2022flow}
\citation{kim2023lip}
\citation{kim2023lip}
\citation{mira2022svts,kim2023lip}
\citation{lancucki2021fastpitch}
\citation{loshchilov2017decoupled}
\citation{radford2023robust}
\citation{kim2021lip}
\citation{mira2022svts}
\citation{kim2023lip}
\citation{kim21f_interspeech}
\newlabel{table:compare}{{1}{5}{Evaluation results. MOS results are presented with $95\%$ confidence interval. `Nat.' and `Intel.' represent MOS for naturalness and intelligibility, respectively. Note that MT\nobreakspace  {}\citep  {kim2023lip} cannot be trained on the Lip2Wav dataset since the model requires text transcription to be trained. $\uparrow $ denotes higher is better, $\downarrow $ denotes lower is better.}{}{}}
\newlabel{fig:dif-frev1}{{2(a)}{5}{}{}{}}
\newlabel{sub@fig:dif-frev1}{{(a)}{5}}
\newlabel{fig:dif-hiv1}{{2(b)}{5}{}{}{}}
\newlabel{sub@fig:dif-hiv1}{{(b)}{5}}
\newlabel{fig:dif-wavenet}{{2(c)}{5}{}{}{}}
\newlabel{sub@fig:dif-wavenet}{{(c)}{5}}
\newlabel{fig:dif-frev2}{{2(d)}{5}{}{}{}}
\newlabel{sub@fig:dif-frev2}{{(d)}{5}}
\newlabel{fig:dif-hiv2}{{2(e)}{5}{}{}{}}
\newlabel{sub@fig:dif-hiv2}{{(e)}{5}}
\newlabel{fig:dif-waveglow}{{2(f)}{5}{}{}{}}
\newlabel{sub@fig:dif-waveglow}{{(f)}{5}}
\newlabel{fig:mel-dif}{{2}{5}{Visualisation of mel-spectrogram. Note that the proposed method better captures fine details with frequency correlations compared to other methods ((c)-(e)), particularly in red boxes.}{}{}}
\citation{kim2023lip}
\citation{baevski2020wav2vec,hsu2021hubert}
\citation{yang21c_interspeech,chang2022distilhubert}
\citation{fan2020exploring,choi2021neural}
\citation{lakhotia2021generative,lee2022hierspeech}
\citation{lakhotia2021generative}
\newlabel{table:pitch}{{2}{6}{Mean ($\mu $), standard deviation ($\sigma $), skewness($\gamma $), and kurtosis($\kappa $) of the pitch distribution for ground truth and synthesised audio.}{}{}}
\citation{zhang2022mixed}
\bibdata{aaai24}
\newlabel{table:mae}{{3}{7}{The MAE between the energy of ground truth and that of synthesised audio. ``E." stands for energy.}{}{}}
\newlabel{sec:ablation}{{}{7}{}{}{}}
\newlabel{table:SSL}{{4}{7}{Evaluation on different configurations of linguistic feature extraction. \#clusters denotes the number of $K$-means clusters and layer means the layer index of HuBERT.}{}{}}
\newlabel{table:ablation}{{5}{7}{CMOS, WER, and CER results of an ablation study.}{}{}}
\gdef \@abspage@last{8}
