%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[]{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
%\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
%  -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
%  -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb,amsmath,amsfonts}
%\usepackage[foot]{amsaddr}
\usepackage{multicol,multirow,enumerate,textcomp} % setspace FORBIDDEN
\usepackage{stackengine,calc,xspace,array}
\usepackage{verbatim}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{remark}[theorem]{Remark}
%\numberwithin{theorem}{section} % ALREADY DEFINED
\newtheorem{definition}[theorem]{Definition}
\newtheorem{problem}[theorem]{Problem}
%\theoremstyle{remark}
%\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
%\numberwithin{example}{section}
\renewcommand{\O}{\mathcal{O}}
\DeclareSymbolFont{rsfscript}{OMS}{rsfs}{m}{n}
\DeclareSymbolFontAlphabet{\mathrsfs}{rsfscript}

\usepackage[dvipsnames]{xcolor}

\usepackage[makeroom]{cancel}


\usepackage{algorithm,algorithmicx}
\usepackage[noend]{algpseudocode}
\MakeRobust{\Call}
\newcommand{\LineIf}[2]{\State \algorithmicif\ {#1}\ \algorithmicthen\ {#2}}% \algorithmicend\ \algorithmicif}
\newcommand{\LineIfElse}[3]{\State \algorithmicif\ {#1}\ \algorithmicthen\ {#2} \algorithmicelse\ {#3}}% \algorithmicend\ \algorithmicif}
\newcommand{\LineForAll}[2]{\State \algorithmicforall\ {#1}\ \algorithmicdo\ {#2}}%\algorithmicend \algorithmicfor}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\alglinenoNew}[1]{\newcounter{ALG@line@#1}}
\newcommand{\alglinenoPop}[1]{\setcounter{ALG@line}{\value{ALG@line@#1}}}
\newcommand{\alglinenoPush}[1]{\setcounter{ALG@line@#1}{\value{ALG@line}}}

\usepackage[T1]{fontenc}


\title{Fast and Knowledge-Free Deep Learning for General Game Playing \\(Student Abstract)}
\author{
    Micha{\l} Maras, Micha{\l} K\k{e}pa, Jakub Kowalski, Marek Szyku{\l}a,
}
\affiliations{
    University of Wroc{\l}aw, Faculty of Mathematics and Computer Science\\
    mmaras1999@gmail.com, kempus1999@gmail.com, jko@cs.uni.wroc.pl, msz@cs.uni.wroc.pl
}



\begin{document}\maketitle\begin{abstract}
We develop a method of adapting the AlphaZero model to General Game Playing (GGP) that focuses on faster model generation and requires less knowledge to be extracted from the game rules. The dataset generation uses MCTS playing instead of self-play; only the value network is used, and attention layers replace the convolutional ones. This allows us to abandon any assumptions about the action space and board topology. We implement the method within the Regular Boardgames GGP system and show that we can build models outperforming the UCT baseline for most games efficiently.
\end{abstract}

\section{Introduction}

General Game Playing (GGP) is an Artificial Intelligence challenge focused on developing an autonomous game-playing agent that can play, without human intervention, any game given its rules \cite{Genesereth2005General}.
Such a task requires a proper formalism to encode a possibly large class of games in a machine-processable and simultaneously human-readable way.
Several such formalisms were developed, Stanford's Game Description Language (GDL) \cite{Genesereth2005General} being arguably the most famous and deep-researched, providing a great domain for testing AI algorithms, especially Monte Carlo Tree Search (MCTS) with UCT \cite{Browne2012ASurvey}.
A newer approach, focusing on more efficient game processing, is Regular Boardgames (RBG) \cite{Kowalski2019RegularBoardgames}, which encodes game rules as regular expressions.
A natural path of GGP research is to apply Neural Networks (NN) and Deep Reinforcement Learning (DRL).

The most famous NN-based approach that was advertised as general was AlphaZero \cite{silver2018general}, applying the same learning algorithm for Go, Chess, and Shogi. However, although the proposed method really generalizes (at least among two-player, zero-sum board games), the network architecture had to be manually prepared for each game, which is inconsistent with the pure GGP principles.
A clone of AlphaZero, based on GDL and aimed to resolve some of these restrictions, was presented in \cite{ThielscherAAAI20}.
MCTS-based DRL approach for Ludii GGP system, created via a bridge to Polygames can be found in \cite{soemers2021deep}.
Deep Reinforcement Learning using only value networks combined with a variant of Unbounded Minimax is described in \cite{cohen2023minimax}.

We address some issues that appear in the GGP context and focus on training within a short time limit and further restricting assumptions about the given game rules.


\begin{table*}[!ht]\small\renewcommand{\arraystretch}{1.045}
\newcommand{\cb}[1]{{\scriptsize $\pm$#1}}
\newcommand{\oppA}{vs.\ $1\times$}
\newcommand{\oppB}{vs.\ $10\times$}
\begin{center}\begin{tabular}{|lrc|r|r|r|r|r|r|r|r|}\hline
\multicolumn{1}{|c}{\multirow{3}{*}{{\bf Game}}} & \multicolumn{2}{l|}{\multirow{3}{*}{\bf Board size}} & \multicolumn{4}{c|}{\bf Ordered board} & \multicolumn{4}{c|}{\bf Permuted board} \\\cline{4-11}
& & & \multicolumn{2}{c|}{{\bf Attention NN}} & \multicolumn{2}{c|}{{\bf Convolutional NN}} & \multicolumn{2}{c|}{{\bf Attention NN}} & \multicolumn{2}{c|}{{\bf Convolutional NN}} \\%\rule{0pt}{9pt}
\multicolumn{3}{|c|}{} & \oppA & \oppB & \oppA & \oppB & \oppA & \oppB & \oppA & \oppB \\\hline

Breakthrough & 36 & $(6\times6)$  & {99}\%\cb{1.1} & 94\%\cb{2.4} & {99}\%\cb{0.8} & \textbf{98}\%\cb{1.4} & \textbf{99}\%\cb{0.7} & {93}\%\cb{2.5}  & 97\%\cb{1.7} & {93}\%\cb{2.5} \\ %\cline{2-10}
Breakthrough & 64 & $ (8\times8)$ & 83\%\cb{3.7} & 45\%\cb{4.9} & \textbf{88}\%\cb{3.2} & \textbf{65}\%\cb{4.7} & \textbf{85}\%\cb{3.5} & \textbf{53}\%\cb{4.9} & 77\%\cb{4.1} & 42\%\cb{4.8} \\ %\cline{2-10}
Breakthrough & 100 & $(10\times10)$ & {51}\%\cb{4.9} & 9\%\cb{2.8} & {47}\%\cb{4.9} & \textbf{15}\%\cb{3.5} & \textbf{17}\%\cb{3.7} & \textbf{4}\%\cb{1.9} & 4\%\cb{2.0} & 1\%\cb{0.8} \\

\multirow{1}{*}{Connect Four} & 42 & $(7\times6)$  & \textbf{72}\%\cb{4.3} & {57}\%\cb{4.8} & 58\%\cb{4.8} & {53}\%\cb{4.8} & 44\%\cb{4.8} & 37\%\cb{4.6}  & \textbf{49}\%\cb{4.9} & \textbf{42}\%\cb{4.8} \\

\multirow{1}{*}{English Draughts} & 32 & $(8\times8)$  & {86}\%\cb{2.3} & {43}\%\cb{3.3} & {85}\%\cb{2.5} & {46}\%\cb{3.4} & 77\%\cb{2.8} & 36\%\cb{3.1}  & \textbf{80}\%\cb{2.7} & \textbf{43}\%\cb{3.3} \\

\multirow{1}{*}{Fox and Hounds} & 32 & $(8\times8)$  & {91}\%\cb{2.8} & \textbf{77}\%\cb{4.1} & {93}\%\cb{2.5} & 72\%\cb{4.4} & 87\%\cb{3.3} & 58\%\cb{4.8}  & \textbf{94}\%\cb{2.4} & \textbf{72}\%\cb{4.4} \\

\multirow{1}{*}{Gomoku} & 225 & $(15\times15)$  & \textbf{88}\%\cb{3.2} & {38}\%\cb{4.8} & 44\%\cb{4.9} & {39}\%\cb{4.8} & \textbf{74}\%\cb{4.3} & \textbf{14}\%\cb{3.4}  & 10\%\cb{2.9} & 7\%\cb{2.5} \\

Hex & 25 & $(5\times5)$  & {83}\%\cb{3.7} & {56}\%\cb{4.9} & {85}\%\cb{3.5} & {60}\%\cb{4.8} & 79\%\cb{4.0} & {62}\%\cb{4.8}  & \textbf{86}\%\cb{3.4} & {65}\%\cb{4.7} \\
Hex & 49 & $(7\times7)$ & \textbf{76}\%\cb{4.2} & \textbf{49}\%\cb{4.9} & 44\%\cb{4.9} & 22\%\cb{4.1} & \textbf{57}\%\cb{4.9} & \textbf{34}\%\cb{4.7}  & 19\%\cb{3.9} & 8\%\cb{2.7} \\
Hex & 81 & $(9\times9)$ & \textbf{10}\%\cb{2.9} & {0}\%\cb{0.5} & 2\%\cb{1.3} & {0}\% & \textbf{4}\%\cb{1.8} & {0}\%\cb{0.5}  & {0}\%\cb{0.5} & 0\% \\

\multirow{1}{*}{Pentago} & 36 & $(6\times6)$  & 82\%\cb{3.8} & 41\%\cb{4.8} & \textbf{94}\%\cb{2.4} & \textbf{72}\%\cb{4.4} & 70\%\cb{4.5} & 28\%\cb{4.4}  & \textbf{80}\%\cb{4.0} & \textbf{46}\%\cb{4.9} \\

\multirow{1}{*}{Reversi} & 64 & $(8\times8)$  & {88}\%\cb{3.2} & {68}\%\cb{4.5} & {88}\%\cb{3.1} & {71}\%\cb{4.4} & \textbf{84}\%\cb{3.5} & \textbf{64}\%\cb{4.6}  & 63\%\cb{4.7} & 35\%\cb{4.6} \\

\multirow{1}{*}{The Mill Game} & 24 & $(8+8+8)$ & {67}\%\cb{4.3} & {36}\%\cb{4.1} & {70}\%\cb{4.2} & {40}\%\cb{4.3} & {61}\%\cb{3.9} & {34}\%\cb{3.6}  & {63}\%\cb{4.4} & {34}\%\cb{4.0} \\\hline
\multicolumn{3}{|c|}{Total average} & 75\% & 47\% & 69\% & 50\% & 64\% & 39\% & 56\% & 38\% \\\hline
\end{tabular}\end{center}
\caption{The results of the NN agents with 600 iterations. The baseline opponent is UCT with 600 ($1\times$) and 6,000 ($10\times$) iterations. The dataset for each game was gathered for at most 2h using 20 CPU cores. The 95\%-confidence intervals are given.
}\label{tab:main_results}
\end{table*}

\begin{table}[!ht]\small\centering\renewcommand{\arraystretch}{1.1}\setlength\tabcolsep{4.9pt}
\newcommand{\cb}[1]{{\scriptsize $\pm$#1}}%
\begin{tabular}{|l|r|r|r|}\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\bf Game}} & \multicolumn{3}{c|}{\bf Size of dataset (MCTS plays)} \\
                          & 200 & 400 & 1,000 \\\hline
                          & \multicolumn{3}{c|}{\bf Generation time + Training time} \\
Breakthrough $(6\times6)$ & 6s   + 20s &  11s + 20s & 25s + 20s \\
English Draughts          & 1.1m + 20s & 1.3m + 20s & 3.5m + 40s \\
Reversi                   & 1.2m + 20s & 1.2m + 20s & 3.5m + 40s \\\hline
                          & \multicolumn{3}{c|}{\bf Attention NN} \\
Breakthrough $(6\times6)$ & 4\%\cb{1.8} & 60\%\cb{4.8} & 74\%\cb{4.3} \\
English Draughts          & 31\%\cb{2.9} & 42\%\cb{3.2} & 54\%\cb{3.3} \\
Reversi                   & 25\%\cb{4.2} & 22\%\cb{4.0} & 28\%\cb{4.3} \\\hline
                          & \multicolumn{3}{c|}{\bf Convolutional NN} \\
Breakthrough $(6\times6)$ & 41\%\cb{4.8} & 48\%\cb{4.9} & 83\%\cb{3.7} \\
English Draughts          & 42\%\cb{3.4} & 55\%\cb{3.5} & 60\%\cb{3.4} \\
Reversi                   & 6\%\cb{2.3} & 7\%\cb{2.4} & 21\%\cb{4.0} \\\hline

\end{tabular}
\caption{Short training. The results of the NN agents with 600 iterations against UCT baseline with also 600 iterations.} \label{tab:fast}
\end{table}

\section{Our Method}

We adapted AlphaZero to the RBG framework and we focus on two-player zero-sum games, but the proposed method is limited only by the requirement of perfect information.
Our modifications focus on the following areas:

\textbf{Action space.}
The original AlphaZero approach requires the knowledge of action space, which must be either defined manually or inferred from the game rules (in GGP).
There are two issues.
First, the action space depends on a particular game encoding, and one game can have many implementations. Second, since tensors generally must have a fixed shape, games with a huge action space are particularly problematic, even if they have a small branching factor.
The second problem could be alternatively addressed by splitting actions into elementary fragments \cite{Kowalski2022SplitMoves}, yet this may lose heuristic information of actions.

To address these problems, we omit the policy network and attempt to use only the value network, abandoning any dependence on the action space. Thus, we use the standard UCT formula instead of the PUCT \cite{Rosin2011}, a variant of the UCT algorithm that uses a predictor to provide recommendations about the order of actions during exploration.



\textbf{Board topology.}
Another piece of information that must be extracted from game rules is the neighborhood of board tiles, which is necessary for the standard convolutional approach.
In GGP, extracting such information from the game description requires additional effort and is not always reliable.
There is no guarantee that the games will have natural board topology, especially for non-rectangular topologies, or even the descriptions can be intentionally obfuscated.
Also, the natural board topology does not guarantee that adjacent cells are, by definition, the most correlated ones.

To avoid assumptions, we propose using an attention-based NN \cite{vaswani2017attention}. The model creates a (sinusoidal or learned positional) embedding for each tile and passes them through the encoder layers. This allows the network to learn the relations between tiles without any game-specific knowledge, regardless of the order of the tiles in the input. The presented method is the first application of attention networks to GGP.
To evaluate the potential of self-attention, we propose comparing it with CNNs based on random permutations of the input board tiles. Reordering the tiles should eliminate the spatially local correlation, which is assumed by default by CNNs.


\textbf{Fast model generation}
To take advantage of the very fast RBG reasoner and to vastly decrease training time, the training dataset is generated by playing games using standard UCT MCTS agents (without NN). This approach provides better quality samples than in the early stages of self-play and allows gathering large amounts of data with limited resources. It can be easily parallelized since MCTS plays can be performed independently on CPU cores and do not require a GPU.



\section{Experiments with Conclusions}

We evaluated the trained NN agents by playing 400 games (200 per side) against a standard MCTS agent with game tree reuse. The results of the constant simulations limit are presented in Table~\ref{tab:main_results}.
Convolutional NN assumes that the board fits within a quad, and the tiles are given in order.
To examine the robustness, we tested the behavior after obfuscating by permuting the tiles randomly (the same permutation used for both NNs).
Overall, the attention NN has a similar performance to the CNN, even if the order of board tiles is random. The results vary a lot depending on the game, especially on its size.
Yet, they suggest the trend that attention is a better choice for larger games, which is particularly visible in size variations of Breakthrough and Hex.
It also suffers a smaller win ratio drop on average on our game set when the board is permuted (the mean drop is respectively 11\% and 8\% for attention NN vs. 13\% and 12\% for CNN).

Table~\ref{tab:fast} shows results after very short training.
We used smaller NNs here, trained on much less data (less than $3\%$ of the 2h dataset from Table~\ref{tab:main_results}).
This experiment was inspired by \cite{ThielscherAAAI20}, where in Breakthrough, after just 400 selfplay games, the winrate close to 80\% was achieved.
To achieve a similar playing strength, we needed about 1,000 MCTS vs.\ MCTS training plays. Thus, our plays are of worse quality, but we can generate them in less than a minute.
Because of the RBG language efficiency \cite{Kowalski2020EfficientReasoning}, in our experiments, optimized Monte-Carlo playouts are much faster than NN evaluation. Such a situation, causing time-based comparison to favor pure MCTS, was not reported in GGP-related literature.

We conclude that knowledge of the action space and board topology is not required for obtaining good models.
The data generated from MCTS plays in place of self-plays is still useful; in some cases, it allows beating the baseline after a few minutes of computing.
Note that our research was focused on budget training, so the landscape could be different in longer settings.




\section{Acknowledgments}
This work was supported in part by the National Science Centre, Poland under project number 2021/41/B/ST6/03691.

Maiores dolores itaque rerum consectetur, ea cumque fugiat.\clearpage
\bibliography{bibliography}
\end{document}