% Encoding: UTF-8

@Article{fierens2015tplp,
  author    = {Fierens, Daan and Van den Broeck, Guy and Renkens, Joris and Shterionov, Dimitar and Gutmann, Bernd and Thon, Ingo and Janssens, Gerda and De Raedt, Luc},
  title     = {Inference and learning in probabilistic logic programs using weighted boolean formulas},
  journal   = {Theory and Practice of Logic Programming},
  year      = {2015},
  volume    = {15},
  number    = {3},
  pages     = {358--401},
  abstract  = {Probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities. This paper investigates how classical inference and learning tasks known from the graphical model community can be tackled for probabilistic logic programs. Several such tasks, such as computing the marginals, given evidence and learning from (partial) interpretations, have not really been addressed for probabilistic logic programs before. The first contribution of this paper is a suite of efficient algorithms for various inference tasks. It is based on the conversion of the program and the queries and evidence to a weighted Boolean formula. This allows us to reduce inference tasks to well-studied tasks, such as weighted model counting, which can be solved using state-of-the-art methods known from the graphical model and knowledge compilation literature. The second contribution is an algorithm for parameter estimation in the learning from interpretations setting. The algorithm employs expectation-maximization, and is built on top of the developed inference algorithms. The proposed approach is experimentally evaluated. The results show that the inference algorithms improve upon the state of the art in probabilistic logic programming, and that it is indeed possible to learn the parameters of a probabilistic logic program from interpretations.},
  doi       = {10.1017/S1471068414000076},
  publisher = {Cambridge Univ Press},
  url       = {https://lirias.kuleuven.be/bitstream/123456789/392821/3/plp2cnf.pdf},
}

@TechReport{jain2009tr,
  author      = {Jain, Dominik and Waldherr, Stefan and Beetz, Michael},
  title       = {Bayesian logic networks},
  institution = {IAS Group, Fakultät für Informatik, Technische Universität München},
  year        = {2009},
  abstract    = {This report introduces Bayesian logic networks (BLNs), a statistical relational knowledge representation formalism that is geared towards practical applicability. A BLN is a meta-model for the construction of a probability distribution from local probability distribution fragments (as in a Bayesian network) and global logical constraints formulated in first-order logic. An instance is thus a mixed network with probabilistic and deterministic constraints.

We provide the formal semantics of BLNs and explain their practical realization as implemented in the open-source software toolbox ProbCog, which supports learning and a wide range of inference algorithms.},
  url         = {http://ias.informatik.tu-muenchen.de/_media/spezial/bib/jain09blns.pdf},
}

@Book{russell2009aiama,
  title     = {Artificial Intelligence: A Modern Approach, 3rd Edition},
  publisher = {Pearson},
  year      = {2009},
  author    = {Russell, Stuart J. and Norvig, Peter},
  url       = {http://www.mypearsonstore.com/bookstore/artificial-intelligence-a-modern-approach-9780136042594},
}

@InProceedings{singla2008aaai,
  author    = {Singla, Parag and Domingos, Pedro M.},
  title     = {Lifted First-Order Belief Propagation},
  booktitle = {Proceedings of the 23rd national conference on Artificial intelligence (AAAI'08)},
  year      = {2008},
  volume    = {2},
  pages     = {1094--1099},
  address   = {Chicago, IL, USA},
  publisher = {AAAI Press},
  abstract  = {Unifying first-order logic and probability is a long-standing goal of AI, and in recent years many representations combining aspects of the two have been proposed. However, inference in them is generally still at the level of propositional logic, creating all ground atoms and formulas and applying standard probabilistic inference methods to the resulting network. Ideally, inference should be lifted as in first-order logic, handling whole sets of indistinguishable objects together, in time independent of their cardinality. Poole (2003) and Braz et al. (2005, 2006) developed a lifted version of the variable elimination algorithm, but it is extremely complex, generally does not scale to realistic domains, and has only been applied to very small artificial problems. In this paper we propose the first lifted version of a scalable probabilistic inference algorithm, belief propagation (loopy or not). Our approach is based on first constructing a lifted network, where each node represents a set of ground atoms that all pass the same messages during belief propagation. We then run belief propagation on this network. We prove the correctness and optimality of our algorithm. Experiments show that it can greatly reduce the cost of inference.},
  url       = {http://homes.cs.washington.edu/~pedrod/papers/aaai08a.pdf},
}

@InProceedings{sang2005aaai,
  author    = {Sang, Tian and Beame, Paul and Kautz, Henry A},
  title     = {Performing {B}ayesian inference by weighted model counting},
  booktitle = {Proceedings of the Twentieth AAAI National Conference on Artificial Intelligence (AAAI'05)},
  year      = {2005},
  pages     = {475--481},
  abstract  = {Over the past decade general satisfiability testing algorithms have proven to be surprisingly effective at solving a wide variety of constraint satisfaction problem, such as planning and scheduling. Solving such NP-complete tasks by compilation to SAT has turned out to be an approach that is of both practical and theoretical interest. Recently, it has been shown that state of the art SAT algorithms can be efficiently extended to the harder task of counting the number of models (satisfying assignments) of a formula, by employing a technique called component caching. This paper begins to investigate the question of whether compilation to model-counting could be a practical technique for solving real-world #P-complete problems, in particular Bayesian inference. We describe an efficient translation from Bayesian networks to weighted model counting, extend the best model-counting algorithms to weighted model counting, develop an efficient method for computing all marginals in a single counting pass, and evaluate the approach on computationally challenging reasoning problems.},
  url       = {https://www.aaai.org/Papers/AAAI/2005/AAAI05-075.pdf
https://www.cs.rochester.edu/u/kautz/papers/aaai05-wmc-sang-beame-kautz.pdf},
}

@Book{getoor2007itsrl,
  title     = {Introduction to statistical relational learning},
  publisher = {MIT press},
  year      = {2007},
  editor    = {Getoor, Lise and Taskar, Ben},
  url       = {https://mitpress.mit.edu/books/introduction-statistical-relational-learning},
}

@InProceedings{braz2005ijcai,
  author    = {Braz, Rodrigo De Salvo and Amir, Eyal and Roth, Dan},
  title     = {Lifted first-order probabilistic inference},
  booktitle = {Proceedings of the 19th International Joint Conference on Artificial intelligence},
  year      = {2005},
  pages     = {1319--1325},
  url       = {http://ijcai.org/Proceedings/05/Papers/1548.pdf},
}

@Article{vlasselaer2016ai,
  author    = {Vlasselaer, Jonas and Meert, Wannes and Van den Broeck, Guy and De Raedt, Luc},
  title     = {Exploiting local and repeated structure in Dynamic {B}ayesian Networks},
  journal   = {Artificial Intelligence},
  year      = {2016},
  volume    = {232},
  pages     = {43--53},
  abstract  = {We introduce the structural interface algorithm for exact probabilistic inference in Dynamic Bayesian Networks. It unifies state-of-the-art techniques for inference in static and dynamic networks, by combining principles of knowledge compilation with the interface algorithm. The resulting algorithm not only exploits the repeated structure in the network, but also the local structure, including determinism, parameter equality and context-specific independence. Empirically, we show that the structural interface algorithm speeds up inference in the presence of local structure, and scales to larger and more complex networks.},
  doi       = {10.1016/j.artint.2015.12.001},
  publisher = {Elsevier},
  url       = {https://lirias.kuleuven.be/bitstream/123456789/510960/6/dbn_paper.pdf},
}

@Book{murphy2012mlapp,
  title     = {Machine Learning - A Probabilistic Perspective},
  publisher = {MIT Press},
  year      = {2012},
  author    = {Murphy, Kevin P.},
  url       = {https://mitpress.mit.edu/books/machine-learning-0},
}

@InProceedings{gogate2011uai,
  author    = {Gogate, Vibhav and Domingos, Pedro},
  title     = {Probabilistic theorem proving},
  booktitle = {Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI)},
  year      = {2011},
  groups    = {[johan:]},
  url       = {http://www.hlt.utdallas.edu/%7Evgogate/papers/uai11-b.pdf},
}

@InProceedings{wang2008aaai,
  author    = {Wang, Jue and Domingos, Pedro M.},
  title     = {Hybrid {M}arkov Logic Networks},
  booktitle = {Proceedings of the 23rd national conference on Artificial Intelligence (AAAI)},
  year      = {2008},
  volume    = {2},
  pages     = {1106--1111},
  abstract  = {Markov logic networks (MLNs) combine first-order logic and Markov networks, allowing us to handle the complexity and uncertainty of real-world problems in a single consistent framework. However, in MLNs all variables and features are discrete, while most real-world applications also contain continuous ones. In this paper we introduce hybrid MLNs, in which continuous properties (e.g., the distance between two objects) and functions over them can appear as features. Hybrid MLNs have all distributions in the exponential family as special cases (e.g., multivariate Gaussians), and allow much more compact modeling of non-i.i.d. data than propositional representations like hybrid Bayesian networks. We also introduce inference algorithms for hybrid MLNs, by extending the MaxWalkSAT and MC-SAT algorithms to continuous domains. Experiments in a mobile robot mapping domain--involving joint classification, clustering and regression--illustrate the power of hybrid MLNs as a modeling language, and the accuracy and efficiency of the inference algorithms.},
  url       = {http://homes.cs.washington.edu/~pedrod/papers/aaai08b.pdf},
}

@InCollection{jain2011ki,
  author    = {Jain, Dominik and von Gleissenthall, Klaus and Beetz, Michael},
  title     = {Bayesian Logic Networks and the Search for Samples with Backward Simulation and Abstract Constraint Learning},
  booktitle = {KI 2011: Advances in Artificial Intelligence},
  publisher = {Springer},
  year      = {2011},
  month     = jan,
  isbn      = {978-3-642-24454-4},
  abstract  = {With Bayesian logic networks (BLNs), we present a practical representation formalism for statistical relational knowledge. Based on the concept of mixed networks with probabilistic and deterministic constraints, BLNs combine the probabilistic semantics of (relational) Bayesian networks with constraints in first-order logic. In practical applications, efficient inference in statistical relational models such as BLNs is a key concern. Motivated by the inherently mixed nature of models instantiated from BLNs, we investigate two novel importance sampling methods: The first combines backward simulation, i.e. sampling backward from the evidence, with systematic search, while the second explores the possibility of recording abstract constraints during the search for samples.},
  date      = {2011-01-01},
  doi       = {10.1007/978-3-642-24455-1_14},
}

@Article{richardson2006ml,
  author    = {Richardson, Matthew and Domingos, Pedro},
  title     = {Markov logic networks},
  journal   = {Machine Learning},
  year      = {2006},
  volume    = {62},
  number    = {1-2},
  pages     = {107--136},
  month     = feb,
  abstract  = {We propose a simple approach to combining first-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a first-order knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.},
  date      = {2006-02-01},
  doi       = {10.1007/s10994-006-5833-1},
  publisher = {Springer},
  url       = {http://dx.doi.org/10.1007/s10994-006-5833-1},
}

@Article{mauch2010taslp,
  author   = {Mauch, Matthias and Dixon, Simon},
  title    = {Simultaneous estimation of chords and musical context from audio},
  journal  = TASLP,
  year     = {2010},
  volume   = {18},
  number   = {6},
  pages    = {1280--1289},
  month    = {August},
  abstract = {Chord labels provide a concise description of musical harmony. In pop and jazz music, a sequence of chord labels is often the only written record of a song, and forms the basis of so-called lead sheets. We devise a fully automatic method to simultaneously estimate from an audio waveform the chord sequence including bass notes, the metric positions of chords, and the key. The core of the method is a six-layered dynamic Bayesian network, in which the four hidden source layers jointly model metric position, key, chord, and bass pitch class, while the two observed layers model low-level audio features corresponding to bass and treble tonal content. Using 109 different chords our method provides substantially more harmonic detail than previous approaches while maintaining a high level of accuracy. We show that with 71% correctly classified chords our method significantly exceeds the state of the art when tested against manually annotated ground truth transcriptions on the 176 audio tracks from the MIREX 2008 Chord Detection Task.We introduce a measure of segmentation quality and show that bass and meter modeling are especially beneficial for obtaining the correct level of granularity.},
  comment  = {Only key signature detected!},
  doi      = {10.1109/TASL.2009.2032947},
  keywords = {chord transcription, dynamic Bayesian networks (DBNs), music signal processing},
}

@InProceedings{cho2011ismir,
  author    = {Cho, Taemin and Bello, Juan Pablo},
  title     = {A feature smoothing method for chord recognition using recurrence plots},
  booktitle = ISMIR2011,
  year      = {2011},
  pages     = {651--656},
  abstract  = {In this paper, we propose a feature smoothing technique for chord recognition tasks based on repeated patterns within a song. By only considering repeated segments of a song, our method can smooth the features without losing chord boundary information and fine details of the original feature. While a similar existing technique requires several hard decisions such as beat quantization and segmentation, our method uses a simple pragmatic approach based on recurrence plot to decide which repeated parts to include in the smoothing process. This approach uses a more formal definition of the repetition search and allows shorter (“chord-size”) repeated segments to contribute to the feature improvement process. In our experiments, our method outperforms conventional and popular smoothing techniques (a moving average filter and a median filter). In particular, it shows a synergistic effect when used with the Viterbi decoder.},
  timestamp = {2012.12.10},
  url       = {http://ismir2011.ismir.net/papers/OS8-4.pdf},
}

@InProceedings{fujishima1999icmc,
  author    = {Fujishima, Takuya},
  title     = {Realtime chord recognition of musical sound: a system using {C}ommon {L}isp {M}usic},
  booktitle = ICMC,
  year      = {1999},
  pages     = {464--467},
  address   = {Ann Arbor, MI, USA},
  publisher = {MPublishing},
  abstract  = {This paper describes a realtime software system which recognizes musical chords from input sound signals. I designed an algorithm and implemented it in Common Lisp Music/Realtime environment. The system runs on Silicon Graphics workstations and on the Intel PC platform. Experiments verified that the system succeeded in correctly identifying chords even on orchestral sounds.},
  comment   = {Features: simple energy chromagram, framestep = 256 ms, framesize = 256 ms with feature smoothing
Acoustic model: template matching with Euclidean distance and inner product
Duration model: none, but feature smoothing
Change model: none},
  doi       = {2027/spo.bbp2372.1999.446},
  keywords  = {music, sound, chord, machine, recognition, realtime, numerical, AI},
  url       = {http://quod.lib.umich.edu/cgi/t/text/text-idx?c=icmc;idno=bbp2372.1999.446;cc=icmc},
}

@Article{papadopoulos2017taslp,
  author   = {Papadopoulos, Hélène and Tzanetakis, George},
  title    = {Models for music analysis from a {M}arkov logic networks perspective},
  journal  = TASLP,
  year     = {2017},
  volume   = {25},
  pages    = {19--34},
  abstract = {Analyzing and formalizing the intricate mechanisms of music is a very challenging goal for Artificial Intelligence. Dealing with real audio recordings requires the ability to handle both uncertainty and complex relational structure at multiple levels of representation. Until now, these two aspects have been generally treated separately, probability being the standard way to represent uncertainty in knowledge, while logical representation being the standard way to represent knowledge and complex relational information. Several approaches attempting a unification of logic and probability have recently been proposed. In particular, Markov logic networks (MLNs), which combine first-order logic and probabilistic graphical models, have attracted increasing attention in recent years in many domains. This paper introduces MLNs as a highly flexible and expressive formalism for the analysis of music that encompasses most of the commonly used probabilistic and logic-based models. We first review and discuss existing approaches for music analysis. We then introduce MLNs in the context of music signal processing by providing a deep understanding of how they specifically relate to traditional models, specifically hidden Markov models and conditional random fields. We then present a detailed application of MLNs for tonal harmony music analysis that illustrates the potential of this framework for music processing.},
  doi      = {10.1109/TASLP.2016.2614351},
  issue    = {1},
}

@InProceedings{papadopoulos2012ismir,
  author         = {Papadopoulos, Hélène and Tzanetakis, George},
  title          = {Modeling chord and key structure with {M}arkov logic},
  booktitle      = ISMIR2012,
  year           = {2012},
  pages          = {121--126},
  abstract       = {We propose the use of Markov Logic Networks (MLNs) as a highly flexible and expressive formalism for the harmonic analysis of audio signals. Using MLNs information about the physical and semantic content of the signal can be intuitively and compactly encoded and expert knowledge can be easily expressed and combined using a single unified formal model that combines probabilities and logic. In particular, we propose a new approach for joint estimation of chord and global key. The proposed model is evaluated on a set of popular music songs. The results show that it can achieve similar performance to a state of the art Hidden Markov Model for chord estimation while at the same time estimating global key. In addition when prior information about global key is used it shows a small but statistically significant improvement in chord estimation performance. Our results demonstrate the potential of MLNs for music analysis as they can express both structured relational knowledge as well as uncertainty.},
  timestamp      = {2012.12.13},
  url            = {http://ismir2012.ismir.net/event/papers/121-ismir-2012.pdf},
}

@InProceedings{mauch2009ismir,
  author    = {Mauch, Matthias and Dixon, Simon},
  title     = {Using musical structure to enhance automatic chord transcription},
  booktitle = ISMIR2009,
  year      = {2009},
  pages     = {231--236},
  abstract  = {Chord extraction from audio is a well-established music computing task, and many valid approaches have been presented in recent years that use different chord templates, smoothing techniques and musical context models. The present work shows that additional exploitation of the repetitive structure of songs can enhance chord extraction, by combining chroma information from multiple occurrences of the same segment type. To justify this claim we modify an existing chord labelling method, providing it with manual or automatic segment labels, and compare chord extraction results on a collection of 125 songs to baseline methods without segmentation information. Our method results in consistent and more readily readable chord labels and provides a statistically significant boost in label accuracy.},
  url       = {http://ismir2009.ismir.net/proceedings/PS2-7.pdf},
}

@InProceedings{papadopoulos2013icassp,
  author    = {Papadopoulos, Hélène and Tzanetakis, George},
  title     = {Exploiting structural relationships in audio music signals using {M}arkov Logic Networks},
  booktitle = ICASSP,
  year      = {2013},
  pages     = {1--5},
  month     = may,
  abstract  = {We propose an innovative approach for music description at several time-scales in a single unified formalism. More specifically, chord information at the analysis-frame level and global semantic structure are integrated in an elegant and flexible model. Using Markov Logic Networks (MLNs) low-level signal features are encoded with high-level information expressed by logical rules, without the need of a transcription step. Our results demonstrate the potential of MLNs for music analysis as they can express both structured relational knowledge through logic as well as uncertainty through probabilities.},
  doi       = {10.1109/ICASSP.2013.6637597},
  issn      = {1520-6149},
  keywords  = {Markov processes, audio coding, music, MLNs, Markov logic networks, analysis-frame level, audio music signals, chord information, global semantic structure, high-level information, logical rules, low-level signal features, music analysis, music description, probability, single unified formalism, structural relationships, structured relational knowledge, Estimation, Hidden Markov models, Markov processes, Multiple signal classification, Music, Probabilistic logic, Semantics, Chord Detection, Markov Logic Networks, Music Information Retrieval, Structure Analysis},
}

@InProceedings{wakefield1999spie,
  author    = {Wakefield, Gregory},
  title     = {Mathematical representation of joint time-chroma distributions},
  booktitle = {Proceedings of the 9th SPIE Conference on Advanced Signal Processing Algorithms, Architectures, and Implementations},
  year      = {1999},
  editor    = {Luk, Franklin T.},
  volume    = {3807},
  pages     = {637--645},
  month     = {November 2},
  abstract  = {Originally coined by the sensory psychologist Roger Shepard in the 1960s, chroma transforms frequency into octave equivalence classes. By extending the concept of chroma to chroma strength and how it varies over time, we have demonstrated the utility of chroma in simplifying the processing and representation of signals dominated by harmonically-related narrowband components. These investigations have utilized an ad hoc procedure for calculating the chromagram from a given time-frequency distribution. The present paper is intended to put this ad hoc procedure on more sound mathematical ground.},
  doi       = {10.1117/12.367679},
  timestamp = {2014.02.21},
  url       = {http://www.cs.northwestern.edu/~pardo/courses/eecs352/papers/wakefield-chromaDis.pdf},
}

@Article{ghahramani2001ijprai,
  author    = {Ghahramani, Zoubin},
  title     = {An introduction to hidden {M}arkov models and {B}ayesian networks},
  journal   = {International Journal of Pattern Recognition and Artificial Intelligence},
  year      = {2001},
  volume    = {15},
  number    = {01},
  pages     = {9--42},
  abstract  = {Hidden Markov models (HMMs) have proven to be one of the most widely used tools for learning probabilistic models of time series data. In an HMM, information about the past is conveyed through a single discrete variable—the hidden state. We discuss a generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner. We describe an exact algorithm for inferring the posterior probabilities of the hidden state variables given the observations, and relate it to the forward–backward algorithm for HMMs and to algorithms for more general graphical models. Due to the combinatorial nature of the hidden state representation, this exact algorithm is intractable. As in other intractable systems, approximate inference can be carried out using Gibbs sampling or variational methods. Within the variational framework, we present a structured approximation in which the the state variables are decoupled, yielding a tractable algorithm for learning the parameters of the model. Empirical comparisons suggest that these approximations are efficient and provide accurate alternatives to the exact methods. Finally, we use the structured approximation to model Bach's chorales and show that factorial HMMs can capture statistical structure in this data set which an unconstrained HMM cannot.},
  doi       = {10.1142/S0218001401000836},
  publisher = {World Scientific},
  url       = {https://www.cs.ucsb.edu/~mturk/Courses/CS281B-2011/Misc/Ghahramani.pdf},
}

@Article{yu2010ai,
  author   = {Yu, Shun-Zeng},
  title    = {Hidden semi-{M}arkov models},
  journal  = {Artificial intelligence},
  year     = {2010},
  volume   = {174},
  number   = {2},
  pages    = {215--243},
  month    = {February},
  abstract = {As an extension to the popular hidden Markov model (HMM), a hidden semi-Markov model (HSMM) allows the underlying stochastic process to be a semi-Markov chain. Each state has variable duration and a number of observations being produced while in the state. This makes it suitable for use in a wider range of applications. Its forward– backward algorithms can be used to estimate/update the model parameters, determine the predicted, filtered and smoothed probabilities, evaluate goodness of an observation sequence fitting to the model, and find the best state sequence of the underlying stochastic process. Since the HSMM was initially introduced in 1980 for machine recognition of speech, it has been applied in thirty scientific and engineering areas, such as speech recognition/synthesis, human activity recognition/prediction, handwriting recognition, functional MRI brain mapping, and network anomaly detection. There are about three hundred papers published in the literature. An overview of HSMMs is presented in this paper, including modelling, inference, estimation, implementation and applications. It first provides a unified description of various HSMMs and discusses the general issues behind them. The boundary conditions of HSMM are extended. Then the conventional models, including the explicit duration, variable transition, and residential time of HSMM, are discussed. Various duration distributions and observation models are presented. Finally, the paper draws an outline of the applications.},
  doi      = {10.1016/j.artint.2009.11.011},
}

@Article{fine1998ml,
  author    = {Fine, Shai and Singer, Yoram and Tishby, Naftali},
  title     = {The hierarchical hidden {M}arkov model: analysis and applications},
  journal   = {Machine Learning},
  year      = {1998},
  volume    = {32},
  pages     = {41--62},
  abstract  = {We introduce, analyze and demonstrate a recursive hierarchical generalization of the widely used hidden Markov models, which we name Hierarchical Hidden Markov Models (HHMM). Our model is motivated by the complex multi-scale structure which appears in many natural sequences, particularly in language, handwriting and speech. We seek a systematic unsupervised approach to the modeling of such structures. By extending the standard Baum-Welch (forward-backward) algorithm, we derive an efficient procedure for estimating the model parameters from unlabeled data. We then use the trained model for automatic hierarchical parsing of observation sequences. We describe two applications of our model and its parameter estimation procedure. In the first application we show how to construct hierarchical models of natural English text. In these models different levels of the hierarchy correspond to structures on different length scales in the text. In the second application we demonstrate how HHMMs can be used to automatically identify repeated strokes that represent combination of letters in cursive handwriting.},
  doi       = {10.1023/A:1007469218079},
  keywords  = {statistical models, temporal pattern recognition, hidden variable models, cursive handwriting},
  timestamp = {2012.03.06},
  url       = {http://www.cs.princeton.edu/courses/archive/spr06/cos598C/papers/FineSingerTishby1998.pdf},
}

@Article{jordan1996nips,
  author  = {Jordan, Michael I. and Ghahramani, Z},
  title   = {Factorial Hidden Markov Models},
  journal = {Advances in Neural Information Processing Systems},
  year    = {1996},
  volume  = {8},
  pages   = {472--472},
}

@InProceedings{pauwels2013ismir,
  author    = {Pauwels, Johan and Kaiser, Florian and Peeters, Geoffroy},
  title     = {Combining harmony-based and novelty-based approaches for structural segmentation},
  booktitle = ISMIR2013,
  year      = {2013},
  pages     = {138--143},
  url       = {http://www.ppgia.pucpr.br/ismir2013/wp-content/uploads/2013/09/138_Paper.pdf},
  abstract  = {This paper describes a novel way to combine a well-proven method of structural segmentation through novelty detection with a new method based on harmonic analysis. The former system works by looking for peaks in novelty curves derived from self-similarity matrices. The latter relies on the detection of key changes and on the differences in prior probability of chord transitions according to their position in a structural segment. Both approaches are integrated into a probabilistic system that jointly estimates keys, chords and structural boundaries. The novelty curves are herein used as observations. In addition, chroma profiles are used as features for the harmony analysis. These observations are then subjected to a constrained transition model that is musically motivated. An information theoretic justification of this model is also given. Finally, an evaluation of the resulting system is performed. It is shown that the combined system improves the results of both constituting components in isolation.},
  address   = {Curitiba, Brazil},
  timestamp = {2013.07.15},
}

@Article{pauwels2014jnmr,
  author    = {Pauwels, Johan and Martens, Jean-Pierre},
  title     = {Combining musicological knowledge about chords and keys in a simultaneous chord and local key estimation system},
  journal   = JNMR,
  year      = {2014},
  volume    = {43},
  number    = {3},
  pages     = {318--330},
  doi       = {10.1080/09298215.2014.917684},
  keywords  = {audio analysis, information retrieval, machine learning, harmony, tonality},
  timestamp = {2014.05.15},
}

@InProceedings{pauwels2019ismir,
  author    = {Pauwels, Johan and O'Hanlon, Ken and Gómez, Emilia and Sandler, Mark B.},
  title     = {20 Years of Automatic Chord Recognition from Audio},
  booktitle = ISMIR2019,
  year      = {2019},
  venue     = {Delft, The Netherlands},
  pages     = {54--63},
  url       = {http://archives.ismir.net/ismir2019/paper/000004.pdf},
  abstract  = {In 1999, Fujishima published "Realtime Chord Recognition of Musical Sound: a System using Common Lisp Music". This paper kickstarted an active research topic that has been popular in and around the ISMIR community. The field of Automatic Chord Recognition (ACR) has evolved considerably from early knowledge-based systems towards data-driven methods, with neural network approaches arguably being central to current ACR research. Nonetheless, many of its core issues were already addressed or referred to in the Fujishima paper. In this paper, we review those twenty years of ACR according to these issues. We furthermore attempt to frame current directions in the field in order to establish some perspective for future research.},
}

@Electronic{grovemusic,
author = {{Grove Music Online}},
title = {Chord},
howpublished = {\url{https://doi.org/10.1093/gmo/9781561592630.article.05671}},
}
@Comment{jabref-meta: databaseType:bibtex;}
