\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Cabot and Navigli(2021)}]{cabot2021rebel}
Cabot, P.-L.~H.; and Navigli, R. 2021.
\newblock REBEL: Relation extraction by end-to-end language generation.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2021}, 2370--2381.

\bibitem[{Chen et~al.(2023)Chen, He, Guo, Zhu, Wang, Tang, and Liu}]{chen2023valor}
Chen, S.; He, X.; Guo, L.; Zhu, X.; Wang, W.; Tang, J.; and Liu, J. 2023.
\newblock Valor: Vision-audio-language omni-perception pretraining model and dataset.
\newblock \emph{arXiv preprint arXiv:2304.08345}.

\bibitem[{Chen and Jiang(2019)}]{chen2019semantic}
Chen, S.; and Jiang, Y.-G. 2019.
\newblock Semantic proposal for activity localization in videos via sentence query.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~33, 8199--8206.

\bibitem[{Chen, Yao, and Jiang(2019)}]{chen2019deep}
Chen, S.; Yao, T.; and Jiang, Y.-G. 2019.
\newblock Deep Learning for Video Captioning: A Review.
\newblock In \emph{IJCAI}, volume~1, 2.

\bibitem[{Cubuk et~al.(2019)Cubuk, Zoph, Shlens, and Le}]{Ekin2019randaugment}
Cubuk, E.~D.; Zoph, B.; Shlens, J.; and Le, Q.~V. 2019.
\newblock RandAugment: Practical data augmentation with no separate search.
\newblock \emph{CoRR}, abs/1909.13719.

\bibitem[{Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly et~al.}]{dosovitskiy2020image}
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et~al. 2020.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}.

\bibitem[{Gao et~al.(2021)Gao, Chen, Huang, and Xiao}]{gao2021video}
Gao, K.; Chen, L.; Huang, Y.; and Xiao, J. 2021.
\newblock Video relation detection via tracklet based visual transformer.
\newblock In \emph{Proceedings of the 29th ACM international conference on multimedia}, 4833--4837.

\bibitem[{Gao et~al.(2023)Gao, Chen, Zhang, Xiao, and Sun}]{gao2023compositional}
Gao, K.; Chen, L.; Zhang, H.; Xiao, J.; and Sun, Q. 2023.
\newblock Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net.

\bibitem[{Gao, Yao, and Chen(2021)}]{gao2021simcse}
Gao, T.; Yao, X.; and Chen, D. 2021.
\newblock Simcse: Simple contrastive learning of sentence embeddings.
\newblock \emph{arXiv preprint arXiv:2104.08821}.

\bibitem[{Gkioxari et~al.(2018)Gkioxari, Girshick, Doll{\'a}r, and He}]{gkioxari2018detecting}
Gkioxari, G.; Girshick, R.; Doll{\'a}r, P.; and He, K. 2018.
\newblock Detecting and recognizing human-object interactions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, 8359--8367.

\bibitem[{Goyal et~al.(2017)Goyal, Kahou, Michalski, Materzynska, Westphal, Kim, Haenel, Fruend, Yianilos, Mueller-Freitag, Hoppe, Thurau, Bax, and Memisevic}]{8237884}
Goyal, R.; Kahou, S.~E.; Michalski, V.; Materzynska, J.; Westphal, S.; Kim, H.; Haenel, V.; Fruend, I.; Yianilos, P.; Mueller-Freitag, M.; Hoppe, F.; Thurau, C.; Bax, I.; and Memisevic, R. 2017.
\newblock The “Something Something” Video Database for Learning and Evaluating Visual Common Sense.
\newblock In \emph{2017 IEEE International Conference on Computer Vision (ICCV)}, 5843--5851.

\bibitem[{Han et~al.(2016)Han, Khorrami, Paine, Ramachandran, Babaeizadeh, Shi, Li, Yan, and Huang}]{han2016seqnms}
Han, W.; Khorrami, P.; Paine, T.~L.; Ramachandran, P.; Babaeizadeh, M.; Shi, H.; Li, J.; Yan, S.; and Huang, T.~S. 2016.
\newblock Seq-NMS for Video Object Detection.
\newblock arXiv:1602.08465.

\bibitem[{Heilbron et~al.(2015)Heilbron, Escorcia, Ghanem, and Niebles}]{Hei2015anet}
Heilbron, F.~C.; Escorcia, V.; Ghanem, B.; and Niebles, J.~C. 2015.
\newblock ActivityNet: A large-scale video benchmark for human activity understanding.
\newblock In \emph{2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 961--970.

\bibitem[{Ji et~al.(2019)Ji, Krishna, Fei{-}Fei, and Niebles}]{genome}
Ji, J.; Krishna, R.; Fei{-}Fei, L.; and Niebles, J.~C. 2019.
\newblock Action Genome: Actions as Composition of Spatio-temporal Scene Graphs.
\newblock \emph{CoRR}, abs/1912.06992.

\bibitem[{Kay et~al.(2017)Kay, Carreira, Simonyan, Zhang, Hillier, Vijayanarasimhan, Viola, Green, Back, Natsev, Suleyman, and Zisserman}]{Kay2017TheKH}
Kay, W.; Carreira, J.; Simonyan, K.; Zhang, B.; Hillier, C.; Vijayanarasimhan, S.; Viola, F.; Green, T.; Back, T.; Natsev, A.; Suleyman, M.; and Zisserman, A. 2017.
\newblock The Kinetics Human Action Video Dataset.
\newblock \emph{ArXiv}, abs/1705.06950.

\bibitem[{Kong and Fu(2022)}]{kong2022human}
Kong, Y.; and Fu, Y. 2022.
\newblock Human action recognition and prediction: A survey.
\newblock \emph{International Journal of Computer Vision}, 130(5): 1366--1401.

\bibitem[{Li et~al.(2022)Li, Liu, Wu, Li, Qiu, Xu, Xu, Fang, and Lu}]{li2022hake}
Li, Y.-L.; Liu, X.; Wu, X.; Li, Y.; Qiu, Z.; Xu, L.; Xu, Y.; Fang, H.-S.; and Lu, C. 2022.
\newblock Hake: a knowledge engine foundation for human activity understanding.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}.

\bibitem[{Li et~al.(2023)Li, Tang, Peng, Qi, and Tang}]{li2023knowledge}
Li, Z.; Tang, H.; Peng, Z.; Qi, G.-J.; and Tang, J. 2023.
\newblock Knowledge-guided semantic transfer network for few-shot image recognition.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}.

\bibitem[{Li, Tang, and Mei(2018)}]{li2018deep}
Li, Z.; Tang, J.; and Mei, T. 2018.
\newblock Deep collaborative embedding for social image understanding.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 41(9): 2070--2083.

\bibitem[{Lin et~al.(2022)Lin, Li, Lin, Ahmed, Gan, Liu, Lu, and Wang}]{lin2022swinbert}
Lin, K.; Li, L.; Lin, C.-C.; Ahmed, F.; Gan, Z.; Liu, Z.; Lu, Y.; and Wang, L. 2022.
\newblock Swinbert: End-to-end transformers with sparse attention for video captioning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 17949--17958.

\bibitem[{Liu et~al.(2022)Liu, Wang, Wang, Ma, and Qiao}]{liu2022fineaction}
Liu, Y.; Wang, L.; Wang, Y.; Ma, X.; and Qiao, Y. 2022.
\newblock FineAction: A Fine-Grained Video Dataset for Temporal Action Localization.
\newblock arXiv:2105.11107.

\bibitem[{Luo et~al.(2022)Luo, Ji, Zhong, Chen, Lei, Duan, and Li}]{luo2022clip4clip}
Luo, H.; Ji, L.; Zhong, M.; Chen, Y.; Lei, W.; Duan, N.; and Li, T. 2022.
\newblock Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning.
\newblock \emph{Neurocomputing}, 508: 293--304.

\bibitem[{Mokady, Hertz, and Bermano(2021)}]{mokady2021clipcap}
Mokady, R.; Hertz, A.; and Bermano, A.~H. 2021.
\newblock Clipcap: Clip prefix for image captioning.
\newblock \emph{arXiv preprint arXiv:2111.09734}.

\bibitem[{Monfort et~al.(2019)Monfort, Andonian, Zhou, Ramakrishnan, Bargal, Yan, Brown, Fan, Gutfruend, Vondrick et~al.}]{monfortmoments}
Monfort, M.; Andonian, A.; Zhou, B.; Ramakrishnan, K.; Bargal, S.~A.; Yan, T.; Brown, L.; Fan, Q.; Gutfruend, D.; Vondrick, C.; et~al. 2019.
\newblock Moments in Time Dataset: one million videos for event understanding.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 1--8.

\bibitem[{Monfort et~al.(2021)Monfort, Pan, Ramakrishnan, Andonian, Mcnamara, Lascelles, Fan, Gutfreund, Feris, and Oliva}]{9609554}
Monfort, M.; Pan, B.; Ramakrishnan, K.; Andonian, A.; Mcnamara, B.~A.; Lascelles, A.; Fan, Q.; Gutfreund, D.; Feris, R.; and Oliva, A. 2021.
\newblock Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 1--1.

\bibitem[{Ni et~al.(2022)Ni, Peng, Chen, Zhang, Meng, Fu, Xiang, and Ling}]{ni2022expanding}
Ni, B.; Peng, H.; Chen, M.; Zhang, S.; Meng, G.; Fu, J.; Xiang, S.; and Ling, H. 2022.
\newblock Expanding Language-Image Pretrained Models for General Video Recognition.
\newblock arXiv:2208.02816.

\bibitem[{Qian et~al.(2023)Qian, Cui, Chen, Peng, Guo, and Jiang}]{qian2023locate}
Qian, T.; Cui, R.; Chen, J.; Peng, P.; Guo, X.; and Jiang, Y.-G. 2023.
\newblock Locate before answering: Answer guided question localization for video question answering.
\newblock \emph{IEEE Transactions on Multimedia}.

\bibitem[{Qian et~al.(2019)Qian, Zhuang, Li, Xiao, Pu, and Xiao}]{10.1145/3343031.3351058}
Qian, X.; Zhuang, Y.; Li, Y.; Xiao, S.; Pu, S.; and Xiao, J. 2019.
\newblock Video Relation Detection with Spatio-Temporal Graph.
\newblock In \emph{Proceedings of the 27th ACM International Conference on Multimedia}, MM '19, 84–93. New York, NY, USA: Association for Computing Machinery.
\newblock ISBN 9781450368896.

\bibitem[{Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark et~al.}]{radford2021learning}
Radford, A.; Kim, J.~W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et~al. 2021.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, 8748--8763. PMLR.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever et~al.}]{radford2019language}
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et~al. 2019.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1(8): 9.

\bibitem[{Rasheed et~al.(2023)Rasheed, khattak, Maaz, Khan, and Khan}]{Rasheed2023vificlip}
Rasheed, H.; khattak, M.~U.; Maaz, M.; Khan, S.; and Khan, F.~S. 2023.
\newblock Finetuned CLIP models are efficient video learners.
\newblock In \emph{The IEEE/CVF Conference on Computer Vision and Pattern Recognition}.

\bibitem[{Sadhu et~al.(2021)Sadhu, Gupta, Yatskar, Nevatia, and Kembhavi}]{sadhu2021visual}
Sadhu, A.; Gupta, T.; Yatskar, M.; Nevatia, R.; and Kembhavi, A. 2021.
\newblock Visual semantic role labeling for video understanding.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 5589--5600.

\bibitem[{Shang et~al.(2019)Shang, Di, Xiao, Cao, Yang, and Chua}]{shang2019annotating}
Shang, X.; Di, D.; Xiao, J.; Cao, Y.; Yang, X.; and Chua, T.-S. 2019.
\newblock Annotating Objects and Relations in User-Generated Videos.
\newblock In \emph{Proceedings of the 2019 on International Conference on Multimedia Retrieval}, 279--287. ACM.

\bibitem[{Shang et~al.(2017)Shang, Ren, Guo, Zhang, and Chua}]{Shang2017VideoVR}
Shang, X.; Ren, T.; Guo, J.; Zhang, H.; and Chua, T.-S. 2017.
\newblock Video Visual Relation Detection.
\newblock \emph{Proceedings of the 25th ACM international conference on Multimedia}.

\bibitem[{Song, Chen, and Jiang(2023)}]{song2023relation}
Song, X.; Chen, J.; and Jiang, Y.-G. 2023.
\newblock Relation Triplet Construction for Cross-modal Text-to-Video Retrieval.
\newblock In \emph{Proceedings of the 31st ACM International Conference on Multimedia}, 4759--4767.

\bibitem[{Song et~al.(2021)Song, Chen, Wu, and Jiang}]{song2021spatial}
Song, X.; Chen, J.; Wu, Z.; and Jiang, Y.-G. 2021.
\newblock Spatial-temporal graphs for cross-modal text2video retrieval.
\newblock \emph{IEEE Transactions on Multimedia}, 24: 2914--2923.

\bibitem[{Tang et~al.(2021)Tang, Wang, Liu, Rao, Li, and Li}]{tang2021clip4caption}
Tang, M.; Wang, Z.; Liu, Z.; Rao, F.; Li, D.; and Li, X. 2021.
\newblock Clip4caption: Clip for video caption.
\newblock In \emph{Proceedings of the 29th ACM International Conference on Multimedia}, 4858--4862.

\bibitem[{Wang et~al.(2022)Wang, Yang, Hu, Li, Lin, Gan, Liu, Liu, and Wang}]{wang2022git}
Wang, J.; Yang, Z.; Hu, X.; Li, L.; Lin, K.; Gan, Z.; Liu, Z.; Liu, C.; and Wang, L. 2022.
\newblock GIT: A Generative Image-to-text Transformer for Vision and Language.
\newblock arXiv:2205.14100.

\bibitem[{Wang et~al.(2019)Wang, Wu, Chen, Li, Wang, and Wang}]{wang2019vatex}
Wang, X.; Wu, J.; Chen, J.; Li, L.; Wang, Y.-F.; and Wang, W.~Y. 2019.
\newblock Vatex: A large-scale, high-quality multilingual dataset for video-and-language research.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, 4581--4591.

\bibitem[{Wang, Chen, and Jiang(2021)}]{wang2021visual}
Wang, Z.; Chen, J.; and Jiang, Y.-G. 2021.
\newblock Visual co-occurrence alignment learning for weakly-supervised video moment retrieval.
\newblock In \emph{Proceedings of the 29th ACM International Conference on Multimedia}, 1459--1468.

\bibitem[{Xia and Zhan(2020)}]{9062498}
Xia, H.; and Zhan, Y. 2020.
\newblock A Survey on Temporal Action Localization.
\newblock \emph{IEEE Access}, 8: 70477--70487.

\bibitem[{Xu et~al.(2023)Xu, Ye, Yan, Shi, Ye, Xu, Li, Bi, Qian, Wang, Xu, Zhang, Huang, Huang, and Zhou}]{xu2023mplug2}
Xu, H.; Ye, Q.; Yan, M.; Shi, Y.; Ye, J.; Xu, Y.; Li, C.; Bi, B.; Qian, Q.; Wang, W.; Xu, G.; Zhang, J.; Huang, S.; Huang, F.; and Zhou, J. 2023.
\newblock mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video.
\newblock arXiv:2302.00402.

\bibitem[{Xu et~al.(2016)Xu, Mei, Yao, and Rui}]{7780940}
Xu, J.; Mei, T.; Yao, T.; and Rui, Y. 2016.
\newblock MSR-VTT: A Large Video Description Dataset for Bridging Video and Language.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 5288--5296.

\bibitem[{Yan et~al.(2023)Yan, Zhu, Wang, Cao, Zhang, Ghosh, Wu, and Yu}]{yan2023videococa}
Yan, S.; Zhu, T.; Wang, Z.; Cao, Y.; Zhang, M.; Ghosh, S.; Wu, Y.; and Yu, J. 2023.
\newblock VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners.
\newblock arXiv:2212.04979.

\bibitem[{Yang et~al.(2018)Yang, Gao, Saba-Sadiya, and Chai}]{yang2018commonsense}
Yang, S.; Gao, Q.; Saba-Sadiya, S.; and Chai, J. 2018.
\newblock Commonsense justification for action explanation.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, 2627--2637.

\bibitem[{Yatskar, Zettlemoyer, and Farhadi(2016)}]{yatskar2016situation}
Yatskar, M.; Zettlemoyer, L.; and Farhadi, A. 2016.
\newblock Situation recognition: Visual semantic role labeling for image understanding.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, 5534--5542.

\bibitem[{Zhang et~al.(2019)Zhang, Wang, Ma, and Liu}]{zhang2019reconstruct}
Zhang, W.; Wang, B.; Ma, L.; and Liu, W. 2019.
\newblock Reconstruct and represent video contents for captioning via reinforcement learning.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 42(12): 3088--3101.

\bibitem[{Zhang et~al.(2021)Zhang, Wu, Weng, Fu, Chen, Jiang, and Davis}]{zhang2021videolt}
Zhang, X.; Wu, Z.; Weng, Z.; Fu, H.; Chen, J.; Jiang, Y.-G.; and Davis, L.~S. 2021.
\newblock Videolt: Large-scale long-tailed video recognition.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, 7960--7969.

\bibitem[{Zheng, Chen, and Jin(2022)}]{zheng2022vrdformer}
Zheng, S.; Chen, S.; and Jin, Q. 2022.
\newblock VRDFormer: End-to-End Video Visual Relation Detection with Transformers.
\newblock In \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 18814--18824.

\bibitem[{Zhong et~al.(2021)Zhong, Yang, Zhang, Li, Codella, Li, Zhou, Dai, Yuan, Li, and Gao}]{zhong2021regionclip}
Zhong, Y.; Yang, J.; Zhang, P.; Li, C.; Codella, N.; Li, L.~H.; Zhou, L.; Dai, X.; Yuan, L.; Li, Y.; and Gao, J. 2021.
\newblock RegionCLIP: Region-based Language-Image Pretraining.
\newblock arXiv:2112.09106.

\end{thebibliography}
